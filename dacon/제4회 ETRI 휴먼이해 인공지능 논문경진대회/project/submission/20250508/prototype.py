# -*- coding: utf-8 -*-
"""prototype.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bYRUFnay1zYTPG_IYTpgEYZa__58snD6
"""

!pip install pyarrow
!pip install fastparquet

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import glob
import random
import os
import matplotlib.pyplot as plt
import seaborn as sns
import ast

import warnings
warnings.filterwarnings('ignore')

from lightgbm import LGBMClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import f1_score

# seed ê³ ì •
SD = 42
random.seed(SD)
np.random.seed(SD)
os.environ['PYTHONHASHSEED'] = str(SD)

"""Data Process"""

# íŒŒì¼ ê²½ë¡œ ì„¤ì •
data_dir = '/content/drive/MyDrive/ETRI/ETRI_lifelog_dataset/ch2025_data_items'

# Parquet íŒŒì¼ ì „ì²´ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
parquet_files = glob.glob(os.path.join(data_dir, 'ch2025_*.parquet'))

# !ls /content/drive/MyDrive
# !ls /content/drive/MyDrive/ETRI
!ls /content/drive/MyDrive/ETRI/ETRI_lifelog_dataset/ch2025_data_items

print("ğŸ” data_dir exists:", os.path.exists(data_dir))
print("ğŸ” Files found:", parquet_files)

# íŒŒì¼ ì´ë¦„ì„ í‚¤ë¡œ, DataFrameì„ ê°’ìœ¼ë¡œ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
lifelog_data = {}

# íŒŒì¼ë³„ë¡œ ì½ê¸°
for file_path in parquet_files:
    name = os.path.basename(file_path).replace('.parquet', '').replace('ch2025_', '')
    lifelog_data[name] = pd.read_parquet(file_path)
    print(f"âœ… Loaded: {name}, shape = {lifelog_data[name].shape}")

# ë”•ì…”ë„ˆë¦¬ì— ìˆëŠ” ëª¨ë“  í•­ëª©ì„ ë…ë¦½ì ì¸ ë³€ìˆ˜ë¡œ í• ë‹¹
for key, df in lifelog_data.items():
    globals()[f"{key}_df"] = df

# lifelog_dateê°€ timestampë‘ ê°™ë‹¤
metrics_train = pd.read_csv('/content/drive/MyDrive/ETRI/ETRI_lifelog_dataset/ch2025_metrics_train.csv')
sample_submission = pd.read_csv('/content/drive/MyDrive/ETRI/ETRI_lifelog_dataset/ch2025_submission_sample.csv')

metrics_train.head()
print('-----------------')
sample_submission.head()

"""ëª¨ë“  ê°’ì€ ë²”ì£¼í˜• í´ë˜ìŠ¤ ì§‘í•© --- ë¶„ë¥˜ ë¬¸ì œ


"""

# âœ… ê¸°ì¤€ ìŒ (subject_id, lifelog_date)
sample_submission['lifelog_date'] = pd.to_datetime(sample_submission['lifelog_date'])
test_keys = set(zip(sample_submission['subject_id'], sample_submission['lifelog_date'].dt.date))

# âœ… DataFrame ë³„ timestamp ì»¬ëŸ¼ ìˆ˜ë™ ì§€ì •
dataframes = {
    'mACStatus': (mACStatus_df, 'timestamp'),
    'mActivity': (mActivity_df, 'timestamp'),
    'mAmbience': (mAmbience_df, 'timestamp'),
    'mBle': (mBle_df, 'timestamp'),
    'mGps': (mGps_df, 'timestamp'),
    'mLight': (mLight_df, 'timestamp'),
    'mScreenStatus': (mScreenStatus_df, 'timestamp'),
    'mUsageStats': (mUsageStats_df, 'timestamp'),
    'mWifi': (mWifi_df, 'timestamp'),
    'wHr': (wHr_df, 'timestamp'),
    'wLight': (wLight_df, 'timestamp'),
    'wPedo': (wPedo_df, 'timestamp'),
}

# âœ… ë¶„ë¦¬ í•¨ìˆ˜
# test_df = ì œì¶œ ëŒ€ìƒ + ë‚ ì§œì™€ ì¼ì¹˜í•˜ëŠ” ë°ì´í„°ë§Œ í¬í•¨
# train_df = ë‚˜ë¨¸ì§€ ëª¨ë“  ë°ì´í„° ( ëª¨ë¸í•™ìŠµì— ì‚¬ìš©ë  ê²ƒ )
def split_test_train(df, subject_col='subject_id', timestamp_col='timestamp'):
    df[timestamp_col] = pd.to_datetime(df[timestamp_col], errors='coerce')
    df = df.dropna(subset=[timestamp_col])
    df['date_only'] = df[timestamp_col].dt.date
    df['key'] = list(zip(df[subject_col], df['date_only']))

    test_df = df[df['key'].isin(test_keys)].drop(columns=['date_only', 'key'])
    train_df = df[~df['key'].isin(test_keys)].drop(columns=['date_only', 'key'])
    return test_df, train_df

# âœ… ê²°ê³¼ ì €ì¥
# ëª¨ë“  df ë°˜ë³µí•˜ë©´ì„œ train/test ë³„ë„ ë³€ìˆ˜ ì €ì¥
for name, (df, ts_col) in dataframes.items():
    print(f"â³ {name} ë¶„ë¦¬ ì¤‘...")
    test_df, train_df = split_test_train(df.copy(), subject_col='subject_id', timestamp_col=ts_col)
    globals()[f"{name}_test"] = test_df
    globals()[f"{name}_train"] = train_df
    print(f"âœ… {name}_test â†’ {test_df.shape}, {name}_train â†’ {train_df.shape}")

"""í˜„ì¬ê¹Œì§€ í•œ ê²ƒ
1. êµ¬ê¸€ë“œë¼ì´ë¸Œ íŒŒì¼ ì…ì¶œë ¥ í™•ì¸
2. íŒŒì¼ ì´ë¦„=key, DataFrame=value ë¡œ ë”•ì…”ë„ˆë¦¬ ì €ì¥
3. ë”•ì…”ë„ˆë¦¬ì—ìˆëŠ” ëª¨ë“  ê±¸ ê° ë³€ìˆ˜ì— ì €ì¥
4. í•™ìŠµìš©csv, ì œì¶œìš©csv ì½ê¸°
5. í•™ìŠµìš©data, í…ŒìŠ¤íŠ¸data ë¶„ë¦¬ ë° ì €ì¥

ì•ìœ¼ë¡œ í•´ì•¼ ë  ê²ƒ
1. ë°ì´í„° ì „ì²˜ë¦¬ ( í•˜ë£¨ ê¸°ì¤€ )
-----
1. ë°ì´í„° í™•ì¸
2. ëª©í‘œ í™•ì¸ -- ì„±ê³µë¥  ì˜ˆì¸¡
3. ê²°ì¸¡ì¹˜ í™•ì¸ ë° ì²˜ë¦¬
4. EDA (ì‹œê°í™” í¬í•¨í•œ íƒìƒ‰ì  ë¶„ì„)
5. ì»¬ëŸ¼ë³„ ë¶„ì„ + ì´ìƒì¹˜ íƒìƒ‰
6. ë°ì´í„° ë¼ë²¨ë§ ë° ë¶ˆë¦¬ì–¸í™”
7. í•„ìš”ì—†ëŠ” ì»¬ëŸ¼ ì œê±°
8. Feature Engineering (íŒŒìƒë³€ìˆ˜ ìƒì„±)
9. Feature Importance / ìƒê´€ ë¶„ì„
10. ê²€ì¦ ì „ëµ ì„¤ì • (KFold ë“±)
11. ëª¨ë¸ ì„ íƒ ë° í•™ìŠµ
12. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
13. ìµœì¢… ëª¨ë¸ ì„ ì •
14. ê²°ê³¼ ì˜ˆì¸¡ ë° ì €ì¥
15. í‰ê°€ ì§€í‘œ í™•ì¸ ë° ëŒ€íšŒ ì œì¶œ
--
ë„ˆë¬´ ì¢‹ìŠµë‹ˆë‹¤. ì„¼ì„œ ë°ì´í„°ëŠ” ì›ë˜ ì‹œê³„ì—´ì´ì§€ë§Œ,
â†’ ë¨¸ì‹ ëŸ¬ë‹ì— ë„£ê¸° ìœ„í•´ì„  ë³´í†µ "í•˜ë£¨ ë‹¨ìœ„ë¡œ ìš”ì•½"í•©ë‹ˆë‹¤.

ì´ë¯¸ ì˜ˆì‹œê°€ ì¤€ë¹„ëœ ì„¼ì„œ:
mACStatus â†’ ì¶©ì „ ë¹„ìœ¨, ì¶©ì „ ì§€ì† ì‹œê°„ ë“±

mActivity â†’ í™œë™ ë¹„ìœ¨, ëŒ€í‘œ í™œë™ ë“±

ë‹¤ìŒìœ¼ë¡œ ì¤€ë¹„í•˜ë©´ ì¢‹ì„ ì„¼ì„œë“¤:
ì„¼ì„œ ì´ë¦„	ìš”ì•½ í”¼ì²˜ ì˜ˆì‹œ
wHr (ì‹¬ë°•ìˆ˜)	í‰ê· /ìµœëŒ€/ìµœì†Œ/í‘œì¤€í¸ì°¨, ì‹¬ë°• ìƒìŠ¹ íšŸìˆ˜
wPedo (ê±¸ìŒ ìˆ˜)	ì´ ê±¸ìŒ ìˆ˜, í™œë™ ì‹œê°„ëŒ€, ë°¤ ì‹œê°„ëŒ€ ê±¸ìŒ ìˆ˜
mLight / wLight (ì¡°ë„)	í‰ê·  ë°ê¸°, ìˆ˜ë©´ ì§ì „ ì¡°ë„ ë³€í™”ëŸ‰
mScreenStatus	í™”ë©´ ì¼œì§ íšŸìˆ˜, ì´ ì‚¬ìš© ì‹œê°„, ë°¤ ì‹œê°„ëŒ€ ì‚¬ìš© ì—¬ë¶€
mUsageStats	ì•± ì‚¬ìš© ì´ ì‹œê°„, ìƒìœ„ ì•± ì‚¬ìš© ë¹„ìœ¨ ë“±
"""

def process_mACStatus(df):
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['date'] = df['timestamp'].dt.date
    df = df.sort_values(['subject_id', 'timestamp'])

    results = []

    for (subj, date), group in df.groupby(['subject_id', 'date']):
        status = group['m_charging'].values  # 0/1 ìƒíƒœ
        times = group['timestamp'].values

        # ì¶©ì „ ìƒíƒœ ë¹„ìœ¨
        ratio_charging = status.mean()

        # ìƒíƒœ ì „ì´ íšŸìˆ˜
        transitions = (status[1:] != status[:-1]).sum()

        # ì—°ì†ëœ 1 ìƒíƒœ ê¸¸ì´ë“¤
        lengths = []
        current_len = 0
        for val in status:
            if val == 1:
                current_len += 1
            elif current_len > 0:
                lengths.append(current_len)
                current_len = 0
        if current_len > 0:
            lengths.append(current_len)

        avg_charging_duration = np.mean(lengths) if lengths else 0
        max_charging_duration = np.max(lengths) if lengths else 0

        results.append({
            'subject_id': subj,
            'date': date,
            'charging_ratio': ratio_charging,
            'charging_transitions': transitions,
            'avg_charging_duration': avg_charging_duration,
            'max_charging_duration': max_charging_duration,
        })

    return pd.DataFrame(results)

mACStatus_df2 = process_mACStatus(mACStatus_df)

def process_mActivity(df):
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['date'] = df['timestamp'].dt.date

    summary = []

    for (subj, date), group in df.groupby(['subject_id', 'date']):
        counts = group['m_activity'].value_counts(normalize=True)  # ë¹„ìœ¨
        row = {'subject_id': subj, 'date': date}

        # 0~8 ë¹„ìœ¨ ì €ì¥
        for i in range(9):
            row[f'activity_{i}_ratio'] = counts.get(i, 0)

        # ì£¼ìš” í™œë™ ì •ë³´
        row['dominant_activity'] = group['m_activity'].mode()[0]
        row['num_unique_activities'] = group['m_activity'].nunique()

        summary.append(row)

    return pd.DataFrame(summary)

mActivity_df2 = process_mActivity(mActivity_df)

# ì§€ì •ëœ 10ê°œ ë¼ë²¨
top_10_labels = [
    "Inside, small room", "Speech", "Silence", "Music",
    "Narration, monologue", "Child speech, kid speaking",
    "Conversation", "Speech synthesizer", "Shout", "Babbling"
]

def process_mAmbience_top10(df):
    df = df.copy()
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['date'] = df['timestamp'].dt.date

    # ì´ˆê¸°í™”
    for label in top_10_labels + ['others']:
        df[label] = 0.0

    for idx, row in df.iterrows():
        parsed = ast.literal_eval(row['m_ambience']) if isinstance(row['m_ambience'], str) else row['m_ambience']
        others_prob = 0.0

        for label, prob in parsed:
            prob = float(prob)
            if label in top_10_labels:
                df.at[idx, label] = prob
            else:
                others_prob += prob

        df.at[idx, 'others'] = others_prob

    return df.drop(columns=['m_ambience'])

mAmbience_df2= process_mAmbience_top10(mAmbience_df)

def summarize_mAmbience_daily(df):
    prob_cols = [col for col in df.columns if col not in ['subject_id', 'timestamp', 'date']]

    # í•˜ë£¨ ë‹¨ìœ„ë¡œ í‰ê· ê°’ ìš”ì•½
    daily_summary = df.groupby(['subject_id', 'date'])[prob_cols].mean().reset_index()
    return daily_summary

mAmbience_df2 = summarize_mAmbience_daily(mAmbience_df2)

def process_mBle(df):
    df = df.copy()
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['date'] = df['timestamp'].dt.date

    features = []

    for idx, row in df.iterrows():
        entry = ast.literal_eval(row['m_ble']) if isinstance(row['m_ble'], str) else row['m_ble']

        rssi_list = []
        class_0_cnt = 0
        class_other_cnt = 0

        for device in entry:
            try:
                rssi = int(device['rssi'])
                rssi_list.append(rssi)

                if str(device['device_class']) == '0':
                    class_0_cnt += 1
                else:
                    class_other_cnt += 1
            except:
                continue  # malformed record

        feature = {
            'subject_id': row['subject_id'],
            'date': row['date'],
            'device_class_0_cnt': class_0_cnt,
            'device_class_others_cnt': class_other_cnt,
            'device_count': len(rssi_list),
            'rssi_mean': np.mean(rssi_list) if rssi_list else np.nan,
            'rssi_min': np.min(rssi_list) if rssi_list else np.nan,
            'rssi_max': np.max(rssi_list) if rssi_list else np.nan,
        }
        features.append(feature)

    return pd.DataFrame(features)

def summarize_mBle_daily(df):
    # row ë‹¨ìœ„ BLE feature ì¶”ì¶œ
    df = process_mBle(df)

    # í•˜ë£¨ ë‹¨ìœ„ë¡œ cnt í•©ì¹˜ê¸°
    grouped = df.groupby(['subject_id', 'date']).agg({
        'device_class_0_cnt': 'sum',
        'device_class_others_cnt': 'sum',
        'rssi_mean': 'mean',
        'rssi_min': 'min',
        'rssi_max': 'max',
    }).reset_index()

    # ì´í•© êµ¬í•´ì„œ ë¹„ìœ¨ ê³„ì‚°
    total_cnt = grouped['device_class_0_cnt'] + grouped['device_class_others_cnt']
    grouped['device_class_0_ratio'] = grouped['device_class_0_cnt'] / total_cnt.replace(0, np.nan)
    grouped['device_class_others_ratio'] = grouped['device_class_others_cnt'] / total_cnt.replace(0, np.nan)

    # í•„ìš” ì—†ëŠ” ì›ë˜ cnt ì»¬ëŸ¼ ì œê±°
    grouped.drop(columns=['device_class_0_cnt', 'device_class_others_cnt'], inplace=True)

    return grouped

mBle_df2 = summarize_mBle_daily(mBle_df)

def process_mGps(df):
    df = df.copy()
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['date'] = df['timestamp'].dt.date

    features = []

    for idx, row in df.iterrows():
        gps_list = ast.literal_eval(row['m_gps']) if isinstance(row['m_gps'], str) else row['m_gps']

        altitudes = []
        latitudes = []
        longitudes = []
        speeds = []

        for entry in gps_list:
            try:
                altitudes.append(float(entry['altitude']))
                latitudes.append(float(entry['latitude']))
                longitudes.append(float(entry['longitude']))
                speeds.append(float(entry['speed']))
            except:
                continue

        features.append({
            'subject_id': row['subject_id'],
            'date': row['date'],
            'altitude_mean': np.mean(altitudes) if altitudes else np.nan,
            'latitude_std': np.std(latitudes) if latitudes else np.nan,
            'longitude_std': np.std(longitudes) if longitudes else np.nan,
            'speed_mean': np.mean(speeds) if speeds else np.nan,
            'speed_max': np.max(speeds) if speeds else np.nan,
            'speed_std': np.std(speeds) if speeds else np.nan,
        })

    return pd.DataFrame(features)

m_Gps_df2 = process_mGps(mGps_df)

m_Gps_df2 = m_Gps_df2.groupby(['subject_id', 'date']).agg({
    'altitude_mean': 'mean',
    'latitude_std': 'mean',
    'longitude_std': 'mean',
    'speed_mean': 'mean',
    'speed_max': 'max',
    'speed_std': 'mean'
}).reset_index()

def process_mLight(df):
    df = df.copy()
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['date'] = df['timestamp'].dt.date
    df['hour'] = df['timestamp'].dt.hour

    # ë°¤(22~05ì‹œ), ë‚®(06~21ì‹œ) êµ¬ë¶„
    df['is_night'] = df['hour'].apply(lambda h: h >= 22 or h < 6)

    # í•˜ë£¨ ë‹¨ìœ„ ìš”ì•½
    daily = df.groupby(['subject_id', 'date']).agg(
        light_mean=('m_light', 'mean'),
        light_std=('m_light', 'std'),
        light_max=('m_light', 'max'),
        light_min=('m_light', 'min'),
        light_night_mean=('m_light', lambda x: x[df.loc[x.index, 'is_night']].mean()),
        light_day_mean=('m_light', lambda x: x[~df.loc[x.index, 'is_night']].mean()),
        light_night_ratio=('is_night', 'mean')  # ë°¤ ì‹œê°„ ì¸¡ì • ë¹„ìœ¨
    ).reset_index()

    return daily

mLight_df2 = process_mLight(mLight_df)

def process_mScreenStatus(df):
    df = df.copy()
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['date'] = df['timestamp'].dt.date

    features = []

    for (subj, date), group in df.groupby(['subject_id', 'date']):
        status = group['m_screen_use'].values
        ratio_on = status.mean()
        transitions = (status[1:] != status[:-1]).sum()

        # ì—°ì†ëœ 1 ìƒíƒœ ê¸¸ì´ë“¤
        durations = []
        current = 0
        for val in status:
            if val == 1:
                current += 1
            elif current > 0:
                durations.append(current)
                current = 0
        if current > 0:
            durations.append(current)

        features.append({
            'subject_id': subj,
            'date': date,
            'screen_on_ratio': ratio_on,
            'screen_on_transitions': transitions,
            'screen_on_duration_avg': np.mean(durations) if durations else 0,
            'screen_on_duration_max': np.max(durations) if durations else 0,
        })

    return pd.DataFrame(features)

mScreenStatus_df2 = process_mScreenStatus(mScreenStatus_df)

top_apps = [
    'One UI í™ˆ', 'ì¹´ì¹´ì˜¤í†¡', 'ì‹œìŠ¤í…œ UI', 'NAVER', 'ìºì‹œì›Œí¬', 'ì„±ê²½ì¼ë…Q',
    'YouTube', 'í†µí™”', 'ë©”ì‹œì§€', 'íƒ€ì„ìŠ¤í”„ë ˆë“œ', 'Instagram']

def process_mUsageStats(df):
    df = df.copy()
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['date'] = df['timestamp'].dt.date

    features = []

    for (subj, date), group in df.groupby(['subject_id', 'date']):
        app_time = {app: 0 for app in top_apps}
        others_time = 0

        for row in group['m_usage_stats']:
            parsed = ast.literal_eval(row) if isinstance(row, str) else row
            for entry in parsed:
                app = entry.get('app_name')
                time = entry.get('total_time', 0)
                if app in top_apps:
                    app_time[app] += int(time)
                else:
                    others_time += int(time)

        feature = {
            'subject_id': subj,
            'date': date,
            'others_time': others_time
        }
        # ê° ì•±ë³„ ì»¬ëŸ¼ ì¶”ê°€
        feature.update({f'{app}_time': app_time[app] for app in top_apps})

        features.append(feature)

    return pd.DataFrame(features)

mUsageStats_df2 = process_mUsageStats(mUsageStats_df)

def process_mWifi(df):
    df = df.copy()
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['date'] = df['timestamp'].dt.date

    results = []

    for (subj, date), group in df.groupby(['subject_id', 'date']):
        rssi_all = []

        for row in group['m_wifi']:
            parsed = ast.literal_eval(row) if isinstance(row, str) else row
            for ap in parsed:
                try:
                    rssi = int(ap['rssi'])
                    rssi_all.append(rssi)
                except:
                    continue

        results.append({
            'subject_id': subj,
            'date': date,
            'wifi_rssi_mean': np.mean(rssi_all) if rssi_all else np.nan,
            'wifi_rssi_min': np.min(rssi_all) if rssi_all else np.nan,
            'wifi_rssi_max': np.max(rssi_all) if rssi_all else np.nan,
            'wifi_detected_cnt': len(rssi_all)
        })

    return pd.DataFrame(results)

mWifi_df2 = process_mWifi(mWifi_df)

def get_time_block(hour):
    if 0 <= hour < 6:
        return 'early_morning'
    elif 6 <= hour < 12:
        return 'morning'
    elif 12 <= hour < 18:
        return 'afternoon'
    else:
        return 'evening'

def process_wHr_by_timeblock(df):
    df = df.copy()
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['date'] = df['timestamp'].dt.date
    df['block'] = df['timestamp'].dt.hour.map(get_time_block)

    results = []

    for (subj, date), group in df.groupby(['subject_id', 'date']):
        block_stats = {'subject_id': subj, 'date': date}

        for block, block_group in group.groupby('block'):
            hr_all = []
            for row in block_group['heart_rate']:
                parsed = ast.literal_eval(row) if isinstance(row, str) else row
                hr_all.extend([int(h) for h in parsed if h is not None])

            if not hr_all:
                continue

            above_100 = [hr for hr in hr_all if hr > 100]
            block_stats[f'hr_{block}_mean'] = np.mean(hr_all)
            block_stats[f'hr_{block}_std'] = np.std(hr_all)
            block_stats[f'hr_{block}_max'] = np.max(hr_all)
            block_stats[f'hr_{block}_min'] = np.min(hr_all)
            block_stats[f'hr_{block}_above_100_ratio'] = len(above_100) / len(hr_all)

        results.append(block_stats)

    return pd.DataFrame(results)

wHr_df2 = process_wHr_by_timeblock(wHr_df)

def get_time_block(hour):
    if 0 <= hour < 6:
        return 'early_morning'
    elif 6 <= hour < 12:
        return 'morning'
    elif 12 <= hour < 18:
        return 'afternoon'
    else:
        return 'evening'

def process_wLight_by_timeblock(df):
    df = df.copy()
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['date'] = df['timestamp'].dt.date
    df['block'] = df['timestamp'].dt.hour.map(get_time_block)

    results = []

    for (subj, date), group in df.groupby(['subject_id', 'date']):
        block_stats = {'subject_id': subj, 'date': date}

        for block, block_group in group.groupby('block'):
            lux = block_group['w_light'].dropna().values
            if len(lux) == 0:
                continue

            block_stats[f'wlight_{block}_mean'] = np.mean(lux)
            block_stats[f'wlight_{block}_std'] = np.std(lux)
            block_stats[f'wlight_{block}_max'] = np.max(lux)
            block_stats[f'wlight_{block}_min'] = np.min(lux)

        results.append(block_stats)

    return pd.DataFrame(results)

wLight_df2 = process_wLight_by_timeblock(wLight_df)

def process_wPedo(df):
    df = df.copy()
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['date'] = df['timestamp'].dt.date

    summary = df.groupby(['subject_id', 'date']).agg({
        'step': 'sum',
        'step_frequency': 'mean',
        'distance': 'sum',
        'speed': ['mean', 'max'],
        'burned_calories': 'sum'
    }).reset_index()

    # ì»¬ëŸ¼ ì´ë¦„ ì •ë¦¬
    summary.columns = ['subject_id', 'date',
                       'step_sum', 'step_frequency_mean',
                       'distance_sum', 'speed_mean', 'speed_max',
                       'burned_calories_sum']

    return summary

wPedo_df2 = process_wPedo(wPedo_df)

"""# ìƒˆ ì„¹ì…˜"""

from functools import reduce

df_list = [
    mACStatus_df2,
    mActivity_df2,
    mAmbience_df2,
    mBle_df2,
    m_Gps_df2,
    mLight_df2,
    mScreenStatus_df2,
    mUsageStats_df2,
    mWifi_df2,
    wHr_df2,
    wHr_df2,
    wLight_df2,
    wPedo_df2
]

merged_df = reduce(lambda left, right: pd.merge(left, right, on=['subject_id', 'date'], how='outer'), df_list)

# metrics_trainì˜ lifelog_date â†’ datetime.date í˜•ìœ¼ë¡œ ë³€í™˜
metrics_train['lifelog_date'] = pd.to_datetime(metrics_train['lifelog_date']).dt.date

# merged_dfì˜ dateë„ ë³€í™˜
merged_df['date'] = pd.to_datetime(merged_df['date']).dt.date

# 1. date ê¸°ì¤€ ì •ë ¬ì„ ìœ„í•´ metrics_trainì˜ lifelog_date -> dateë¡œ ë§ì¶”ê¸°
metrics_train_renamed = metrics_train.rename(columns={'lifelog_date': 'date'})

# 2. train_df: metrics_trainê³¼ ì¼ì¹˜í•˜ëŠ” (subject_id, date) â†’ ë¼ë²¨ í¬í•¨
train_df = pd.merge(metrics_train_renamed, merged_df, on=['subject_id', 'date'], how='inner')

# 3. test_df: metrics_trainì— ì—†ëŠ” (subject_id, date)
merged_keys = merged_df[['subject_id', 'date']]
train_keys = metrics_train_renamed[['subject_id', 'date']]
test_keys = pd.merge(merged_keys, train_keys, on=['subject_id', 'date'], how='left', indicator=True)
test_keys = test_keys[test_keys['_merge'] == 'left_only'].drop(columns=['_merge'])

test_df = pd.merge(test_keys, merged_df, on=['subject_id', 'date'], how='left')

"""ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥"""

# ì €ì¥í•  ê²½ë¡œ (Google Drive ì•ˆì— ìˆëŠ” í´ë”)
save_dir = "/content/drive/MyDrive/ETRI/processed_data"
os.makedirs(save_dir, exist_ok=True)

train_df.to_csv(f"{save_dir}/train_df.csv", index=False, encoding='utf-8-sig')
test_df.to_csv(f"{save_dir}/test_df.csv", index=False, encoding='utf-8-sig')
merged_df.to_csv(f"{save_dir}/merged_df.csv", index=False, encoding='utf-8-sig')
metrics_train.to_csv(f"{save_dir}/metrics_train.csv", index=False, encoding='utf-8-sig')
sample_submission.to_csv(f"{save_dir}/submission_sample.csv", index=False, encoding='utf-8-sig')


# Feather (ë¹ ë¦„, ì‹¤ì œ ì‘ì—…ìš© ì¶”ì²œ)
train_df.to_feather(f"{save_dir}/train_df.feather")
test_df.to_feather(f"{save_dir}/test_df.feather")
merged_df.to_feather(f"{save_dir}/merged_df.feather")

# ë¼ë²¨ ë° ìƒ˜í”Œ ì œì¶œ íŒŒì¼ë„ ê°™ì´ ì €ì¥
metrics_train.to_csv(f"{save_dir}/metrics_train.csv", index=False)
sample_submission.to_csv(f"{save_dir}/submission_sample.csv", index=False)

print("âœ… ëª¨ë“  íŒŒì¼ ì €ì¥ ì™„ë£Œ")

"""ë¶ˆëŸ¬ì˜¤ê¸°"""

# ì €ì¥í–ˆë˜ ê²½ë¡œ
load_dir = "/content/drive/MyDrive/ETRI/processed_data"

# Featherë¡œ ë¹ ë¥´ê²Œ ë¶ˆëŸ¬ì˜¤ê¸°
train_df = pd.read_feather(f"{load_dir}/train_df.feather")
test_df = pd.read_feather(f"{load_dir}/test_df.feather")
merged_df = pd.read_feather(f"{load_dir}/merged_df.feather")

# CSVë¡œ ì €ì¥ëœ ë¼ë²¨/ì œì¶œ íŒŒì¼
metrics_train = pd.read_csv(f"{load_dir}/metrics_train.csv")
submission_sample = pd.read_csv(f"{load_dir}/submission_sample.csv")

print("âœ… ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ:", train_df.shape, test_df.shape)

"""ëª¨ë¸í…ŒìŠ¤íŠ¸"""

# âœ… íƒ€ê²Ÿ ë¦¬ìŠ¤íŠ¸
targets_binary = ['Q1', 'Q2', 'Q3', 'S2', 'S3']
target_multiclass = 'S1'

# âœ… feature ì¤€ë¹„
X = train_df.drop(columns=['subject_id', 'sleep_date', 'date', 'Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3'])
X.fillna(0, inplace=True)  # ê²°ì¸¡ê°’ ì²˜ë¦¬

test_X = test_df.drop(columns=['subject_id', 'date'])
test_X.fillna(0, inplace=True)

# ì»¬ëŸ¼ ì´ë¦„ì—ì„œ íŠ¹ìˆ˜ ë¬¸ì ì œê±°/ë³€í™˜
def sanitize_column_names(df):
    df.columns = (
        df.columns
        .str.replace(r"[^\w]", "_", regex=True)  # íŠ¹ìˆ˜ë¬¸ì â†’ _
        .str.replace(r"__+", "_", regex=True)    # ì—°ì†ëœ _ ì œê±°
        .str.strip("_")                          # ì•ë’¤ _ ì œê±°
    )
    return df

# ëª¨ë“  ì…ë ¥ì— ì ìš©
X = sanitize_column_names(X)
test_X = sanitize_column_names(test_X)

# ê²°ê³¼ ì €ì¥
binary_preds = {}
multiclass_pred = None

common_params = {
    'n_estimators': 1000,
    'learning_rate': 0.03,
    'random_state': 42,
    'n_jobs': -1,
    'verbosity': -1
}

# ì´ì§„ ë¶„ë¥˜ í•™ìŠµ
for col in targets_binary:
    y = train_df[col]
    model = LGBMClassifier(**common_params)
    model.fit(X, y)
    binary_preds[col] = model.predict(test_X)  # ğŸ”¥ í™•ë¥ X, í´ë˜ìŠ¤ ì§ì ‘ ì˜ˆì¸¡

# ë‹¤ì¤‘ ë¶„ë¥˜ í•™ìŠµ (S1)
y_multi = train_df['S1']
model_s1 = LGBMClassifier(**common_params, objective='multiclass', num_class=3)
model_s1.fit(X, y_multi)
multiclass_pred = model_s1.predict(test_X)  # ğŸ”¥ í´ë˜ìŠ¤ ì§ì ‘ ì˜ˆì¸¡

# importance ì¶œë ¥
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': model_s1.feature_importances_
}).sort_values('importance', ascending=False)

# ì‹œê°í™”
plt.figure(figsize=(10, 10))
sns.barplot(x='importance', y='feature', data=feature_importance)
plt.title('Feature Importance')
plt.show()

# sample ê¸°ë°˜ ì œì¶œ í¬ë§· ê°€ì ¸ì˜¤ê¸°
submission_final = sample_submission[['subject_id', 'sleep_date', 'lifelog_date']].copy()

# lifelog_date ê¸°ì¤€ìœ¼ë¡œ string â†’ date í˜•ì‹ í†µì¼
submission_final['lifelog_date'] = pd.to_datetime(submission_final['lifelog_date']).dt.date

# ID ë§Œë“¤ê¸° (submissionì—ì„œ ì˜ˆì¸¡í•œ ê²°ê³¼ì™€ ì—°ê²°í•˜ê¸° ìœ„í•´)
submission_final['ID'] = submission_final['subject_id'] + '_' + submission_final['lifelog_date'].astype(str)

# ì˜ˆì¸¡ ê²°ê³¼ ì—°ê²°í•  ìˆ˜ ìˆë„ë¡ ë™ì¼í•œ ìˆœì„œë¡œ ì •ë ¬
# ë³´í†µ ì˜ˆì¸¡ ê²°ê³¼ëŠ” test_df ê¸°ì¤€ì´ë¯€ë¡œ ì •ë ¬ ë³´ì¥ë˜ì–´ì•¼ í•¨
assert len(submission_final) == len(multiclass_pred)  # shape ì²´í¬

# ë‹¤ì¤‘ ë¶„ë¥˜ ì˜ˆì¸¡ ë¶™ì´ê¸°
submission_final['S1'] = multiclass_pred

# ì´ì§„ ë¶„ë¥˜ ê²°ê³¼ ë¶™ì´ê¸°
for col in ['Q1', 'Q2', 'Q3', 'S2', 'S3']:
    submission_final[col] = binary_preds[col].astype(int)  # í™•ë¥  ì•„ë‹Œ class ì˜ˆì¸¡

# ìµœì¢… ì œì¶œ í˜•ì‹ ì •ë ¬
submission_final = submission_final[['subject_id', 'sleep_date', 'lifelog_date', 'Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']]

# ì €ì¥
submission_final.to_csv("submission_final.csv", index=False)

from google.colab import files
files.download("submission_final.csv")