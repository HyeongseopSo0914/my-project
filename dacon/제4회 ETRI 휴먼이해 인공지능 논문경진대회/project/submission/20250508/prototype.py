# -*- coding: utf-8 -*-
"""prototype.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bYRUFnay1zYTPG_IYTpgEYZa__58snD6
"""

!pip install pyarrow
!pip install fastparquet

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import glob
import random
import os
import matplotlib.pyplot as plt
import seaborn as sns
import ast

import warnings
warnings.filterwarnings('ignore')

from lightgbm import LGBMClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import f1_score

# seed Í≥†Ï†ï
SD = 42
random.seed(SD)
np.random.seed(SD)
os.environ['PYTHONHASHSEED'] = str(SD)

"""Data Process"""

# ÌååÏùº Í≤ΩÎ°ú ÏÑ§Ï†ï
data_dir = '/content/drive/MyDrive/ETRI/ETRI_lifelog_dataset/ch2025_data_items'

# Parquet ÌååÏùº Ï†ÑÏ≤¥ Í≤ΩÎ°ú Î¶¨Ïä§Ìä∏
parquet_files = glob.glob(os.path.join(data_dir, 'ch2025_*.parquet'))

# !ls /content/drive/MyDrive
# !ls /content/drive/MyDrive/ETRI
!ls /content/drive/MyDrive/ETRI/ETRI_lifelog_dataset/ch2025_data_items

print("üîç data_dir exists:", os.path.exists(data_dir))
print("üîç Files found:", parquet_files)

# ÌååÏùº Ïù¥Î¶ÑÏùÑ ÌÇ§Î°ú, DataFrameÏùÑ Í∞íÏúºÎ°ú Ï†ÄÏû•Ìï† ÎîïÏÖîÎÑàÎ¶¨
lifelog_data = {}

# ÌååÏùºÎ≥ÑÎ°ú ÏùΩÍ∏∞
for file_path in parquet_files:
    name = os.path.basename(file_path).replace('.parquet', '').replace('ch2025_', '')
    lifelog_data[name] = pd.read_parquet(file_path)
    print(f"‚úÖ Loaded: {name}, shape = {lifelog_data[name].shape}")

# ÎîïÏÖîÎÑàÎ¶¨Ïóê ÏûàÎäî Î™®Îì† Ìï≠Î™©ÏùÑ ÎèÖÎ¶ΩÏ†ÅÏù∏ Î≥ÄÏàòÎ°ú Ìï†Îãπ
for key, df in lifelog_data.items():
    globals()[f"{key}_df"] = df

# lifelog_dateÍ∞Ä timestampÎûë Í∞ôÎã§
metrics_train = pd.read_csv('/content/drive/MyDrive/ETRI/ETRI_lifelog_dataset/ch2025_metrics_train.csv')
sample_submission = pd.read_csv('/content/drive/MyDrive/ETRI/ETRI_lifelog_dataset/ch2025_submission_sample.csv')

metrics_train.head()
print('-----------------')
sample_submission.head()

"""Î™®Îì† Í∞íÏùÄ Î≤îÏ£ºÌòï ÌÅ¥ÎûòÏä§ ÏßëÌï© --- Î∂ÑÎ•ò Î¨∏Ï†ú


"""

# ‚úÖ Í∏∞Ï§Ä Ïåç (subject_id, lifelog_date)
sample_submission['lifelog_date'] = pd.to_datetime(sample_submission['lifelog_date'])
test_keys = set(zip(sample_submission['subject_id'], sample_submission['lifelog_date'].dt.date))

# ‚úÖ DataFrame Î≥Ñ timestamp Ïª¨Îüº ÏàòÎèô ÏßÄÏ†ï
dataframes = {
    'mACStatus': (mACStatus_df, 'timestamp'),
    'mActivity': (mActivity_df, 'timestamp'),
    'mAmbience': (mAmbience_df, 'timestamp'),
    'mBle': (mBle_df, 'timestamp'),
    'mGps': (mGps_df, 'timestamp'),
    'mLight': (mLight_df, 'timestamp'),
    'mScreenStatus': (mScreenStatus_df, 'timestamp'),
    'mUsageStats': (mUsageStats_df, 'timestamp'),
    'mWifi': (mWifi_df, 'timestamp'),
    'wHr': (wHr_df, 'timestamp'),
    'wLight': (wLight_df, 'timestamp'),
    'wPedo': (wPedo_df, 'timestamp'),
}

# ‚úÖ Î∂ÑÎ¶¨ Ìï®Ïàò
# test_df = Ï†úÏ∂ú ÎåÄÏÉÅ + ÎÇ†ÏßúÏôÄ ÏùºÏπòÌïòÎäî Îç∞Ïù¥ÌÑ∞Îßå Ìè¨Ìï®
# train_df = ÎÇòÎ®∏ÏßÄ Î™®Îì† Îç∞Ïù¥ÌÑ∞ ( Î™®Îç∏ÌïôÏäµÏóê ÏÇ¨Ïö©Îê† Í≤É )
def split_test_train(df, subject_col='subject_id', timestamp_col='timestamp'):
    df[timestamp_col] = pd.to_datetime(df[timestamp_col], errors='coerce')
    df = df.dropna(subset=[timestamp_col])
    df['date_only'] = df[timestamp_col].dt.date
    df['key'] = list(zip(df[subject_col], df['date_only']))

    test_df = df[df['key'].isin(test_keys)].drop(columns=['date_only', 'key'])
    train_df = df[~df['key'].isin(test_keys)].drop(columns=['date_only', 'key'])
    return test_df, train_df

# ‚úÖ Í≤∞Í≥º Ï†ÄÏû•
# Î™®Îì† df Î∞òÎ≥µÌïòÎ©¥ÏÑú train/test Î≥ÑÎèÑ Î≥ÄÏàò Ï†ÄÏû•
for name, (df, ts_col) in dataframes.items():
    print(f"‚è≥ {name} Î∂ÑÎ¶¨ Ï§ë...")
    test_df, train_df = split_test_train(df.copy(), subject_col='subject_id', timestamp_col=ts_col)
    globals()[f"{name}_test"] = test_df
    globals()[f"{name}_train"] = train_df
    print(f"‚úÖ {name}_test ‚Üí {test_df.shape}, {name}_train ‚Üí {train_df.shape}")

"""ÌòÑÏû¨ÍπåÏßÄ Ìïú Í≤É
1. Íµ¨Í∏ÄÎìúÎùºÏù¥Î∏å ÌååÏùº ÏûÖÏ∂úÎ†• ÌôïÏù∏
2. ÌååÏùº Ïù¥Î¶Ñ=key, DataFrame=value Î°ú ÎîïÏÖîÎÑàÎ¶¨ Ï†ÄÏû•
3. ÎîïÏÖîÎÑàÎ¶¨ÏóêÏûàÎäî Î™®Îì† Í±∏ Í∞Å Î≥ÄÏàòÏóê Ï†ÄÏû•
4. ÌïôÏäµÏö©csv, Ï†úÏ∂úÏö©csv ÏùΩÍ∏∞
5. ÌïôÏäµÏö©data, ÌÖåÏä§Ìä∏data Î∂ÑÎ¶¨ Î∞è Ï†ÄÏû•

ÏïûÏúºÎ°ú Ìï¥Ïïº Îê† Í≤É
1. Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨ ( ÌïòÎ£® Í∏∞Ï§Ä )
-----
1. Îç∞Ïù¥ÌÑ∞ ÌôïÏù∏
2. Î™©Ìëú ÌôïÏù∏ -- ÏÑ±Í≥µÎ•† ÏòàÏ∏°
3. Í≤∞Ï∏°Ïπò ÌôïÏù∏ Î∞è Ï≤òÎ¶¨
4. EDA (ÏãúÍ∞ÅÌôî Ìè¨Ìï®Ìïú ÌÉêÏÉâÏ†Å Î∂ÑÏÑù)
5. Ïª¨ÎüºÎ≥Ñ Î∂ÑÏÑù + Ïù¥ÏÉÅÏπò ÌÉêÏÉâ
6. Îç∞Ïù¥ÌÑ∞ ÎùºÎ≤®ÎßÅ Î∞è Î∂àÎ¶¨Ïñ∏Ìôî
7. ÌïÑÏöîÏóÜÎäî Ïª¨Îüº Ï†úÍ±∞
8. Feature Engineering (ÌååÏÉùÎ≥ÄÏàò ÏÉùÏÑ±)
9. Feature Importance / ÏÉÅÍ¥Ä Î∂ÑÏÑù
10. Í≤ÄÏ¶ù Ï†ÑÎûµ ÏÑ§Ï†ï (KFold Îì±)
11. Î™®Îç∏ ÏÑ†ÌÉù Î∞è ÌïôÏäµ
12. ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÌäúÎãù
13. ÏµúÏ¢Ö Î™®Îç∏ ÏÑ†Ï†ï
14. Í≤∞Í≥º ÏòàÏ∏° Î∞è Ï†ÄÏû•
15. ÌèâÍ∞Ä ÏßÄÌëú ÌôïÏù∏ Î∞è ÎåÄÌöå Ï†úÏ∂ú
--
ÎÑàÎ¨¥ Ï¢ãÏäµÎãàÎã§. ÏÑºÏÑú Îç∞Ïù¥ÌÑ∞Îäî ÏõêÎûò ÏãúÍ≥ÑÏó¥Ïù¥ÏßÄÎßå,
‚Üí Î®∏Ïã†Îü¨ÎãùÏóê ÎÑ£Í∏∞ ÏúÑÌï¥ÏÑ† Î≥¥ÌÜµ "ÌïòÎ£® Îã®ÏúÑÎ°ú ÏöîÏïΩ"Ìï©ÎãàÎã§.

Ïù¥ÎØ∏ ÏòàÏãúÍ∞Ä Ï§ÄÎπÑÎêú ÏÑºÏÑú:
mACStatus ‚Üí Ï∂©Ï†Ñ ÎπÑÏú®, Ï∂©Ï†Ñ ÏßÄÏÜç ÏãúÍ∞Ñ Îì±

mActivity ‚Üí ÌôúÎèô ÎπÑÏú®, ÎåÄÌëú ÌôúÎèô Îì±

Îã§ÏùåÏúºÎ°ú Ï§ÄÎπÑÌïòÎ©¥ Ï¢ãÏùÑ ÏÑºÏÑúÎì§:
ÏÑºÏÑú Ïù¥Î¶Ñ	ÏöîÏïΩ ÌîºÏ≤ò ÏòàÏãú
wHr (Ïã¨Î∞ïÏàò)	ÌèâÍ∑†/ÏµúÎåÄ/ÏµúÏÜå/ÌëúÏ§ÄÌé∏Ï∞®, Ïã¨Î∞ï ÏÉÅÏäπ ÌöüÏàò
wPedo (Í±∏Ïùå Ïàò)	Ï¥ù Í±∏Ïùå Ïàò, ÌôúÎèô ÏãúÍ∞ÑÎåÄ, Î∞§ ÏãúÍ∞ÑÎåÄ Í±∏Ïùå Ïàò
mLight / wLight (Ï°∞ÎèÑ)	ÌèâÍ∑† Î∞ùÍ∏∞, ÏàòÎ©¥ ÏßÅÏ†Ñ Ï°∞ÎèÑ Î≥ÄÌôîÎüâ
mScreenStatus	ÌôîÎ©¥ ÏºúÏßê ÌöüÏàò, Ï¥ù ÏÇ¨Ïö© ÏãúÍ∞Ñ, Î∞§ ÏãúÍ∞ÑÎåÄ ÏÇ¨Ïö© Ïó¨Î∂Ä
mUsageStats	Ïï± ÏÇ¨Ïö© Ï¥ù ÏãúÍ∞Ñ, ÏÉÅÏúÑ Ïï± ÏÇ¨Ïö© ÎπÑÏú® Îì±
"""

def process_mACStatus(df):
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['date'] = df['timestamp'].dt.date
    df = df.sort_values(['subject_id', 'timestamp'])

    results = []

    for (subj, date), group in df.groupby(['subject_id', 'date']):
        status = group['m_charging'].values  # 0/1 ÏÉÅÌÉú
        times = group['timestamp'].values

        # Ï∂©Ï†Ñ ÏÉÅÌÉú ÎπÑÏú®
        ratio_charging = status.mean()

        # ÏÉÅÌÉú Ï†ÑÏù¥ ÌöüÏàò
        transitions = (status[1:] != status[:-1]).sum()

        # Ïó∞ÏÜçÎêú 1 ÏÉÅÌÉú Í∏∏Ïù¥Îì§
        lengths = []
        current_len = 0
        for val in status:
            if val == 1:
                current_len += 1
            elif current_len > 0:
                lengths.append(current_len)
                current_len = 0
        if current_len > 0:
            lengths.append(current_len)

        avg_charging_duration = np.mean(lengths) if lengths else 0
        max_charging_duration = np.max(lengths) if lengths else 0

        results.append({
            'subject_id': subj,
            'date': date,
            'charging_ratio': ratio_charging,
            'charging_transitions': transitions,
            'avg_charging_duration': avg_charging_duration,
            'max_charging_duration': max_charging_duration,
        })

    return pd.DataFrame(results)

mACStatus_df2 = process_mACStatus(mACStatus_df)

def process_mActivity(df):
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['date'] = df['timestamp'].dt.date

    summary = []

    for (subj, date), group in df.groupby(['subject_id', 'date']):
        counts = group['m_activity'].value_counts(normalize=True)  # ÎπÑÏú®
        row = {'subject_id': subj, 'date': date}

        # 0~8 ÎπÑÏú® Ï†ÄÏû•
        for i in range(9):
            row[f'activity_{i}_ratio'] = counts.get(i, 0)

        # Ï£ºÏöî ÌôúÎèô Ï†ïÎ≥¥
        row['dominant_activity'] = group['m_activity'].mode()[0]
        row['num_unique_activities'] = group['m_activity'].nunique()

        summary.append(row)

    return pd.DataFrame(summary)

mActivity_df2 = process_mActivity(mActivity_df)

# ÏßÄÏ†ïÎêú 10Í∞ú ÎùºÎ≤®
top_10_labels = [
    "Inside, small room", "Speech", "Silence", "Music",
    "Narration, monologue", "Child speech, kid speaking",
    "Conversation", "Speech synthesizer", "Shout", "Babbling"
]

def process_mAmbience_top10(df):
    df = df.copy()
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['date'] = df['timestamp'].dt.date

    # Ï¥àÍ∏∞Ìôî
    for label in top_10_labels + ['others']:
        df[label] = 0.0

    for idx, row in df.iterrows():
        parsed = ast.literal_eval(row['m_ambience']) if isinstance(row['m_ambience'], str) else row['m_ambience']
        others_prob = 0.0

        for label, prob in parsed:
            prob = float(prob)
            if label in top_10_labels:
                df.at[idx, label] = prob
            else:
                others_prob += prob

        df.at[idx, 'others'] = others_prob

    return df.drop(columns=['m_ambience'])

mAmbience_df2= process_mAmbience_top10(mAmbience_df)

def summarize_mAmbience_daily(df):
    prob_cols = [col for col in df.columns if col not in ['subject_id', 'timestamp', 'date']]

    # ÌïòÎ£® Îã®ÏúÑÎ°ú ÌèâÍ∑†Í∞í ÏöîÏïΩ
    daily_summary = df.groupby(['subject_id', 'date'])[prob_cols].mean().reset_index()
    return daily_summary

mAmbience_df2 = summarize_mAmbience_daily(mAmbience_df2)

def process_mBle(df):
    df = df.copy()
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['date'] = df['timestamp'].dt.date

    features = []

    for idx, row in df.iterrows():
        entry = ast.literal_eval(row['m_ble']) if isinstance(row['m_ble'], str) else row['m_ble']

        rssi_list = []
        class_0_cnt = 0
        class_other_cnt = 0

        for device in entry:
            try:
                rssi = int(device['rssi'])
                rssi_list.append(rssi)

                if str(device['device_class']) == '0':
                    class_0_cnt += 1
                else:
                    class_other_cnt += 1
            except:
                continue  # malformed record

        feature = {
            'subject_id': row['subject_id'],
            'date': row['date'],
            'device_class_0_cnt': class_0_cnt,
            'device_class_others_cnt': class_other_cnt,
            'device_count': len(rssi_list),
            'rssi_mean': np.mean(rssi_list) if rssi_list else np.nan,
            'rssi_min': np.min(rssi_list) if rssi_list else np.nan,
            'rssi_max': np.max(rssi_list) if rssi_list else np.nan,
        }
        features.append(feature)

    return pd.DataFrame(features)

def summarize_mBle_daily(df):
    # row Îã®ÏúÑ BLE feature Ï∂îÏ∂ú
    df = process_mBle(df)

    # ÌïòÎ£® Îã®ÏúÑÎ°ú cnt Ìï©ÏπòÍ∏∞
    grouped = df.groupby(['subject_id', 'date']).agg({
        'device_class_0_cnt': 'sum',
        'device_class_others_cnt': 'sum',
        'rssi_mean': 'mean',
        'rssi_min': 'min',
        'rssi_max': 'max',
    }).reset_index()

    # Ï¥ùÌï© Íµ¨Ìï¥ÏÑú ÎπÑÏú® Í≥ÑÏÇ∞
    total_cnt = grouped['device_class_0_cnt'] + grouped['device_class_others_cnt']
    grouped['device_class_0_ratio'] = grouped['device_class_0_cnt'] / total_cnt.replace(0, np.nan)
    grouped['device_class_others_ratio'] = grouped['device_class_others_cnt'] / total_cnt.replace(0, np.nan)

    # ÌïÑÏöî ÏóÜÎäî ÏõêÎûò cnt Ïª¨Îüº Ï†úÍ±∞
    grouped.drop(columns=['device_class_0_cnt', 'device_class_others_cnt'], inplace=True)

    return grouped

mBle_df2 = summarize_mBle_daily(mBle_df)

def process_mGps(df):
    df = df.copy()
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['date'] = df['timestamp'].dt.date

    features = []

    for idx, row in df.iterrows():
        gps_list = ast.literal_eval(row['m_gps']) if isinstance(row['m_gps'], str) else row['m_gps']

        altitudes = []
        latitudes = []
        longitudes = []
        speeds = []

        for entry in gps_list:
            try:
                altitudes.append(float(entry['altitude']))
                latitudes.append(float(entry['latitude']))
                longitudes.append(float(entry['longitude']))
                speeds.append(float(entry['speed']))
            except:
                continue

        features.append({
            'subject_id': row['subject_id'],
            'date': row['date'],
            'altitude_mean': np.mean(altitudes) if altitudes else np.nan,
            'latitude_std': np.std(latitudes) if latitudes else np.nan,
            'longitude_std': np.std(longitudes) if longitudes else np.nan,
            'speed_mean': np.mean(speeds) if speeds else np.nan,
            'speed_max': np.max(speeds) if speeds else np.nan,
            'speed_std': np.std(speeds) if speeds else np.nan,
        })

    return pd.DataFrame(features)

m_Gps_df2 = process_mGps(mGps_df)

m_Gps_df2 = m_Gps_df2.groupby(['subject_id', 'date']).agg({
    'altitude_mean': 'mean',
    'latitude_std': 'mean',
    'longitude_std': 'mean',
    'speed_mean': 'mean',
    'speed_max': 'max',
    'speed_std': 'mean'
}).reset_index()

def process_mLight(df):
    df = df.copy()
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['date'] = df['timestamp'].dt.date
    df['hour'] = df['timestamp'].dt.hour

    # Î∞§(22~05Ïãú), ÎÇÆ(06~21Ïãú) Íµ¨Î∂Ñ
    df['is_night'] = df['hour'].apply(lambda h: h >= 22 or h < 6)

    # ÌïòÎ£® Îã®ÏúÑ ÏöîÏïΩ
    daily = df.groupby(['subject_id', 'date']).agg(
        light_mean=('m_light', 'mean'),
        light_std=('m_light', 'std'),
        light_max=('m_light', 'max'),
        light_min=('m_light', 'min'),
        light_night_mean=('m_light', lambda x: x[df.loc[x.index, 'is_night']].mean()),
        light_day_mean=('m_light', lambda x: x[~df.loc[x.index, 'is_night']].mean()),
        light_night_ratio=('is_night', 'mean')  # Î∞§ ÏãúÍ∞Ñ Ï∏°Ï†ï ÎπÑÏú®
    ).reset_index()

    return daily

mLight_df2 = process_mLight(mLight_df)

def process_mScreenStatus(df):
    df = df.copy()
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['date'] = df['timestamp'].dt.date

    features = []

    for (subj, date), group in df.groupby(['subject_id', 'date']):
        status = group['m_screen_use'].values
        ratio_on = status.mean()
        transitions = (status[1:] != status[:-1]).sum()

        # Ïó∞ÏÜçÎêú 1 ÏÉÅÌÉú Í∏∏Ïù¥Îì§
        durations = []
        current = 0
        for val in status:
            if val == 1:
                current += 1
            elif current > 0:
                durations.append(current)
                current = 0
        if current > 0:
            durations.append(current)

        features.append({
            'subject_id': subj,
            'date': date,
            'screen_on_ratio': ratio_on,
            'screen_on_transitions': transitions,
            'screen_on_duration_avg': np.mean(durations) if durations else 0,
            'screen_on_duration_max': np.max(durations) if durations else 0,
        })

    return pd.DataFrame(features)

mScreenStatus_df2 = process_mScreenStatus(mScreenStatus_df)

top_apps = [
    'One UI Ìôà', 'Ïπ¥Ïπ¥Ïò§ÌÜ°', 'ÏãúÏä§ÌÖú UI', 'NAVER', 'Ï∫êÏãúÏõåÌÅ¨', 'ÏÑ±Í≤ΩÏùºÎèÖQ',
    'YouTube', 'ÌÜµÌôî', 'Î©îÏãúÏßÄ', 'ÌÉÄÏûÑÏä§ÌîÑÎ†àÎìú', 'Instagram']

def process_mUsageStats(df):
    df = df.copy()
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['date'] = df['timestamp'].dt.date

    features = []

    for (subj, date), group in df.groupby(['subject_id', 'date']):
        app_time = {app: 0 for app in top_apps}
        others_time = 0

        for row in group['m_usage_stats']:
            parsed = ast.literal_eval(row) if isinstance(row, str) else row
            for entry in parsed:
                app = entry.get('app_name')
                time = entry.get('total_time', 0)
                if app in top_apps:
                    app_time[app] += int(time)
                else:
                    others_time += int(time)

        feature = {
            'subject_id': subj,
            'date': date,
            'others_time': others_time
        }
        # Í∞Å Ïï±Î≥Ñ Ïª¨Îüº Ï∂îÍ∞Ä
        feature.update({f'{app}_time': app_time[app] for app in top_apps})

        features.append(feature)

    return pd.DataFrame(features)

mUsageStats_df2 = process_mUsageStats(mUsageStats_df)

def process_mWifi(df):
    df = df.copy()
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['date'] = df['timestamp'].dt.date

    results = []

    for (subj, date), group in df.groupby(['subject_id', 'date']):
        rssi_all = []

        for row in group['m_wifi']:
            parsed = ast.literal_eval(row) if isinstance(row, str) else row
            for ap in parsed:
                try:
                    rssi = int(ap['rssi'])
                    rssi_all.append(rssi)
                except:
                    continue

        results.append({
            'subject_id': subj,
            'date': date,
            'wifi_rssi_mean': np.mean(rssi_all) if rssi_all else np.nan,
            'wifi_rssi_min': np.min(rssi_all) if rssi_all else np.nan,
            'wifi_rssi_max': np.max(rssi_all) if rssi_all else np.nan,
            'wifi_detected_cnt': len(rssi_all)
        })

    return pd.DataFrame(results)

mWifi_df2 = process_mWifi(mWifi_df)

def get_time_block(hour):
    if 0 <= hour < 6:
        return 'early_morning'
    elif 6 <= hour < 12:
        return 'morning'
    elif 12 <= hour < 18:
        return 'afternoon'
    else:
        return 'evening'

def process_wHr_by_timeblock(df):
    df = df.copy()
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['date'] = df['timestamp'].dt.date
    df['block'] = df['timestamp'].dt.hour.map(get_time_block)

    results = []

    for (subj, date), group in df.groupby(['subject_id', 'date']):
        block_stats = {'subject_id': subj, 'date': date}

        for block, block_group in group.groupby('block'):
            hr_all = []
            for row in block_group['heart_rate']:
                parsed = ast.literal_eval(row) if isinstance(row, str) else row
                hr_all.extend([int(h) for h in parsed if h is not None])

            if not hr_all:
                continue

            above_100 = [hr for hr in hr_all if hr > 100]
            block_stats[f'hr_{block}_mean'] = np.mean(hr_all)
            block_stats[f'hr_{block}_std'] = np.std(hr_all)
            block_stats[f'hr_{block}_max'] = np.max(hr_all)
            block_stats[f'hr_{block}_min'] = np.min(hr_all)
            block_stats[f'hr_{block}_above_100_ratio'] = len(above_100) / len(hr_all)

        results.append(block_stats)

    return pd.DataFrame(results)

wHr_df2 = process_wHr_by_timeblock(wHr_df)

def get_time_block(hour):
    if 0 <= hour < 6:
        return 'early_morning'
    elif 6 <= hour < 12:
        return 'morning'
    elif 12 <= hour < 18:
        return 'afternoon'
    else:
        return 'evening'

def process_wLight_by_timeblock(df):
    df = df.copy()
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['date'] = df['timestamp'].dt.date
    df['block'] = df['timestamp'].dt.hour.map(get_time_block)

    results = []

    for (subj, date), group in df.groupby(['subject_id', 'date']):
        block_stats = {'subject_id': subj, 'date': date}

        for block, block_group in group.groupby('block'):
            lux = block_group['w_light'].dropna().values
            if len(lux) == 0:
                continue

            block_stats[f'wlight_{block}_mean'] = np.mean(lux)
            block_stats[f'wlight_{block}_std'] = np.std(lux)
            block_stats[f'wlight_{block}_max'] = np.max(lux)
            block_stats[f'wlight_{block}_min'] = np.min(lux)

        results.append(block_stats)

    return pd.DataFrame(results)

wLight_df2 = process_wLight_by_timeblock(wLight_df)

def process_wPedo(df):
    df = df.copy()
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['date'] = df['timestamp'].dt.date

    summary = df.groupby(['subject_id', 'date']).agg({
        'step': 'sum',
        'step_frequency': 'mean',
        'distance': 'sum',
        'speed': ['mean', 'max'],
        'burned_calories': 'sum'
    }).reset_index()

    # Ïª¨Îüº Ïù¥Î¶Ñ Ï†ïÎ¶¨
    summary.columns = ['subject_id', 'date',
                       'step_sum', 'step_frequency_mean',
                       'distance_sum', 'speed_mean', 'speed_max',
                       'burned_calories_sum']

    return summary

wPedo_df2 = process_wPedo(wPedo_df)

"""# ÏÉà ÏÑπÏÖò"""

from functools import reduce

df_list = [
    mACStatus_df2,
    mActivity_df2,
    mAmbience_df2,
    mBle_df2,
    m_Gps_df2,
    mLight_df2,
    mScreenStatus_df2,
    mUsageStats_df2,
    mWifi_df2,
    wHr_df2,
    wHr_df2,
    wLight_df2,
    wPedo_df2
]

merged_df = reduce(lambda left, right: pd.merge(left, right, on=['subject_id', 'date'], how='outer'), df_list)

# metrics_trainÏùò lifelog_date ‚Üí datetime.date ÌòïÏúºÎ°ú Î≥ÄÌôò
metrics_train['lifelog_date'] = pd.to_datetime(metrics_train['lifelog_date']).dt.date

# merged_dfÏùò dateÎèÑ Î≥ÄÌôò
merged_df['date'] = pd.to_datetime(merged_df['date']).dt.date

# 1. date Í∏∞Ï§Ä Ï†ïÎ†¨ÏùÑ ÏúÑÌï¥ metrics_trainÏùò lifelog_date -> dateÎ°ú ÎßûÏ∂îÍ∏∞
metrics_train_renamed = metrics_train.rename(columns={'lifelog_date': 'date'})

# 2. train_df: metrics_trainÍ≥º ÏùºÏπòÌïòÎäî (subject_id, date) ‚Üí ÎùºÎ≤® Ìè¨Ìï®
train_df = pd.merge(metrics_train_renamed, merged_df, on=['subject_id', 'date'], how='inner')

# 3. test_df: metrics_trainÏóê ÏóÜÎäî (subject_id, date)
merged_keys = merged_df[['subject_id', 'date']]
train_keys = metrics_train_renamed[['subject_id', 'date']]
test_keys = pd.merge(merged_keys, train_keys, on=['subject_id', 'date'], how='left', indicator=True)
test_keys = test_keys[test_keys['_merge'] == 'left_only'].drop(columns=['_merge'])

test_df = pd.merge(test_keys, merged_df, on=['subject_id', 'date'], how='left')

"""Ï†ÑÏ≤òÎ¶¨Îêú Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû•"""

# Ï†ÄÏû•Ìï† Í≤ΩÎ°ú (Google Drive ÏïàÏóê ÏûàÎäî Ìè¥Îçî)
save_dir = "/content/drive/MyDrive/ETRI/processed_data"
os.makedirs(save_dir, exist_ok=True)

train_df.to_csv(f"{save_dir}/train_df.csv", index=False, encoding='utf-8-sig')
test_df.to_csv(f"{save_dir}/test_df.csv", index=False, encoding='utf-8-sig')
merged_df.to_csv(f"{save_dir}/merged_df.csv", index=False, encoding='utf-8-sig')
metrics_train.to_csv(f"{save_dir}/metrics_train.csv", index=False, encoding='utf-8-sig')
sample_submission.to_csv(f"{save_dir}/submission_sample.csv", index=False, encoding='utf-8-sig')


# Feather (Îπ†Î¶Ñ, Ïã§Ï†ú ÏûëÏóÖÏö© Ï∂îÏ≤ú)
train_df.to_feather(f"{save_dir}/train_df.feather")
test_df.to_feather(f"{save_dir}/test_df.feather")
merged_df.to_feather(f"{save_dir}/merged_df.feather")

# ÎùºÎ≤® Î∞è ÏÉòÌîå Ï†úÏ∂ú ÌååÏùºÎèÑ Í∞ôÏù¥ Ï†ÄÏû•
metrics_train.to_csv(f"{save_dir}/metrics_train.csv", index=False)
sample_submission.to_csv(f"{save_dir}/submission_sample.csv", index=False)

print("‚úÖ Î™®Îì† ÌååÏùº Ï†ÄÏû• ÏôÑÎ£å")

"""Î∂àÎü¨Ïò§Í∏∞"""

# Ï†ÄÏû•ÌñàÎçò Í≤ΩÎ°ú
load_dir = "/content/drive/MyDrive/ETRI/processed_data"

# FeatherÎ°ú Îπ†Î•¥Í≤å Î∂àÎü¨Ïò§Í∏∞
train_df = pd.read_feather(f"{load_dir}/train_df.feather")
test_df = pd.read_feather(f"{load_dir}/test_df.feather")
merged_df = pd.read_feather(f"{load_dir}/merged_df.feather")

# CSVÎ°ú Ï†ÄÏû•Îêú ÎùºÎ≤®/Ï†úÏ∂ú ÌååÏùº
metrics_train = pd.read_csv(f"{load_dir}/metrics_train.csv")
submission_sample = pd.read_csv(f"{load_dir}/submission_sample.csv")

print("‚úÖ Î∂àÎü¨Ïò§Í∏∞ ÏôÑÎ£å:", train_df.shape, test_df.shape)

"""Î™®Îç∏ÌÖåÏä§Ìä∏"""

# ‚úÖ ÌÉÄÍ≤ü Î¶¨Ïä§Ìä∏
targets_binary = ['Q1', 'Q2', 'Q3', 'S2', 'S3']
target_multiclass = 'S1'

# ‚úÖ feature Ï§ÄÎπÑ
X = train_df.drop(columns=['subject_id', 'sleep_date', 'date', 'Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3'])
X.fillna(0, inplace=True)  # Í≤∞Ï∏°Í∞í Ï≤òÎ¶¨

test_X = test_df.drop(columns=['subject_id', 'date'])
test_X.fillna(0, inplace=True)

# Ïª¨Îüº Ïù¥Î¶ÑÏóêÏÑú ÌäπÏàò Î¨∏Ïûê Ï†úÍ±∞/Î≥ÄÌôò
def sanitize_column_names(df):
    df.columns = (
        df.columns
        .str.replace(r"[^\w]", "_", regex=True)  # ÌäπÏàòÎ¨∏Ïûê ‚Üí _
        .str.replace(r"__+", "_", regex=True)    # Ïó∞ÏÜçÎêú _ Ï†úÍ±∞
        .str.strip("_")                          # ÏïûÎí§ _ Ï†úÍ±∞
    )
    return df

# Î™®Îì† ÏûÖÎ†•Ïóê Ï†ÅÏö©
X = sanitize_column_names(X)
test_X = sanitize_column_names(test_X)

# Í≤∞Í≥º Ï†ÄÏû•
binary_preds = {}
multiclass_pred = None

common_params = {
    'n_estimators': 1000,
    'learning_rate': 0.03,
    'random_state': 42,
    'n_jobs': -1,
    'verbosity': -1
}

# Ïù¥ÏßÑ Î∂ÑÎ•ò ÌïôÏäµ
for col in targets_binary:
    y = train_df[col]
    model = LGBMClassifier(**common_params)
    model.fit(X, y)
    binary_preds[col] = model.predict(test_X)  # üî• ÌôïÎ•†X, ÌÅ¥ÎûòÏä§ ÏßÅÏ†ë ÏòàÏ∏°

# Îã§Ï§ë Î∂ÑÎ•ò ÌïôÏäµ (S1)
y_multi = train_df['S1']
model_s1 = LGBMClassifier(**common_params, objective='multiclass', num_class=3)
model_s1.fit(X, y_multi)
multiclass_pred = model_s1.predict(test_X)  # üî• ÌÅ¥ÎûòÏä§ ÏßÅÏ†ë ÏòàÏ∏°

# importance Ï∂úÎ†•
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': model_s1.feature_importances_
}).sort_values('importance', ascending=False)

# ÏãúÍ∞ÅÌôî
plt.figure(figsize=(10, 10))
sns.barplot(x='importance', y='feature', data=feature_importance)
plt.title('Feature Importance')
plt.show()

# sample Í∏∞Î∞ò Ï†úÏ∂ú Ìè¨Îß∑ Í∞ÄÏ†∏Ïò§Í∏∞
submission_final = sample_submission[['subject_id', 'sleep_date', 'lifelog_date']].copy()

# lifelog_date Í∏∞Ï§ÄÏúºÎ°ú string ‚Üí date ÌòïÏãù ÌÜµÏùº
submission_final['lifelog_date'] = pd.to_datetime(submission_final['lifelog_date']).dt.date

# ID ÎßåÎì§Í∏∞ (submissionÏóêÏÑú ÏòàÏ∏°Ìïú Í≤∞Í≥ºÏôÄ Ïó∞Í≤∞ÌïòÍ∏∞ ÏúÑÌï¥)
submission_final['ID'] = submission_final['subject_id'] + '_' + submission_final['lifelog_date'].astype(str)

# ÏòàÏ∏° Í≤∞Í≥º Ïó∞Í≤∞Ìï† Ïàò ÏûàÎèÑÎ°ù ÎèôÏùºÌïú ÏàúÏÑúÎ°ú Ï†ïÎ†¨
# Î≥¥ÌÜµ ÏòàÏ∏° Í≤∞Í≥ºÎäî test_df Í∏∞Ï§ÄÏù¥ÎØÄÎ°ú Ï†ïÎ†¨ Î≥¥Ïû•ÎêòÏñ¥Ïïº Ìï®
assert len(submission_final) == len(multiclass_pred)  # shape Ï≤¥ÌÅ¨

# Îã§Ï§ë Î∂ÑÎ•ò ÏòàÏ∏° Î∂ôÏù¥Í∏∞
submission_final['S1'] = multiclass_pred

# Ïù¥ÏßÑ Î∂ÑÎ•ò Í≤∞Í≥º Î∂ôÏù¥Í∏∞
for col in ['Q1', 'Q2', 'Q3', 'S2', 'S3']:
    submission_final[col] = binary_preds[col].astype(int)  # ÌôïÎ•† ÏïÑÎãå class ÏòàÏ∏°

# ÏµúÏ¢Ö Ï†úÏ∂ú ÌòïÏãù Ï†ïÎ†¨
submission_final = submission_final[['subject_id', 'sleep_date', 'lifelog_date', 'Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']]

# Ï†ÄÏû•
submission_final.to_csv("submission_final.csv", index=False)

from google.colab import files
files.download("submission_final.csv")