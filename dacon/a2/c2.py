# ğŸ“¦ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”©
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import mean_absolute_error
from xgboost import XGBRegressor

# âœ… í•œê¸€ ì„¤ì • ë° ë§ˆì´ë„ˆìŠ¤ ê¹¨ì§ ë°©ì§€
plt.rc('font', family='Malgun Gothic')
plt.rcParams['axes.unicode_minus'] = False

# ğŸ“‚ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
train = pd.read_csv('train.csv').drop(columns=['ID'])
test = pd.read_csv('test.csv').drop(columns=['ID'])
sample_submission = pd.read_csv('sample_submission.csv')

# ğŸ”§ ë²”ìœ„ê°’ ì²˜ë¦¬ í•¨ìˆ˜
def convert_range_to_float(value):
    if isinstance(value, str) and '-' in value:
        try:
            low, high = map(float, value.split('-'))
            return (low + high) / 2
        except:
            return np.nan
    try:
        return float(value)
    except:
        return np.nan

# ğŸ”§ ë²”ì£¼í˜• ì¸ì½”ë”©
def encode_categoricals(df, cols):
    df = df.copy()
    for col in cols:
        df[col] = LabelEncoder().fit_transform(df[col].astype(str))
    return df

# ğŸ”§ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ í•¨ìˆ˜
def fill_missing_values(df, is_train=True):
    df = df.copy()

    # ë²”ìœ„ ë¬¸ìì—´ â†’ í‰ê·  ìˆ«ì ì²˜ë¦¬
    for col in ['ì—°ë§¤ì¶œ(ì–µì›)', 'ì´ íˆ¬ìê¸ˆ(ì–µì›)', 'ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)']:
        df[col] = df[col].apply(convert_range_to_float)

    # ë¶„ì•¼ ê²°ì¸¡ ë° ì¸ì½”ë”©
    if 'ë¶„ì•¼' in df.columns:
        df['ë¶„ì•¼'] = df['ë¶„ì•¼'].fillna('Unknown')
        df['ë¶„ì•¼'] = LabelEncoder().fit_transform(df['ë¶„ì•¼'])

    # êµ­ê°€, íˆ¬ìë‹¨ê³„ ì¸ì½”ë”©
    df = encode_categoricals(df, ['êµ­ê°€', 'íˆ¬ìë‹¨ê³„'])

    # âœ… ê²°ì¸¡ í”Œë˜ê·¸ ì¶”ê°€ í•¨ìˆ˜
    def add_missing_flag(column):
        flag_col = f'{column}_ê²°ì¸¡'
        df[flag_col] = df[column].isnull().astype(int)

    # âœ… í”¼ì²˜ì…‹ ìƒì„± í•¨ìˆ˜
    def get_features(base):
        return base + (['ì„±ê³µí™•ë¥ '] if is_train else [])

    # 1. ì§ì› ìˆ˜
    if 'ì§ì› ìˆ˜' in df.columns:
        add_missing_flag('ì§ì› ìˆ˜')
        features = get_features(['ì„¤ë¦½ì—°ë„', 'êµ­ê°€', 'íˆ¬ìë‹¨ê³„', 'ì—°ë§¤ì¶œ(ì–µì›)', 'ì´ íˆ¬ìê¸ˆ(ì–µì›)', 'SNS íŒ”ë¡œì›Œ ìˆ˜(ë°±ë§Œëª…)'])
        complete = df[df['ì§ì› ìˆ˜'].notnull()]
        missing = df[df['ì§ì› ìˆ˜'].isnull()]
        if not complete.empty and not missing.empty:
            model = GradientBoostingRegressor()
            model.fit(complete[features], complete['ì§ì› ìˆ˜'])
            df.loc[df['ì§ì› ìˆ˜'].isnull(), 'ì§ì› ìˆ˜'] = model.predict(missing[features])

    # 2. ê³ ê° ìˆ˜
    if 'ê³ ê°ìˆ˜(ë°±ë§Œëª…)' in df.columns:
        add_missing_flag('ê³ ê°ìˆ˜(ë°±ë§Œëª…)')
        features = get_features(['ì„¤ë¦½ì—°ë„', 'ì§ì› ìˆ˜', 'ë¶„ì•¼', 'ì—°ë§¤ì¶œ(ì–µì›)', 'ì´ íˆ¬ìê¸ˆ(ì–µì›)', 'SNS íŒ”ë¡œì›Œ ìˆ˜(ë°±ë§Œëª…)'])
        complete = df[df['ê³ ê°ìˆ˜(ë°±ë§Œëª…)'].notnull()]
        missing = df[df['ê³ ê°ìˆ˜(ë°±ë§Œëª…)'].isnull()]
        if not complete.empty and not missing.empty:
            model = GradientBoostingRegressor()
            model.fit(complete[features], complete['ê³ ê°ìˆ˜(ë°±ë§Œëª…)'])
            df.loc[df['ê³ ê°ìˆ˜(ë°±ë§Œëª…)'].isnull(), 'ê³ ê°ìˆ˜(ë°±ë§Œëª…)'] = model.predict(missing[features])

    # 3. ê¸°ì—…ê°€ì¹˜
    if 'ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)' in df.columns:
        add_missing_flag('ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)')
        features = get_features(['ì„¤ë¦½ì—°ë„', 'ì§ì› ìˆ˜', 'ê³ ê°ìˆ˜(ë°±ë§Œëª…)', 'ë¶„ì•¼', 'ì—°ë§¤ì¶œ(ì–µì›)', 'ì´ íˆ¬ìê¸ˆ(ì–µì›)', 'SNS íŒ”ë¡œì›Œ ìˆ˜(ë°±ë§Œëª…)'])
        complete = df[df['ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)'].notnull()]
        missing = df[df['ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)'].isnull()]
        if not complete.empty and not missing.empty:
            model = GradientBoostingRegressor()
            model.fit(complete[features], complete['ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)'])
            df.loc[df['ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)'].isnull(), 'ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)'] = model.predict(missing[features])

    return df

# ğŸ“Œ ë°ì´í„° ì „ì²˜ë¦¬
train_filled = fill_missing_values(train, is_train=True)
test_filled = fill_missing_values(test, is_train=False)

# ğŸ“Œ ì´ìƒì¹˜ ì²˜ë¦¬ í•¨ìˆ˜
def process_outliers(train_df, test_df, num_cols, method='flag+clip'):
    train, test = train_df.copy(), test_df.copy()
    for col in num_cols:
        Q1, Q3 = train[col].quantile([0.25, 0.75])
        IQR = Q3 - Q1
        lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR

        if 'flag' in method:
            train[f'{col}_ì´ìƒì¹˜ì—¬ë¶€'] = ((train[col] < lower) | (train[col] > upper)).astype(int)
            test[f'{col}_ì´ìƒì¹˜ì—¬ë¶€'] = ((test[col] < lower) | (test[col] > upper)).astype(int)

        if 'clip' in method:
            train[col] = train[col].clip(lower, upper)
            test[col] = test[col].clip(lower, upper)

    return train, test

num_cols = train_filled.select_dtypes(include=np.number).columns.drop('ì„±ê³µí™•ë¥ ').tolist()
train_processed, test_processed = process_outliers(train_filled, test_filled, num_cols)



def detect_outliers_summary(df, columns):
    summary = []
    for col in columns:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
        summary.append({
            'ì»¬ëŸ¼ëª…': col,
            'ì´ìƒì¹˜ ìˆ˜': len(outliers),
            'ì „ì²´ ëŒ€ë¹„ ë¹„ìœ¨(%)': round(len(outliers) / len(df) * 100, 2)
        })
    return pd.DataFrame(summary).sort_values(by='ì´ìƒì¹˜ ìˆ˜', ascending=False)

# log ë³€í™˜ (0ë³´ë‹¤ í° ê°’ë§Œ ë³€í™˜, log1pëŠ” log(1+x))
num_cols = train_filled.select_dtypes(include=['int64', 'float64']).columns.tolist()
print(num_cols)
log_train = train_filled[num_cols].copy()
for col in num_cols:
    if (log_train[col] > 0).all():  # ìŒìˆ˜, 0 ìˆëŠ” ì»¬ëŸ¼ì€ ì œì™¸
        log_train[col] = np.log1p(log_train[col])

def process_outliers_train_test(train_df, test_df, num_cols, method='flag+clip'):
    """
    train ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ IQR ì´ìƒì¹˜ íƒì§€ ê¸°ì¤€ì„ ì¡ê³ ,
    train/test ëª¨ë‘ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì´ìƒì¹˜ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
    """
    train_processed = train_df.copy()
    test_processed = test_df.copy()
    outlier_bounds = {}

    for col in num_cols:
        Q1 = train_df[col].quantile(0.25)
        Q3 = train_df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR
        outlier_bounds[col] = (lower, upper)

        # ì´ìƒì¹˜ í”Œë˜ê·¸
        if 'flag' in method:
            train_processed[f'{col}_ì´ìƒì¹˜ì—¬ë¶€'] = ((train_df[col] < lower) | (train_df[col] > upper)).astype(int)
            test_processed[f'{col}_ì´ìƒì¹˜ì—¬ë¶€'] = ((test_df[col] < lower) | (test_df[col] > upper)).astype(int)

        # í´ë¦¬í•‘
        if 'clip' in method:
            train_processed[col] = train_df[col].clip(lower, upper)
            test_processed[col] = test_df[col].clip(lower, upper)

        # ë¡œê·¸ ë³€í™˜
        if 'log' in method:
            if (train_processed[col] >= 0).all() and (test_processed[col] >= 0).all():
                train_processed[col] = np.log1p(train_processed[col])
                test_processed[col] = np.log1p(test_processed[col])
            else:
                print(f"[ê²½ê³ ] {col}ì€ log1p ë¶ˆê°€ëŠ¥ (ìŒìˆ˜ ë˜ëŠ” 0 í¬í•¨)")

    return train_processed, test_processed, outlier_bounds

# 'ì„±ê³µí™•ë¥ 'ì€ trainì—ë§Œ ìˆìœ¼ë¯€ë¡œ ì œì™¸
num_cols = train_filled.select_dtypes(include=np.number).columns.tolist()
num_cols = [col for col in num_cols if col != 'ì„±ê³µí™•ë¥ ']

# ì´ìƒì¹˜ ì²˜ë¦¬ ìˆ˜í–‰
train_processed, test_processed, bounds = process_outliers_train_test(train_filled, test_filled, num_cols, method='flag+clip')

# ê²°ê³¼ í™•ì¸: ì¼ë¶€ ì»¬ëŸ¼ì— ëŒ€í•´ ì´ìƒì¹˜ ì—¬ë¶€ í”Œë˜ê·¸ ë¶„í¬ ì¶œë ¥
for col in ['ì—°ë§¤ì¶œ(ì–µì›)', 'ê³ ê°ìˆ˜(ë°±ë§Œëª…)', 'SNS íŒ”ë¡œì›Œ ìˆ˜(ë°±ë§Œëª…)']:
    flag_col = f'{col}_ì´ìƒì¹˜ì—¬ë¶€'
    if flag_col in train_processed.columns:
        print(f"â–¶ {flag_col} - ì´ìƒì¹˜ ê°œìˆ˜(train): {train_processed[flag_col].sum()}")

# ğŸ“Œ íŒŒìƒë³€ìˆ˜ ìƒì„±
def create_features(df):
    df = df.copy()
    df['ì§ì› ìˆ˜_ë¡œê·¸'] = np.log1p(df['ì§ì› ìˆ˜'])
    df['ì—°ë§¤ì¶œ_ë¡œê·¸'] = np.log1p(df['ì—°ë§¤ì¶œ(ì–µì›)'])
    df['ì´ íˆ¬ìê¸ˆ_ë¡œê·¸'] = np.log1p(df['ì´ íˆ¬ìê¸ˆ(ì–µì›)'])
    df['ê³ ê°ìˆ˜_ì§ì›ë¹„'] = df['ê³ ê°ìˆ˜(ë°±ë§Œëª…)'] / (df['ì§ì› ìˆ˜'] + 1)
    df['ì—°ë§¤ì¶œ_ì§ì›ë¹„'] = df['ì—°ë§¤ì¶œ(ì–µì›)'] / (df['ì§ì› ìˆ˜'] + 1)
    df['íˆ¬ìëŒ€ë¹„ë§¤ì¶œ'] = df['ì—°ë§¤ì¶œ(ì–µì›)'] / (df['ì´ íˆ¬ìê¸ˆ(ì–µì›)'] + 1)
    df['SNSë‹¹ê³ ê°'] = df['ê³ ê°ìˆ˜(ë°±ë§Œëª…)'] / (df['SNS íŒ”ë¡œì›Œ ìˆ˜(ë°±ë§Œëª…)'] + 1)
    df['ê¸°ì—…ê°€ì¹˜ëŒ€ë¹„íˆ¬ì'] = df['ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)'] / (df['ì´ íˆ¬ìê¸ˆ(ì–µì›)'] + 1)
    df['ì„¤ë¦½ë…„ì°¨'] = 2025 - df['ì„¤ë¦½ì—°ë„']
    df['ì„¤ë¦½Xë‹¨ê³„'] = df['ì„¤ë¦½ë…„ì°¨'] * df['íˆ¬ìë‹¨ê³„']
    df['ë§¤ì¶œXê³ ê°ë¹„'] = df['ì—°ë§¤ì¶œ_ì§ì›ë¹„'] * df['ê³ ê°ìˆ˜_ì§ì›ë¹„']
    df['SNSë§¤ì¶œí•©'] = np.log1p(df['SNS íŒ”ë¡œì›Œ ìˆ˜(ë°±ë§Œëª…)'] + df['ì—°ë§¤ì¶œ(ì–µì›)'])
    return df

X = train_processed.copy()  # â† ì—¬ê¸°ì„œ train_processedì— ì´ìƒì¹˜ í”Œë˜ê·¸ê°€ ë“¤ì–´ìˆì–´ì•¼ í•¨
X = create_features(X)      # íŒŒìƒë³€ìˆ˜ ì¶”ê°€
X_test = test_processed.copy()
X_test = create_features(X_test)
y = train_processed['ì„±ê³µí™•ë¥ ']  # ë˜ëŠ” ë¯¸ë¦¬ ë¶„ë¦¬í•œ y ì‚¬ìš©

# ğŸ“Œ í•„ìš”ì—†ëŠ” ì»¬ëŸ¼ ì œê±°
remove_cols = ['ì§ì› ìˆ˜', 'ì—°ë§¤ì¶œ(ì–µì›)', 'ì„¤ë¦½ì—°ë„', 'ì´ íˆ¬ìê¸ˆ(ì–µì›)', 'ì§ì› ìˆ˜_ê²°ì¸¡', 'ê³ ê°ìˆ˜_ê²°ì¸¡']
X.drop(columns=[col for col in remove_cols if col in X.columns], inplace=True)
X_test.drop(columns=[col for col in remove_cols if col in X_test.columns], inplace=True)

# âœ… ì´ì§„í˜• ì¸ì½”ë”©
for col in ['ì¸ìˆ˜ì—¬ë¶€', 'ìƒì¥ì—¬ë¶€']:
    for df in [X, X_test]:
        if col in df.columns:
            df[col] = df[col].map({'No': 0, 'Yes': 1})

# 'ì„±ê³µí™•ë¥ ' ì»¬ëŸ¼ì´ ìˆë‹¤ë©´ ì‚­ì œ
if 'ì„±ê³µí™•ë¥ ' in X.columns:
    X = X.drop(columns=['ì„±ê³µí™•ë¥ '])

# y ê²°í•©
X_with_y = X.join(y.rename("ì„±ê³µí™•ë¥ "))            

# ğŸ§ª StratifiedKFold ê¸°ë°˜ êµì°¨ê²€ì¦
bins = np.linspace(0, 1, 6)
y_binned = np.digitize(y, bins)

cv_scores, test_preds = [], []
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for fold, (train_idx, val_idx) in enumerate(skf.split(X, y_binned)):
    print(f"Fold {fold+1}")
    X_tr, y_tr = X.iloc[train_idx], y.iloc[train_idx]
    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]

    model = XGBRegressor(
        n_estimators=1325,
        learning_rate=0.00375,
        max_depth=15,
        subsample=0.58315,
        colsample_bytree=0.75715,
        random_state=42,
        n_jobs=-1
    )
    model.fit(X_tr, y_tr)
    y_pred = model.predict(X_val)
    mae = mean_absolute_error(y_val, y_pred)
    print(f"  MAE: {mae:.5f}")

    cv_scores.append(mae)
    #models.append(model)
    test_preds.append(model.predict(X_test))

# ê²°ê³¼ ì¶œë ¥ ë° ì €ì¥
print(f"\ní‰ê·  MAE: {np.mean(cv_scores):.5f}")
final_test_pred = np.mean(test_preds, axis=0)
sample_submission['ì„±ê³µí™•ë¥ '] = final_test_pred
sample_submission.to_csv('submission_xgb_kfold_c.csv', index=False, encoding='utf-8-sig')
print("ğŸ“ ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ!")