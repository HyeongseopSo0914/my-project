{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d541dac",
   "metadata": {},
   "source": [
    "data load, preprocess, import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e22804e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 데이터 로딩\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# ID 제거\n",
    "train.drop(columns=['ID'], inplace=True)\n",
    "test.drop(columns=['ID'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93274781",
   "metadata": {},
   "source": [
    "결측 처리 및 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "104951a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "설립연도              0\n",
       "국가                0\n",
       "분야                0\n",
       "투자단계              0\n",
       "직원 수              0\n",
       "인수여부              0\n",
       "상장여부              0\n",
       "고객수(백만명)          0\n",
       "총 투자금(억원)         0\n",
       "연매출(억원)           0\n",
       "SNS 팔로워 수(백만명)    0\n",
       "기업가치(백억원)         0\n",
       "직원 수_결측           0\n",
       "고객수(백만명)_결측       0\n",
       "기업가치(백억원)_결측      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 기업가치(백억원) 숫자화\n",
    "# 기업가치 컬럼도 범위 문자열 처리 추가\n",
    "\n",
    "def convert_range_to_float(value):\n",
    "    if isinstance(value, str) and '-' in value:\n",
    "        try:\n",
    "            low, high = map(float, value.split('-'))\n",
    "            return (low + high) / 2\n",
    "        except:\n",
    "            return np.nan\n",
    "    try:\n",
    "        return float(value)\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "def encode_categoricals(df, cols):\n",
    "    df = df.copy()\n",
    "    for col in cols:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "    return df\n",
    "\n",
    "def fill_missing_values_v3(df, is_train=True):\n",
    "    df = df.copy()\n",
    "\n",
    "    # 범위 문자열 → 평균 숫자 처리\n",
    "    for col in ['연매출(억원)', '총 투자금(억원)', '기업가치(백억원)']:\n",
    "        df[col] = df[col].apply(convert_range_to_float)\n",
    "\n",
    "    # 분야 결측 및 인코딩\n",
    "    if '분야' in df.columns:\n",
    "        df['분야'] = df['분야'].fillna('Unknown')\n",
    "        df['분야'] = LabelEncoder().fit_transform(df['분야'])\n",
    "\n",
    "    # 국가, 투자단계 인코딩\n",
    "    df = encode_categoricals(df, ['국가', '투자단계'])\n",
    "\n",
    "    # ✅ 결측 플래그 추가 함수\n",
    "    def add_missing_flag(column):\n",
    "        flag_col = f'{column}_결측'\n",
    "        df[flag_col] = df[column].isnull().astype(int)\n",
    "\n",
    "    # ✅ 피처셋 생성 함수\n",
    "    def get_features(base):\n",
    "        return base + (['성공확률'] if is_train else [])\n",
    "\n",
    "    # 1. 직원 수\n",
    "    if '직원 수' in df.columns:\n",
    "        add_missing_flag('직원 수')\n",
    "        features = get_features(['설립연도', '국가', '투자단계', '연매출(억원)', '총 투자금(억원)', 'SNS 팔로워 수(백만명)'])\n",
    "        complete = df[df['직원 수'].notnull()]\n",
    "        missing = df[df['직원 수'].isnull()]\n",
    "        if not complete.empty and not missing.empty:\n",
    "            model = GradientBoostingRegressor()\n",
    "            model.fit(complete[features], complete['직원 수'])\n",
    "            df.loc[df['직원 수'].isnull(), '직원 수'] = model.predict(missing[features])\n",
    "\n",
    "    # 2. 고객 수\n",
    "    if '고객수(백만명)' in df.columns:\n",
    "        add_missing_flag('고객수(백만명)')\n",
    "        features = get_features(['설립연도', '직원 수', '분야', '연매출(억원)', '총 투자금(억원)', 'SNS 팔로워 수(백만명)'])\n",
    "        complete = df[df['고객수(백만명)'].notnull()]\n",
    "        missing = df[df['고객수(백만명)'].isnull()]\n",
    "        if not complete.empty and not missing.empty:\n",
    "            model = GradientBoostingRegressor()\n",
    "            model.fit(complete[features], complete['고객수(백만명)'])\n",
    "            df.loc[df['고객수(백만명)'].isnull(), '고객수(백만명)'] = model.predict(missing[features])\n",
    "\n",
    "    # 3. 기업가치\n",
    "    if '기업가치(백억원)' in df.columns:\n",
    "        add_missing_flag('기업가치(백억원)')\n",
    "        features = get_features(['설립연도', '직원 수', '고객수(백만명)', '분야', '연매출(억원)', '총 투자금(억원)', 'SNS 팔로워 수(백만명)'])\n",
    "        complete = df[df['기업가치(백억원)'].notnull()]\n",
    "        missing = df[df['기업가치(백억원)'].isnull()]\n",
    "        if not complete.empty and not missing.empty:\n",
    "            model = GradientBoostingRegressor()\n",
    "            model.fit(complete[features], complete['기업가치(백억원)'])\n",
    "            df.loc[df['기업가치(백억원)'].isnull(), '기업가치(백억원)'] = model.predict(missing[features])\n",
    "\n",
    "    return df\n",
    "\n",
    "# 최종 결측치 보간 시도\n",
    "train_filled = fill_missing_values_v3(train, is_train=True)\n",
    "train_filled.isnull().sum()  # 모든 결측치가 잘 채워졌는지 확인\n",
    "\n",
    "test_filled = fill_missing_values_v3(test, is_train=False)\n",
    "test_filled.isnull().sum()  # 모든 결측치가 잘 채워졌는지 확인\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f74fc1",
   "metadata": {},
   "source": [
    "이상치 처리 및 파생변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0505dbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_outliers_train_test(train_df, test_df, num_cols, method='flag+clip'):\n",
    "    train_processed = train_df.copy()\n",
    "    test_processed = test_df.copy()\n",
    "    for col in num_cols:\n",
    "        Q1 = train_df[col].quantile(0.25)\n",
    "        Q3 = train_df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - 1.5 * IQR\n",
    "        upper = Q3 + 1.5 * IQR\n",
    "        if 'flag' in method:\n",
    "            train_processed[f'{col}_이상치여부'] = ((train_df[col] < lower) | (train_df[col] > upper)).astype(int)\n",
    "            test_processed[f'{col}_이상치여부'] = ((test_df[col] < lower) | (test_df[col] > upper)).astype(int)\n",
    "        if 'clip' in method:\n",
    "            train_processed[col] = train_df[col].clip(lower, upper)\n",
    "            test_processed[col] = test_df[col].clip(lower, upper)\n",
    "    return train_processed, test_processed\n",
    "\n",
    "num_cols = train_filled.select_dtypes(include='number').columns.difference(['성공확률']).tolist()\n",
    "train_processed, test_processed = process_outliers_train_test(train_filled, test_filled, num_cols)\n",
    "\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    df['직원 수_로그'] = np.log1p(df['직원 수'])\n",
    "    df['연매출_로그'] = np.log1p(df['연매출(억원)'])\n",
    "    df['총 투자금_로그'] = np.log1p(df['총 투자금(억원)'])\n",
    "    df['고객수_직원비'] = df['고객수(백만명)'] / (df['직원 수'] + 1)\n",
    "    df['연매출_직원비'] = df['연매출(억원)'] / (df['직원 수'] + 1)\n",
    "    df['투자대비매출'] = df['연매출(억원)'] / (df['총 투자금(억원)'] + 1)\n",
    "    df['SNS당고객'] = df['고객수(백만명)'] / (df['SNS 팔로워 수(백만명)'] + 1)\n",
    "    df['기업가치대비투자'] = df['기업가치(백억원)'] / (df['총 투자금(억원)'] + 1)\n",
    "    df['설립년차'] = 2025 - df['설립연도']\n",
    "    return df\n",
    "\n",
    "X = create_features(train_processed)\n",
    "X_test = create_features(test_processed)\n",
    "y = train_processed['성공확률']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318f70f9",
   "metadata": {},
   "source": [
    "피처 제거 + 파생변수 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e001935d",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = [\n",
    "    '직원 수_결측_이상치여부', '고객수(백만명)_결측', '고객수(백만명)_결측_이상치여부',\n",
    "    '기업가치(백억원)_결측', '기업가치(백억원)_결측_이상치여부',\n",
    "    '투자단계_이상치여부', '분야_이상치여부'\n",
    "]\n",
    "if '성공확률' in X.columns:\n",
    "    X = X.drop(columns=['성공확률'])\n",
    "X_reduced = X.drop(columns=[col for col in columns_to_remove if col in X.columns])\n",
    "X_test_reduced = X_test.drop(columns=[col for col in columns_to_remove if col in X_test.columns])\n",
    "X_test_reduced = X_test_reduced[X_reduced.columns]\n",
    "\n",
    "def add_extra_features(df):\n",
    "    df = df.copy()\n",
    "    df['총투자_직원비'] = df['총 투자금(억원)'] / (df['직원 수_로그'] + 1)\n",
    "    df['SNS당매출'] = df['연매출_로그'] / (df['SNS 팔로워 수(백만명)'] + 1)\n",
    "    #df['설립년차_제곱'] = df['설립년차'] ** 2\n",
    "    df['고객당가치'] = df['기업가치(백억원)'] / (df['고객수(백만명)'] + 1)\n",
    "    df['연매출_기업가치비'] = df['연매출_로그'] / (df['기업가치(백억원)'] + 1)\n",
    "    # 설립년차 * 투자단계\n",
    "    # if '설립년차' in df.columns and '투자단계' in df.columns:\n",
    "    #     df['설립X투자단계'] = df['설립년차'] * df['투자단계']\n",
    "    \n",
    "    # 고객수_직원비 * 연매출_직원비\n",
    "    if '고객수_직원비' in df.columns and '연매출_직원비' in df.columns:\n",
    "        df['고객X매출직원비'] = df['고객수_직원비'] * df['연매출_직원비']\n",
    "    \n",
    "    # # log(SNS 수) + log(연매출)\n",
    "    # if 'SNS 팔로워 수(백만명)' in df.columns and '연매출(억원)' in df.columns:\n",
    "    #     df['로그SNS'] = np.log1p(df['SNS 팔로워 수(백만명)'])\n",
    "    #     df['로그매출'] = np.log1p(df['연매출(억원)'])\n",
    "    #     df['로그SNS+매출'] = df['로그SNS'] + df['로그매출']\n",
    "    return df\n",
    "\n",
    "X_enhanced = add_extra_features(X_reduced)\n",
    "X_test_enhanced = add_extra_features(X_test_reduced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241ebef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 13:59:12,413] A new study created in memory with name: no-name-18f29a7b-8d85-4824-87cb-f3acb07b2eb3\n",
      "[W 2025-05-01 13:59:12,492] Trial 0 failed with parameters: {'n_estimators': 559, 'learning_rate': 0.004491255333070179, 'max_depth': 10, 'subsample': 0.8903363528873041, 'colsample_bytree': 0.9740530745125215, 'reg_alpha': 3.887784360494307, 'reg_lambda': 8.003240366717359} because of the following error: ValueError('\\nAll the 3 fits failed.\\nIt is very likely that your model is misconfigured.\\nYou can try to debug the error by setting error_score=\\'raise\\'.\\n\\nBelow are more details about the failures:\\n--------------------------------------------------------------------------------\\n3 fits failed with the following error:\\nTraceback (most recent call last):\\n  File \"c:\\\\Users\\\\human\\\\.conda\\\\envs\\\\dacon\\\\Lib\\\\site-packages\\\\xgboost\\\\data.py\", line 407, in pandas_feature_info\\n    new_feature_types.append(_pandas_dtype_mapper[dtype.name])\\n                             ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\\nKeyError: \\'object\\'\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \"c:\\\\Users\\\\human\\\\.conda\\\\envs\\\\dacon\\\\Lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py\", line 866, in _fit_and_score\\n    estimator.fit(X_train, y_train, **fit_params)\\n  File \"c:\\\\Users\\\\human\\\\.conda\\\\envs\\\\dacon\\\\Lib\\\\site-packages\\\\xgboost\\\\core.py\", line 729, in inner_f\\n    return func(**kwargs)\\n           ^^^^^^^^^^^^^^\\n  File \"c:\\\\Users\\\\human\\\\.conda\\\\envs\\\\dacon\\\\Lib\\\\site-packages\\\\xgboost\\\\sklearn.py\", line 1222, in fit\\n    train_dmatrix, evals = _wrap_evaluation_matrices(\\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"c:\\\\Users\\\\human\\\\.conda\\\\envs\\\\dacon\\\\Lib\\\\site-packages\\\\xgboost\\\\sklearn.py\", line 628, in _wrap_evaluation_matrices\\n    train_dmatrix = create_dmatrix(\\n                    ^^^^^^^^^^^^^^^\\n  File \"c:\\\\Users\\\\human\\\\.conda\\\\envs\\\\dacon\\\\Lib\\\\site-packages\\\\xgboost\\\\sklearn.py\", line 1137, in _create_dmatrix\\n    return QuantileDMatrix(\\n           ^^^^^^^^^^^^^^^^\\n  File \"c:\\\\Users\\\\human\\\\.conda\\\\envs\\\\dacon\\\\Lib\\\\site-packages\\\\xgboost\\\\core.py\", line 729, in inner_f\\n    return func(**kwargs)\\n           ^^^^^^^^^^^^^^\\n  File \"c:\\\\Users\\\\human\\\\.conda\\\\envs\\\\dacon\\\\Lib\\\\site-packages\\\\xgboost\\\\core.py\", line 1614, in __init__\\n    self._init(\\n  File \"c:\\\\Users\\\\human\\\\.conda\\\\envs\\\\dacon\\\\Lib\\\\site-packages\\\\xgboost\\\\core.py\", line 1678, in _init\\n    it.reraise()\\n  File \"c:\\\\Users\\\\human\\\\.conda\\\\envs\\\\dacon\\\\Lib\\\\site-packages\\\\xgboost\\\\core.py\", line 572, in reraise\\n    raise exc  # pylint: disable=raising-bad-type\\n    ^^^^^^^^^\\n  File \"c:\\\\Users\\\\human\\\\.conda\\\\envs\\\\dacon\\\\Lib\\\\site-packages\\\\xgboost\\\\core.py\", line 553, in _handle_exception\\n    return fn()\\n           ^^^^\\n  File \"c:\\\\Users\\\\human\\\\.conda\\\\envs\\\\dacon\\\\Lib\\\\site-packages\\\\xgboost\\\\core.py\", line 640, in <lambda>\\n    return self._handle_exception(lambda: int(self.next(input_data)), 0)\\n                                              ^^^^^^^^^^^^^^^^^^^^^\\n  File \"c:\\\\Users\\\\human\\\\.conda\\\\envs\\\\dacon\\\\Lib\\\\site-packages\\\\xgboost\\\\data.py\", line 1654, in next\\n    input_data(**self.kwargs)\\n  File \"c:\\\\Users\\\\human\\\\.conda\\\\envs\\\\dacon\\\\Lib\\\\site-packages\\\\xgboost\\\\core.py\", line 729, in inner_f\\n    return func(**kwargs)\\n           ^^^^^^^^^^^^^^\\n  File \"c:\\\\Users\\\\human\\\\.conda\\\\envs\\\\dacon\\\\Lib\\\\site-packages\\\\xgboost\\\\core.py\", line 620, in input_data\\n    new, cat_codes, feature_names, feature_types = _proxy_transform(\\n                                                   ^^^^^^^^^^^^^^^^^\\n  File \"c:\\\\Users\\\\human\\\\.conda\\\\envs\\\\dacon\\\\Lib\\\\site-packages\\\\xgboost\\\\data.py\", line 1707, in _proxy_transform\\n    df, feature_names, feature_types = _transform_pandas_df(\\n                                       ^^^^^^^^^^^^^^^^^^^^^\\n  File \"c:\\\\Users\\\\human\\\\.conda\\\\envs\\\\dacon\\\\Lib\\\\site-packages\\\\xgboost\\\\data.py\", line 640, in _transform_pandas_df\\n    feature_names, feature_types = pandas_feature_info(\\n                                   ^^^^^^^^^^^^^^^^^^^^\\n  File \"c:\\\\Users\\\\human\\\\.conda\\\\envs\\\\dacon\\\\Lib\\\\site-packages\\\\xgboost\\\\data.py\", line 409, in pandas_feature_info\\n    _invalid_dataframe_dtype(data)\\n  File \"c:\\\\Users\\\\human\\\\.conda\\\\envs\\\\dacon\\\\Lib\\\\site-packages\\\\xgboost\\\\data.py\", line 372, in _invalid_dataframe_dtype\\n    raise ValueError(msg)\\nValueError: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:인수여부: object, 상장여부: object\\n').\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\human\\AppData\\Local\\Temp\\ipykernel_15088\\3545318896.py\", line 18, in objective\n",
      "    scores = cross_val_score(model, X, y, cv=3, scoring='neg_mean_absolute_error')\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 216, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 684, in cross_val_score\n",
      "    cv_results = cross_validate(\n",
      "                 ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 216, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 431, in cross_validate\n",
      "    _warn_or_raise_about_fit_failures(results, error_score)\n",
      "  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 517, in _warn_or_raise_about_fit_failures\n",
      "    raise ValueError(all_fits_failed_message)\n",
      "ValueError: \n",
      "All the 3 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\data.py\", line 407, in pandas_feature_info\n",
      "    new_feature_types.append(_pandas_dtype_mapper[dtype.name])\n",
      "                             ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
      "KeyError: 'object'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\sklearn.py\", line 1222, in fit\n",
      "    train_dmatrix, evals = _wrap_evaluation_matrices(\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\sklearn.py\", line 628, in _wrap_evaluation_matrices\n",
      "    train_dmatrix = create_dmatrix(\n",
      "                    ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\sklearn.py\", line 1137, in _create_dmatrix\n",
      "    return QuantileDMatrix(\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\core.py\", line 1614, in __init__\n",
      "    self._init(\n",
      "  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\core.py\", line 1678, in _init\n",
      "    it.reraise()\n",
      "  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\core.py\", line 572, in reraise\n",
      "    raise exc  # pylint: disable=raising-bad-type\n",
      "    ^^^^^^^^^\n",
      "  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\core.py\", line 553, in _handle_exception\n",
      "    return fn()\n",
      "           ^^^^\n",
      "  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\core.py\", line 640, in <lambda>\n",
      "    return self._handle_exception(lambda: int(self.next(input_data)), 0)\n",
      "                                              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\data.py\", line 1654, in next\n",
      "    input_data(**self.kwargs)\n",
      "  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\core.py\", line 620, in input_data\n",
      "    new, cat_codes, feature_names, feature_types = _proxy_transform(\n",
      "                                                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\data.py\", line 1707, in _proxy_transform\n",
      "    df, feature_names, feature_types = _transform_pandas_df(\n",
      "                                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\data.py\", line 640, in _transform_pandas_df\n",
      "    feature_names, feature_types = pandas_feature_info(\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\data.py\", line 409, in pandas_feature_info\n",
      "    _invalid_dataframe_dtype(data)\n",
      "  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\data.py\", line 372, in _invalid_dataframe_dtype\n",
      "    raise ValueError(msg)\n",
      "ValueError: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:인수여부: object, 상장여부: object\n",
      "\n",
      "[W 2025-05-01 13:59:12,526] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 3 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n3 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\data.py\", line 407, in pandas_feature_info\n    new_feature_types.append(_pandas_dtype_mapper[dtype.name])\n                             ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 'object'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\sklearn.py\", line 1222, in fit\n    train_dmatrix, evals = _wrap_evaluation_matrices(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\sklearn.py\", line 628, in _wrap_evaluation_matrices\n    train_dmatrix = create_dmatrix(\n                    ^^^^^^^^^^^^^^^\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\sklearn.py\", line 1137, in _create_dmatrix\n    return QuantileDMatrix(\n           ^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\core.py\", line 1614, in __init__\n    self._init(\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\core.py\", line 1678, in _init\n    it.reraise()\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\core.py\", line 572, in reraise\n    raise exc  # pylint: disable=raising-bad-type\n    ^^^^^^^^^\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\core.py\", line 553, in _handle_exception\n    return fn()\n           ^^^^\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\core.py\", line 640, in <lambda>\n    return self._handle_exception(lambda: int(self.next(input_data)), 0)\n                                              ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\data.py\", line 1654, in next\n    input_data(**self.kwargs)\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\core.py\", line 620, in input_data\n    new, cat_codes, feature_names, feature_types = _proxy_transform(\n                                                   ^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\data.py\", line 1707, in _proxy_transform\n    df, feature_names, feature_types = _transform_pandas_df(\n                                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\data.py\", line 640, in _transform_pandas_df\n    feature_names, feature_types = pandas_feature_info(\n                                   ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\data.py\", line 409, in pandas_feature_info\n    _invalid_dataframe_dtype(data)\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\data.py\", line 372, in _invalid_dataframe_dtype\n    raise ValueError(msg)\nValueError: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:인수여부: object, 상장여부: object\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m -scores.mean()\n\u001b[32m     21\u001b[39m study = optuna.create_study(direction=\u001b[33m'\u001b[39m\u001b[33mminimize\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest MAE:\u001b[39m\u001b[33m\"\u001b[39m, study.best_value)\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest parameters:\u001b[39m\u001b[33m\"\u001b[39m, study.best_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\optuna\\study\\study.py:475\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    374\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    375\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    382\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    383\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    385\u001b[39m \n\u001b[32m    386\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    473\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    241\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    244\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    245\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    246\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    247\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    199\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    200\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m      5\u001b[39m params = {\n\u001b[32m      6\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mn_estimators\u001b[39m\u001b[33m'\u001b[39m: trial.suggest_int(\u001b[33m'\u001b[39m\u001b[33mn_estimators\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m500\u001b[39m, \u001b[32m1500\u001b[39m),\n\u001b[32m      7\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mlearning_rate\u001b[39m\u001b[33m'\u001b[39m: trial.suggest_float(\u001b[33m'\u001b[39m\u001b[33mlearning_rate\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0.001\u001b[39m, \u001b[32m0.05\u001b[39m, log=\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mn_jobs\u001b[39m\u001b[33m'\u001b[39m: -\u001b[32m1\u001b[39m\n\u001b[32m     15\u001b[39m }\n\u001b[32m     17\u001b[39m model = XGBRegressor(**params)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m scores = \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mneg_mean_absolute_error\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m -scores.mean()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:684\u001b[39m, in \u001b[36mcross_val_score\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[39m\n\u001b[32m    681\u001b[39m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[32m    682\u001b[39m scorer = check_scoring(estimator, scoring=scoring)\n\u001b[32m--> \u001b[39m\u001b[32m684\u001b[39m cv_results = \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[33m\"\u001b[39m\u001b[33mtest_score\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:431\u001b[39m, in \u001b[36mcross_validate\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[39m\n\u001b[32m    410\u001b[39m parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n\u001b[32m    411\u001b[39m results = parallel(\n\u001b[32m    412\u001b[39m     delayed(_fit_and_score)(\n\u001b[32m    413\u001b[39m         clone(estimator),\n\u001b[32m   (...)\u001b[39m\u001b[32m    428\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[32m    429\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m431\u001b[39m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[32m    434\u001b[39m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[32m    435\u001b[39m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(scoring):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:517\u001b[39m, in \u001b[36m_warn_or_raise_about_fit_failures\u001b[39m\u001b[34m(results, error_score)\u001b[39m\n\u001b[32m    510\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits == num_fits:\n\u001b[32m    511\u001b[39m     all_fits_failed_message = (\n\u001b[32m    512\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    513\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    514\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou can try to debug the error by setting error_score=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    515\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    516\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[32m    519\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    520\u001b[39m     some_fits_failed_message = (\n\u001b[32m    521\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    522\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe score on these train-test partitions for these parameters\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    526\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    527\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: \nAll the 3 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n3 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\data.py\", line 407, in pandas_feature_info\n    new_feature_types.append(_pandas_dtype_mapper[dtype.name])\n                             ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 'object'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\sklearn.py\", line 1222, in fit\n    train_dmatrix, evals = _wrap_evaluation_matrices(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\sklearn.py\", line 628, in _wrap_evaluation_matrices\n    train_dmatrix = create_dmatrix(\n                    ^^^^^^^^^^^^^^^\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\sklearn.py\", line 1137, in _create_dmatrix\n    return QuantileDMatrix(\n           ^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\core.py\", line 1614, in __init__\n    self._init(\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\core.py\", line 1678, in _init\n    it.reraise()\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\core.py\", line 572, in reraise\n    raise exc  # pylint: disable=raising-bad-type\n    ^^^^^^^^^\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\core.py\", line 553, in _handle_exception\n    return fn()\n           ^^^^\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\core.py\", line 640, in <lambda>\n    return self._handle_exception(lambda: int(self.next(input_data)), 0)\n                                              ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\data.py\", line 1654, in next\n    input_data(**self.kwargs)\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\core.py\", line 620, in input_data\n    new, cat_codes, feature_names, feature_types = _proxy_transform(\n                                                   ^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\data.py\", line 1707, in _proxy_transform\n    df, feature_names, feature_types = _transform_pandas_df(\n                                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\data.py\", line 640, in _transform_pandas_df\n    feature_names, feature_types = pandas_feature_info(\n                                   ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\data.py\", line 409, in pandas_feature_info\n    _invalid_dataframe_dtype(data)\n  File \"c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\data.py\", line 372, in _invalid_dataframe_dtype\n    raise ValueError(msg)\nValueError: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:인수여부: object, 상장여부: object\n"
     ]
    }
   ],
   "source": [
    "# import optuna\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "#         'n_estimators': trial.suggest_int('n_estimators', 500, 1500),\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.05, log=True),\n",
    "#         'max_depth': trial.suggest_int('max_depth', 5, 20),\n",
    "#         'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "#         'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "#         'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "#         'random_state': 42,\n",
    "#         'n_jobs': -1\n",
    "#     }\n",
    "\n",
    "#     model = XGBRegressor(**params)\n",
    "#     scores = cross_val_score(model, X, y, cv=3, scoring='neg_mean_absolute_error')\n",
    "#     return -scores.mean()\n",
    "\n",
    "# study = optuna.create_study(direction='minimize')\n",
    "# study.optimize(objective, n_trials=50)\n",
    "\n",
    "# print(\"Best MAE:\", study.best_value)\n",
    "# print(\"Best parameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be0c2ec",
   "metadata": {},
   "source": [
    "모델 학습 및 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa02c0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1\n",
      "✅ Fold 1 MAE: 0.19701\n",
      "\n",
      "Fold 2\n",
      "✅ Fold 2 MAE: 0.19584\n",
      "\n",
      "Fold 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     26\u001b[39m     X_val, y_val = X_enhanced.iloc[val_idx], y.iloc[val_idx]\n\u001b[32m     28\u001b[39m     model = XGBRegressor(\n\u001b[32m     29\u001b[39m     n_estimators=\u001b[32m1325\u001b[39m,\n\u001b[32m     30\u001b[39m     learning_rate=\u001b[32m0.00375\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     n_jobs=-\u001b[32m1\u001b[39m\n\u001b[32m     36\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m     y_pred = model.predict(X_val)\n\u001b[32m     40\u001b[39m     mae = mean_absolute_error(y_val, y_pred)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\sklearn.py:1247\u001b[39m, in \u001b[36mXGBModel.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1245\u001b[39m     obj = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1247\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1249\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1250\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1259\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1261\u001b[39m \u001b[38;5;28mself\u001b[39m._set_evaluation_result(evals_result)\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\training.py:183\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[43mbst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\xgboost\\core.py:2247\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, dtrain, iteration, fobj)\u001b[39m\n\u001b[32m   2243\u001b[39m \u001b[38;5;28mself\u001b[39m._assign_dmatrix_features(dtrain)\n\u001b[32m   2245\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2246\u001b[39m     _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m2247\u001b[39m         \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2250\u001b[39m     )\n\u001b[32m   2251\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2252\u001b[39m     pred = \u001b[38;5;28mself\u001b[39m.predict(dtrain, output_margin=\u001b[38;5;28;01mTrue\u001b[39;00m, training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 이진형 인코딩\n",
    "for col in ['인수여부', '상장여부']:\n",
    "    if col in X_enhanced.columns:\n",
    "        X_enhanced[col] = X_enhanced[col].map({'No': 0, 'Yes': 1})\n",
    "    if col in X_test_enhanced.columns:\n",
    "        X_test_enhanced[col] = X_test_enhanced[col].map({'No': 0, 'Yes': 1})\n",
    "\n",
    "n_repeats = 3\n",
    "n_splits = 5\n",
    "bins = np.linspace(0, 1, 6)\n",
    "y_binned = np.digitize(y, bins)\n",
    "\n",
    "#skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "rkf = RepeatedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "\n",
    "cv_scores = []\n",
    "test_preds = []\n",
    "\n",
    "# for fold, (train_idx, val_idx) in enumerate(skf.split(X_enhanced, y_binned)):\n",
    "# for fold, (train_idx, val_idx) in enumerate(rkf.split(X_enhanced, y)):\n",
    "for repeat in range(n_repeats):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42 + repeat)\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_enhanced, y_binned)):\n",
    "        print(f\"\\nFold {fold+1}\")\n",
    "        X_tr, y_tr = X_enhanced.iloc[train_idx], y.iloc[train_idx]\n",
    "        X_val, y_val = X_enhanced.iloc[val_idx], y.iloc[val_idx]\n",
    "\n",
    "        model = XGBRegressor(\n",
    "        n_estimators=1325,\n",
    "        learning_rate=0.00375,\n",
    "        max_depth=15,\n",
    "        subsample=0.58315,\n",
    "        colsample_bytree=0.75715,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_val)\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        print(f\"✅ Fold {fold+1} MAE: {mae:.5f}\")\n",
    "        cv_scores.append(mae)\n",
    "\n",
    "        test_preds.append(model.predict(X_test_enhanced))\n",
    "\n",
    "print(\"\\n📉 평균 MAE:\", np.mean(cv_scores))\n",
    "\n",
    "final_preds = np.mean(test_preds, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1a3894",
   "metadata": {},
   "source": [
    "제출파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f687599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 최종 제출 파일 저장 완료\n",
      "<bound method NDFrame.head of              ID      성공확률\n",
      "0     TEST_0000  0.495430\n",
      "1     TEST_0001  0.476088\n",
      "2     TEST_0002  0.423867\n",
      "3     TEST_0003  0.518815\n",
      "4     TEST_0004  0.641490\n",
      "...         ...       ...\n",
      "1750  TEST_1750  0.534823\n",
      "1751  TEST_1751  0.566253\n",
      "1752  TEST_1752  0.487948\n",
      "1753  TEST_1753  0.476240\n",
      "1754  TEST_1754  0.524418\n",
      "\n",
      "[1755 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "sample_submission['성공확률'] = final_preds\n",
    "sample_submission.to_csv('클린코드1차.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"✅ 최종 제출 파일 저장 완료\")\n",
    "print(sample_submission.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41f988f",
   "metadata": {},
   "source": [
    "해석1차 - 폐기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c970179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_interaction_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 설립년차 * 투자단계\n",
    "    if '설립년차' in df.columns and '투자단계' in df.columns:\n",
    "        df['설립X투자단계'] = df['설립년차'] * df['투자단계']\n",
    "    \n",
    "    # 고객수_직원비 * 연매출_직원비\n",
    "    if '고객수_직원비' in df.columns and '연매출_직원비' in df.columns:\n",
    "        df['고객X매출직원비'] = df['고객수_직원비'] * df['연매출_직원비']\n",
    "    \n",
    "    # log(SNS 수) + log(연매출)\n",
    "    if 'SNS 팔로워 수(백만명)' in df.columns and '연매출(억원)' in df.columns:\n",
    "        df['로그SNS'] = np.log1p(df['SNS 팔로워 수(백만명)'])\n",
    "        df['로그매출'] = np.log1p(df['연매출(억원)'])\n",
    "        df['로그SNS+매출'] = df['로그SNS'] + df['로그매출']\n",
    "    \n",
    "    return df\n",
    "\n",
    "X_enhanced = add_interaction_features(X_enhanced)\n",
    "X_test_enhanced = add_interaction_features(X_test_enhanced)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c669b074",
   "metadata": {},
   "source": [
    "회사랑 국가별?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30c6776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 기준으로 국가별 평균 성공률 구하기\n",
    "country_success_mean = train_filled.groupby('국가')['성공확률'].mean()\n",
    "\n",
    "# train/test 모두에 국가 평균 성공률 피처로 추가\n",
    "X_enhanced['국가_성공률평균'] = X_enhanced['국가'].map(country_success_mean)\n",
    "X_test_enhanced['국가_성공률평균'] = X_test_enhanced['국가'].map(country_success_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8076df1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m bins = \u001b[43mnp\u001b[49m.linspace(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m6\u001b[39m)\n\u001b[32m      2\u001b[39m y_binned = np.digitize(y, bins)\n\u001b[32m      4\u001b[39m skf = StratifiedKFold(n_splits=\u001b[32m5\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, random_state=\u001b[32m42\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "bins = np.linspace(0, 1, 6)\n",
    "y_binned = np.digitize(y, bins)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_scores = []\n",
    "test_preds = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_enhanced, y_binned)):\n",
    "    print(f\"\\nFold {fold+1}\")\n",
    "    X_tr, y_tr = X_enhanced.iloc[train_idx], y.iloc[train_idx]\n",
    "    X_val, y_val = X_enhanced.iloc[val_idx], y.iloc[val_idx]\n",
    "\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.03,\n",
    "        max_depth=15,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    model.fit(X_tr, y_tr)\n",
    "    y_pred = model.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    print(f\"✅ Fold {fold+1} MAE: {mae:.5f}\")\n",
    "    cv_scores.append(mae)\n",
    "\n",
    "    test_preds.append(model.predict(X_test_enhanced))\n",
    "\n",
    "print(\"\\n📉 평균 MAE:\", np.mean(cv_scores))\n",
    "\n",
    "final_preds = np.mean(test_preds, axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
