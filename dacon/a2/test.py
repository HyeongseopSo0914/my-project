
import pandas as pd
import numpy as np

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import KFold

import torch
from pytorch_tabnet.pretraining import TabNetPretrainer
from pytorch_tabnet.tab_model import TabNetRegressor

train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')
sample_submission = pd.read_csv('sample_submission.csv')

#íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ë¦¬
train = train.drop(columns=['ID'], axis = 1)
test = test.drop(columns=['ID'], axis = 1)

# âœ… 1. ë°ì´í„° ì „ì²˜ë¦¬
CURRENT_YEAR = 2025

# ë‚˜ì´ ë³€í™˜
train['ë‚˜ì´'] = CURRENT_YEAR - train['ì„¤ë¦½ì—°ë„']
test['ë‚˜ì´'] = CURRENT_YEAR - test['ì„¤ë¦½ì—°ë„']

# êµ­ê°€ Encoding
le_country = LabelEncoder()
train['êµ­ê°€'] = le_country.fit_transform(train['êµ­ê°€'].fillna('Missing'))
test['êµ­ê°€'] = le_country.transform(test['êµ­ê°€'].fillna('Missing'))

# ë¶„ì•¼ ì ìˆ˜í™” (êµ¬ê°„ ë§¤í•‘)
field_score = {
    'AI': 5, 'í•€í…Œí¬': 5, 'ê¸°ìˆ ': 4, 'ë¬¼ë¥˜': 3, 'ì—ë“€í…Œí¬': 3,
    'í‘¸ë“œí…Œí¬': 2, 'ê²Œì„': 2, 'ì—ë„ˆì§€': 1, 'ì´ì»¤ë¨¸ìŠ¤': 1, 'í—¬ìŠ¤ì¼€ì–´': 1, 0:0
}
train['ë¶„ì•¼ì ìˆ˜'] = train['ë¶„ì•¼'].map(field_score).fillna(0)
test['ë¶„ì•¼ì ìˆ˜'] = test['ë¶„ì•¼'].map(field_score).fillna(0)

# íˆ¬ìë‹¨ê³„ ìˆ˜ì¹˜í™”
stage_mapping = {'Seed':0, 'Series A':1, 'Series B':2, 'Series C':3, 'IPO':4}
train['íˆ¬ìë‹¨ê³„'] = train['íˆ¬ìë‹¨ê³„'].map(stage_mapping).fillna(0)
test['íˆ¬ìë‹¨ê³„'] = test['íˆ¬ìë‹¨ê³„'].map(stage_mapping).fillna(0)

# ì¸ìˆ˜/ìƒì¥ ì—¬ë¶€ 0/1 ë§¤í•‘
bool_map = {'Yes':1, 'No':0}
for feature in ['ì¸ìˆ˜ì—¬ë¶€', 'ìƒì¥ì—¬ë¶€']:
    train[feature] = train[feature].map(bool_map).fillna(0)
    test[feature] = test[feature].map(bool_map).fillna(0)

numeric_features = ['ì§ì› ìˆ˜','ê³ ê°ìˆ˜(ë°±ë§Œëª…)','ì´ íˆ¬ìê¸ˆ(ì–µì›)','ì—°ë§¤ì¶œ(ì–µì›)','SNS íŒ”ë¡œì›Œ ìˆ˜(ë°±ë§Œëª…)']
# ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ê²°ì¸¡ì¹˜ë¥¼ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´
for feature in numeric_features:
    mean_value = train[feature].mean()
    train[feature] = train[feature].fillna(mean_value)
    test[feature] = test[feature].fillna(mean_value)

# íˆ¬ì ëŒ€ë¹„ ë§¤ì¶œ Feature ì¶”ê°€
train['íˆ¬ìëŒ€ë¹„ë§¤ì¶œ'] = train['ì—°ë§¤ì¶œ(ì–µì›)'].fillna(0) / (train['ì´ íˆ¬ìê¸ˆ(ì–µì›)'].fillna(0)+1)
test['íˆ¬ìëŒ€ë¹„ë§¤ì¶œ'] = test['ì—°ë§¤ì¶œ(ì–µì›)'].fillna(0) / (test['ì´ íˆ¬ìê¸ˆ(ì–µì›)'].fillna(0)+1)

# SNS íŒ”ë¡œì›Œ ìˆ˜ êµ¬ê°„í™”
def sns_bin(x):
    if x < 1: return 0
    elif x < 3: return 1
    elif x < 5: return 2
    else: return 3
train['SNSíŒ”ë¡œì›Œêµ¬ê°„'] = train['SNS íŒ”ë¡œì›Œ ìˆ˜(ë°±ë§Œëª…)'].apply(lambda x: sns_bin(x) if not pd.isna(x) else 0)
test['SNSíŒ”ë¡œì›Œêµ¬ê°„'] = test['SNS íŒ”ë¡œì›Œ ìˆ˜(ë°±ë§Œëª…)'].apply(lambda x: sns_bin(x) if not pd.isna(x) else 0)

# ê¸°ì—…ê°€ì¹˜ ìˆ˜ì¹˜í™”
def parse_value(x):
    if pd.isnull(x): return 0
    if '6000' in str(x): return 6
    if '4500' in str(x): return 5
    if '3500' in str(x): return 4
    if '2500' in str(x): return 3
    if '1500' in str(x): return 2
    return 1
train['ê¸°ì—…ê°€ì¹˜_í´ë˜ìŠ¤'] = train['ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)'].apply(parse_value)
test['ê¸°ì—…ê°€ì¹˜_í´ë˜ìŠ¤'] = test['ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)'].apply(parse_value)

# # ê²°ì¸¡ì¹˜ëŠ” ëª¨ë‘ 0 ì²˜ë¦¬
# train = train.fillna(0)
# test = test.fillna(0)
# ìµœì¢… Feature ë¦¬ìŠ¤íŠ¸
features = [
    'ë‚˜ì´', 'êµ­ê°€', 'ë¶„ì•¼ì ìˆ˜', 'íˆ¬ìë‹¨ê³„', 'ì§ì› ìˆ˜',
    'ì¸ìˆ˜ì—¬ë¶€', 'ìƒì¥ì—¬ë¶€', 'ê³ ê°ìˆ˜(ë°±ë§Œëª…)', 'íˆ¬ìëŒ€ë¹„ë§¤ì¶œ',
    'SNSíŒ”ë¡œì›Œêµ¬ê°„', 'ê¸°ì—…ê°€ì¹˜_í´ë˜ìŠ¤'
]
category_features = ['êµ­ê°€', 'ë¶„ì•¼ì ìˆ˜', 'íˆ¬ìë‹¨ê³„', 'SNSíŒ”ë¡œì›Œêµ¬ê°„', 'ê¸°ì—…ê°€ì¹˜_í´ë˜ìŠ¤']
cat_idxs = [features.index(col) for col in category_features]
cat_dims = [int(train[col].max())+2 for col in category_features]


# âœ… 2. TabNet í•™ìŠµ (KFold)
N_FOLDS = 5
kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)

models = []
cv_scores = []

target = train['ì„±ê³µí™•ë¥ ']
X = train[features]
y = target

for fold, (train_idx, valid_idx) in enumerate(kf.split(X)):
    print(f"\nğŸ” Fold {fold+1}/{N_FOLDS}")

    X_train, X_valid = X.iloc[train_idx].values, X.iloc[valid_idx].values
    y_train, y_valid = y.iloc[train_idx].values.reshape(-1,1), y.iloc[valid_idx].values.reshape(-1,1)

    print("â–¶ Pretraining...")
    pretrainer = TabNetPretrainer(
        cat_idxs=cat_idxs,
        cat_dims=cat_dims,
        seed=42+fold,
        verbose=0
    )
    pretrainer.fit(
        X_train=X_train,
        eval_set=[X_valid],
        max_epochs=100,
        batch_size=512,
        virtual_batch_size=64,
        patience=10
    )

    print("â–¶ Fine-tuning...")
    model = TabNetRegressor(
        cat_idxs=cat_idxs,
        cat_dims=cat_dims,
        seed=42+fold,
        verbose=0,
        optimizer_fn=torch.optim.AdamW
    )
    model.fit(
        X_train=X_train, y_train=y_train,
        eval_set=[(X_valid, y_valid)],
        from_unsupervised=pretrainer,
        eval_metric=['mae'],
        max_epochs=100,
        patience=10
    )

    models.append(model)
    cv_scores.append(model.best_cost)

print("\nâœ… ëª¨ë“  fold í•™ìŠµ ì™„ë£Œ!")
print("CV Scores:", cv_scores)
print("Mean CV:", np.mean(cv_scores))


# âœ… 3. Feature Importance ë³´ê¸°
model = models[0]
importance_df = pd.DataFrame({
    'feature': features,
    'importance': model.feature_importances_
}).sort_values(by='importance', ascending=False)

# ì €ì¥ëœ ëª¨ë¸ë“¤ë¡œ ì˜ˆì¸¡
predictions_list = []

for fold, model in enumerate(models):
    print(f"Predict with fold {fold+1}")
    preds = model.predict(test[features].values)
    predictions_list.append(preds)

# í‰ê·  ì˜ˆì¸¡
final_predictions = np.mean(predictions_list, axis=0)

sample_submission['ì„±ê³µí™•ë¥ '] = final_predictions
sample_submission.to_csv('./baseline_submission.csv', index = False, encoding = 'utf-8-sig')
