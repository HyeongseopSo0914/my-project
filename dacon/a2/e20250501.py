
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import seaborn as sns

from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import GradientBoostingRegressor

# ë§‘ì€ ê³ ë”• ì„¤ì •
plt.rc('font', family='Malgun Gothic')  # Windows
# plt.rc('font', family='AppleGothic')  # macOS
# plt.rc('font', family='NanumGothic')  # Linux (Colab ë“±ì—ì„œ)

# ë§ˆì´ë„ˆìŠ¤ ê¹¨ì§ ë°©ì§€
plt.rcParams['axes.unicode_minus'] = False

train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')
sample_submission = pd.read_csv('sample_submission.csv')

#íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ë¦¬
train = train.drop(columns=['ID'], axis = 1)
test = test.drop(columns=['ID'], axis = 1)

# ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›) ìˆ«ìí™”
# ê¸°ì—…ê°€ì¹˜ ì»¬ëŸ¼ë„ ë²”ìœ„ ë¬¸ìì—´ ì²˜ë¦¬ ì¶”ê°€

def convert_range_to_float(value):
    if isinstance(value, str) and '-' in value:
        try:
            low, high = map(float, value.split('-'))
            return (low + high) / 2
        except:
            return np.nan
    try:
        return float(value)
    except:
        return np.nan
    
def encode_categoricals(df, cols):
    df = df.copy()
    for col in cols:
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col].astype(str))
    return df

def fill_missing_values_v3(df, is_train=True):
    df = df.copy()

    # ë²”ìœ„ ë¬¸ìì—´ â†’ í‰ê·  ìˆ«ì ì²˜ë¦¬
    for col in ['ì—°ë§¤ì¶œ(ì–µì›)', 'ì´ íˆ¬ìê¸ˆ(ì–µì›)', 'ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)']:
        df[col] = df[col].apply(convert_range_to_float)

    # ë¶„ì•¼ ê²°ì¸¡ ë° ì¸ì½”ë”©
    if 'ë¶„ì•¼' in df.columns:
        df['ë¶„ì•¼'] = df['ë¶„ì•¼'].fillna('Unknown')
        df['ë¶„ì•¼'] = LabelEncoder().fit_transform(df['ë¶„ì•¼'])

    # êµ­ê°€, íˆ¬ìë‹¨ê³„ ì¸ì½”ë”©
    df = encode_categoricals(df, ['êµ­ê°€', 'íˆ¬ìë‹¨ê³„'])

    # âœ… ê²°ì¸¡ í”Œë˜ê·¸ ì¶”ê°€ í•¨ìˆ˜
    def add_missing_flag(column):
        flag_col = f'{column}_ê²°ì¸¡'
        df[flag_col] = df[column].isnull().astype(int)

    # âœ… í”¼ì²˜ì…‹ ìƒì„± í•¨ìˆ˜
    def get_features(base):
        return base + (['ì„±ê³µí™•ë¥ '] if is_train else [])

    # 1. ì§ì› ìˆ˜
    if 'ì§ì› ìˆ˜' in df.columns:
        add_missing_flag('ì§ì› ìˆ˜')
        features = get_features(['ì„¤ë¦½ì—°ë„', 'êµ­ê°€', 'íˆ¬ìë‹¨ê³„', 'ì—°ë§¤ì¶œ(ì–µì›)', 'ì´ íˆ¬ìê¸ˆ(ì–µì›)', 'SNS íŒ”ë¡œì›Œ ìˆ˜(ë°±ë§Œëª…)'])
        complete = df[df['ì§ì› ìˆ˜'].notnull()]
        missing = df[df['ì§ì› ìˆ˜'].isnull()]
        if not complete.empty and not missing.empty:
            model = GradientBoostingRegressor()
            model.fit(complete[features], complete['ì§ì› ìˆ˜'])
            df.loc[df['ì§ì› ìˆ˜'].isnull(), 'ì§ì› ìˆ˜'] = model.predict(missing[features])

    # 2. ê³ ê° ìˆ˜
    if 'ê³ ê°ìˆ˜(ë°±ë§Œëª…)' in df.columns:
        add_missing_flag('ê³ ê°ìˆ˜(ë°±ë§Œëª…)')
        features = get_features(['ì„¤ë¦½ì—°ë„', 'ì§ì› ìˆ˜', 'ë¶„ì•¼', 'ì—°ë§¤ì¶œ(ì–µì›)', 'ì´ íˆ¬ìê¸ˆ(ì–µì›)', 'SNS íŒ”ë¡œì›Œ ìˆ˜(ë°±ë§Œëª…)'])
        complete = df[df['ê³ ê°ìˆ˜(ë°±ë§Œëª…)'].notnull()]
        missing = df[df['ê³ ê°ìˆ˜(ë°±ë§Œëª…)'].isnull()]
        if not complete.empty and not missing.empty:
            model = GradientBoostingRegressor()
            model.fit(complete[features], complete['ê³ ê°ìˆ˜(ë°±ë§Œëª…)'])
            df.loc[df['ê³ ê°ìˆ˜(ë°±ë§Œëª…)'].isnull(), 'ê³ ê°ìˆ˜(ë°±ë§Œëª…)'] = model.predict(missing[features])

    # 3. ê¸°ì—…ê°€ì¹˜
    if 'ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)' in df.columns:
        add_missing_flag('ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)')
        features = get_features(['ì„¤ë¦½ì—°ë„', 'ì§ì› ìˆ˜', 'ê³ ê°ìˆ˜(ë°±ë§Œëª…)', 'ë¶„ì•¼', 'ì—°ë§¤ì¶œ(ì–µì›)', 'ì´ íˆ¬ìê¸ˆ(ì–µì›)', 'SNS íŒ”ë¡œì›Œ ìˆ˜(ë°±ë§Œëª…)'])
        complete = df[df['ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)'].notnull()]
        missing = df[df['ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)'].isnull()]
        if not complete.empty and not missing.empty:
            model = GradientBoostingRegressor()
            model.fit(complete[features], complete['ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)'])
            df.loc[df['ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)'].isnull(), 'ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)'] = model.predict(missing[features])

    return df

# ìµœì¢… ê²°ì¸¡ì¹˜ ë³´ê°„ ì‹œë„
train_filled = fill_missing_values_v3(train, is_train=True)
train_filled.isnull().sum()  # ëª¨ë“  ê²°ì¸¡ì¹˜ê°€ ì˜ ì±„ì›Œì¡ŒëŠ”ì§€ í™•ì¸

test_filled = fill_missing_values_v3(test, is_train=False)
test_filled.isnull().sum()  # ëª¨ë“  ê²°ì¸¡ì¹˜ê°€ ì˜ ì±„ì›Œì¡ŒëŠ”ì§€ í™•ì¸

import lightgbm as lgb
import matplotlib.pyplot as plt
import seaborn as sns

# 1. í”¼ì²˜ ë° íƒ€ê²Ÿ ì„¤ì •
feature_cols = [col for col in train_filled.columns if col not in ['ì„±ê³µí™•ë¥ ']]
target_col = 'ì„±ê³µí™•ë¥ '

X = train_filled[feature_cols].copy()
y = train_filled[target_col].copy()

for col in X.select_dtypes(include='object').columns:
    if set(X[col].unique()) <= {'Yes', 'No'}:
        X[col] = X[col].map({'No': 0, 'Yes': 1})
print(X.dtypes[X.dtypes == 'object'])         
# 2. ëª¨ë¸ í›ˆë ¨
model = lgb.LGBMRegressor()
model.fit(X, y)

# 3. Feature Importance ì¶”ì¶œ
importance_df = pd.DataFrame({
    'feature': feature_cols,
    'importance': model.feature_importances_
}).sort_values(by='importance', ascending=False)

def detect_outliers_summary(df, columns):
    summary = []
    for col in columns:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
        summary.append({
            'ì»¬ëŸ¼ëª…': col,
            'ì´ìƒì¹˜ ìˆ˜': len(outliers),
            'ì „ì²´ ëŒ€ë¹„ ë¹„ìœ¨(%)': round(len(outliers) / len(df) * 100, 2)
        })
    return pd.DataFrame(summary).sort_values(by='ì´ìƒì¹˜ ìˆ˜', ascending=False)

# log ë³€í™˜ (0ë³´ë‹¤ í° ê°’ë§Œ ë³€í™˜, log1pëŠ” log(1+x))
num_cols = train_filled.select_dtypes(include=['int64', 'float64']).columns.tolist()
print(num_cols)
log_train = train_filled[num_cols].copy()
for col in num_cols:
    if (log_train[col] > 0).all():  # ìŒìˆ˜, 0 ìˆëŠ” ì»¬ëŸ¼ì€ ì œì™¸
        log_train[col] = np.log1p(log_train[col])

import numpy as np
import pandas as pd

def process_outliers_train_test(train_df, test_df, num_cols, method='flag+clip'):
    """
    train ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ IQR ì´ìƒì¹˜ íƒì§€ ê¸°ì¤€ì„ ì¡ê³ ,
    train/test ëª¨ë‘ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì´ìƒì¹˜ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
    """
    train_processed = train_df.copy()
    test_processed = test_df.copy()
    outlier_bounds = {}

    for col in num_cols:
        Q1 = train_df[col].quantile(0.25)
        Q3 = train_df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR
        outlier_bounds[col] = (lower, upper)

        # ì´ìƒì¹˜ í”Œë˜ê·¸
        if 'flag' in method:
            train_processed[f'{col}_ì´ìƒì¹˜ì—¬ë¶€'] = ((train_df[col] < lower) | (train_df[col] > upper)).astype(int)
            test_processed[f'{col}_ì´ìƒì¹˜ì—¬ë¶€'] = ((test_df[col] < lower) | (test_df[col] > upper)).astype(int)

        # í´ë¦¬í•‘
        if 'clip' in method:
            train_processed[col] = train_df[col].clip(lower, upper)
            test_processed[col] = test_df[col].clip(lower, upper)

        # ë¡œê·¸ ë³€í™˜
        if 'log' in method:
            if (train_processed[col] >= 0).all() and (test_processed[col] >= 0).all():
                train_processed[col] = np.log1p(train_processed[col])
                test_processed[col] = np.log1p(test_processed[col])
            else:
                print(f"[ê²½ê³ ] {col}ì€ log1p ë¶ˆê°€ëŠ¥ (ìŒìˆ˜ ë˜ëŠ” 0 í¬í•¨)")

    return train_processed, test_processed, outlier_bounds

# 'ì„±ê³µí™•ë¥ 'ì€ trainì—ë§Œ ìˆìœ¼ë¯€ë¡œ ì œì™¸
num_cols = train_filled.select_dtypes(include=np.number).columns.tolist()
num_cols = [col for col in num_cols if col != 'ì„±ê³µí™•ë¥ ']

# ì´ìƒì¹˜ ì²˜ë¦¬ ìˆ˜í–‰
train_processed, test_processed, bounds = process_outliers_train_test(train_filled, test_filled, num_cols, method='flag+clip')

# Xì— í•„ìš”í•œ ì»¬ëŸ¼ë“¤ì´ ëª¨ë‘ ìˆëŠ”ì§€ í™•ì¸
required_cols = ['ì§ì› ìˆ˜', 'ì—°ë§¤ì¶œ(ì–µì›)', 'ì´ íˆ¬ìê¸ˆ(ì–µì›)', 'ê³ ê°ìˆ˜(ë°±ë§Œëª…)', 
                 'SNS íŒ”ë¡œì›Œ ìˆ˜(ë°±ë§Œëª…)', 'ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)', 'ì„¤ë¦½ì—°ë„']

missing = [col for col in required_cols if col not in X.columns]
print("â— ëˆ„ë½ëœ ì»¬ëŸ¼:", missing if missing else "ì—†ìŒ â€” íŒŒìƒë³€ìˆ˜ ìƒì„± ê°€ëŠ¥")

def create_features(df):
    df = df.copy()
    df['ì§ì› ìˆ˜_ë¡œê·¸'] = np.log1p(df['ì§ì› ìˆ˜'])
    df['ì—°ë§¤ì¶œ_ë¡œê·¸'] = np.log1p(df['ì—°ë§¤ì¶œ(ì–µì›)'])
    df['ì´ íˆ¬ìê¸ˆ_ë¡œê·¸'] = np.log1p(df['ì´ íˆ¬ìê¸ˆ(ì–µì›)'])
    
    df['ê³ ê°ìˆ˜_ì§ì›ë¹„'] = df['ê³ ê°ìˆ˜(ë°±ë§Œëª…)'] / (df['ì§ì› ìˆ˜'] + 1)
    df['ì—°ë§¤ì¶œ_ì§ì›ë¹„'] = df['ì—°ë§¤ì¶œ(ì–µì›)'] / (df['ì§ì› ìˆ˜'] + 1)
    df['íˆ¬ìëŒ€ë¹„ë§¤ì¶œ'] = df['ì—°ë§¤ì¶œ(ì–µì›)'] / (df['ì´ íˆ¬ìê¸ˆ(ì–µì›)'] + 1)
    df['SNSë‹¹ê³ ê°'] = df['ê³ ê°ìˆ˜(ë°±ë§Œëª…)'] / (df['SNS íŒ”ë¡œì›Œ ìˆ˜(ë°±ë§Œëª…)'] + 1)
    df['ê¸°ì—…ê°€ì¹˜ëŒ€ë¹„íˆ¬ì'] = df['ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)'] / (df['ì´ íˆ¬ìê¸ˆ(ì–µì›)'] + 1)

    df['ì„¤ë¦½ë…„ì°¨'] = 2025 - df['ì„¤ë¦½ì—°ë„']

    df['ê³ ê°ìˆ˜_ê²°ì¸¡'] = df['ê³ ê°ìˆ˜(ë°±ë§Œëª…)'].isna().astype(int)
    df['ì§ì› ìˆ˜_ê²°ì¸¡'] = df['ì§ì› ìˆ˜'].isna().astype(int)
    
    return df

X = train_processed.copy()  # â† ì—¬ê¸°ì„œ train_processedì— ì´ìƒì¹˜ í”Œë˜ê·¸ê°€ ë“¤ì–´ìˆì–´ì•¼ í•¨
X = create_features(X)      # íŒŒìƒë³€ìˆ˜ ì¶”ê°€

outlier_flags = [col for col in X.columns if 'ì´ìƒì¹˜ì—¬ë¶€' in col]
print("ğŸ“Œ í¬í•¨ëœ ì´ìƒì¹˜ í”Œë˜ê·¸ ì»¬ëŸ¼ë“¤:")
print(outlier_flags)

ols_to_drop = [
    'ì§ì› ìˆ˜', 'ì—°ë§¤ì¶œ(ì–µì›)', 
    'ì§ì› ìˆ˜_ê²°ì¸¡', 'ê³ ê°ìˆ˜_ê²°ì¸¡', 'ì„¤ë¦½ì—°ë„'
    ,'ì´ íˆ¬ìê¸ˆ(ì–µì›)'
]

existing_cols_to_drop = [col for col in ols_to_drop if col in X.columns]
X = X.drop(columns=existing_cols_to_drop)

y = train_processed['ì„±ê³µí™•ë¥ ']  # ë˜ëŠ” ë¯¸ë¦¬ ë¶„ë¦¬í•œ y ì‚¬ìš©

# 'ì„±ê³µí™•ë¥ ' ì»¬ëŸ¼ì´ ìˆë‹¤ë©´ ì‚­ì œ
if 'ì„±ê³µí™•ë¥ ' in X.columns:
    X = X.drop(columns=['ì„±ê³µí™•ë¥ '])

# y ê²°í•©
X_with_y = X.join(y.rename("ì„±ê³µí™•ë¥ "))

# ìˆ«ìí˜•ë§Œ ë‚¨ê¸°ê³  ìƒê´€ê³„ìˆ˜ ê³„ì‚°
X_with_y_numeric = X_with_y.select_dtypes(include=['number'])
cor_target = X_with_y_numeric.corr()['ì„±ê³µí™•ë¥ '].sort_values(key=abs, ascending=False)

print(cor_target.head(15))

from sklearn.model_selection import KFold
from sklearn.metrics import mean_absolute_error
from catboost import CatBoostRegressor, Pool
from xgboost import XGBRegressor
import random
from sklearn.model_selection import StratifiedKFold
random.seed(42)
np.random.seed(42)
X_test = test_processed.copy()
X_test = create_features(X_test)
cols_to_drop = ['ì§ì› ìˆ˜', 'ì—°ë§¤ì¶œ(ì–µì›)','ì§ì› ìˆ˜_ê²°ì¸¡', 'ê³ ê°ìˆ˜_ê²°ì¸¡', 'ì„¤ë¦½ì—°ë„'
                #  ,'ì´ íˆ¬ìê¸ˆ(ì–µì›)'
                 ]
X_test = X_test.drop(columns=cols_to_drop, errors='ignore')
X_test = X_test[X.columns]

# kf = KFold(n_splits=5, shuffle=True, random_state=42)
# cv_scores = []
# models = []
# test_preds = []

categorical_features = ['êµ­ê°€', 'ë¶„ì•¼', 'íˆ¬ìë‹¨ê³„']  # í•„ìš”ì‹œ ì‹¤ì œ ë²”ì£¼í˜• ì»¬ëŸ¼ ì´ë¦„ìœ¼ë¡œ ìˆ˜ì •
for col in ['ì¸ìˆ˜ì—¬ë¶€', 'ìƒì¥ì—¬ë¶€']:
    X[col] = X[col].map({'No': 0, 'Yes': 1})
    X_test[col] = X_test[col].map({'No': 0, 'Yes': 1})  # testì—ë„ ë™ì¼í•˜ê²Œ ì ìš©

print(X.dtypes[X.dtypes == 'object'])

bins = np.linspace(0, 1, 6)
y_binned = np.digitize(y, bins)

# StratifiedKFold ì ìš©
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = []
models = []
test_preds = []

for fold, (train_idx, val_idx) in enumerate(skf.split(X, y_binned)):
    print(f"Fold {fold+1}")
    X_tr, y_tr = X.iloc[train_idx], y.iloc[train_idx]
    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]

    # model = CatBoostRegressor(
    #     iterations=1000,
    #     learning_rate=0.05,
    #     depth=6,
    #     cat_features=categorical_features,
    #     random_state=42,
    #     early_stopping_rounds=50,
    #     verbose=100
    # )
    # model.fit(X_tr, y_tr, eval_set=(X_val, y_val))
    model = XGBRegressor(
        n_estimators=500,
        learning_rate=0.03,
        max_depth=15,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        n_jobs=-1
    )
    model.fit(X_tr, y_tr)


    

    y_pred = model.predict(X_val)
    mae = mean_absolute_error(y_val, y_pred)
    cv_scores.append(mae)
    models.append(model)

    test_preds.append(model.predict(X_test))
    print(f"Fold {fold+1} MAE: {mae:.4f}")

plt.boxplot(cv_scores)
plt.title("MAE ë¶„í¬ (CV)")
plt.show()

print(f"\nAverage MAE across folds: {np.mean(cv_scores):.5f}")

final_test_pred = np.mean(test_preds, axis=0)

# test_preds = np.zeros(len(X_test))

# for model in models:
#     test_preds += model.predict(X_test) / kf.get_n_splits()

# final_preds = test_preds

print("\nâœ… Test ì˜ˆì¸¡ ì™„ë£Œ (KFold ëª¨ë¸ í‰ê· )")
print(final_test_pred[:10])

# ì œê±°í•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ (SHAP, Importance, ìƒê´€ê´€ê³„ ëª¨ë‘ ë°˜ì˜)
columns_to_remove = [
    'ì§ì› ìˆ˜_ê²°ì¸¡_ì´ìƒì¹˜ì—¬ë¶€',
    'ê³ ê°ìˆ˜(ë°±ë§Œëª…)_ê²°ì¸¡', 'ê³ ê°ìˆ˜(ë°±ë§Œëª…)_ê²°ì¸¡_ì´ìƒì¹˜ì—¬ë¶€',
    'ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)_ê²°ì¸¡', 'ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)_ê²°ì¸¡_ì´ìƒì¹˜ì—¬ë¶€',
    # 'ì´ íˆ¬ìê¸ˆ(ì–µì›)_ì´ìƒì¹˜ì—¬ë¶€',
    # 'ì—°ë§¤ì¶œ(ì–µì›)_ì´ìƒì¹˜ì—¬ë¶€',
    # 'ì—°ë§¤ì¶œ(ì–µì›)_ì´ìƒì¹˜ì—¬ë¶€',
    'íˆ¬ìë‹¨ê³„_ì´ìƒì¹˜ì—¬ë¶€',
    'ë¶„ì•¼_ì´ìƒì¹˜ì—¬ë¶€'
]
# ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì œê±°
columns_to_remove = [col for col in columns_to_remove if col in X.columns]

# ì œê±° í›„ ìƒˆë¡œìš´ í•™ìŠµìš© ë°ì´í„°ì…‹ ìƒì„±
X_reduced = X.drop(columns=columns_to_remove)
X_test_reduced = X_test.drop(columns=columns_to_remove, errors='ignore')
X_test_reduced = X_test_reduced[X_reduced.columns]

print(f"ğŸ” ì œê±°ëœ ì»¬ëŸ¼ ìˆ˜: {len(columns_to_remove)}ê°œ")
print(f"âœ… ìµœì¢… í•™ìŠµìš© í”¼ì²˜ ìˆ˜: X {X_reduced.shape[1]}ê°œ")
print(f"âœ… ìµœì¢… í•™ìŠµìš© í”¼ì²˜ ìˆ˜: X_test {X_test_reduced.shape[1]}ê°œ")

print(f"âœ… ìµœì¢… í•™ìŠµìš© í”¼ì²˜ ìˆ˜: {X_reduced.shape[1]}ê°œ")
print("ì‚¬ìš© ì¤‘ì¸ ì»¬ëŸ¼ ëª©ë¡:")
print(X_reduced.columns.tolist())

required_cols = ['ì´ íˆ¬ìê¸ˆ(ì–µì›)', 'ì§ì› ìˆ˜_ë¡œê·¸', 'ì—°ë§¤ì¶œ_ë¡œê·¸', 
                 'SNS íŒ”ë¡œì›Œ ìˆ˜(ë°±ë§Œëª…)', 'ì„¤ë¦½ë…„ì°¨', 
                 'ê³ ê°ìˆ˜(ë°±ë§Œëª…)', 'ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)']

missing_cols = [col for col in required_cols if col not in X_reduced.columns]
print("â— ëˆ„ë½ëœ í•„ìˆ˜ ì»¬ëŸ¼:", missing_cols)


def add_extra_features(df):
    df = df.copy()
    df['ì´íˆ¬ì_ì§ì›ë¹„'] = df['ì´ íˆ¬ìê¸ˆ(ì–µì›)'] / (df['ì§ì› ìˆ˜_ë¡œê·¸'] + 1)
    df['SNSë‹¹ë§¤ì¶œ'] = df['ì—°ë§¤ì¶œ_ë¡œê·¸'] / (df['SNS íŒ”ë¡œì›Œ ìˆ˜(ë°±ë§Œëª…)'] + 1)
    df['ì„¤ë¦½ë…„ì°¨_ì œê³±'] = df['ì„¤ë¦½ë…„ì°¨'] ** 2
    df['ê³ ê°ë‹¹ê°€ì¹˜'] = df['ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)'] / (df['ê³ ê°ìˆ˜(ë°±ë§Œëª…)'] + 1)
    df['ì—°ë§¤ì¶œ_ê¸°ì—…ê°€ì¹˜ë¹„'] = df['ì—°ë§¤ì¶œ_ë¡œê·¸'] / (df['ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)'] + 1)
    return df

X_enhanced = add_extra_features(X_reduced)
X_test_enhanced = add_extra_features(X_test[X_reduced.columns])


