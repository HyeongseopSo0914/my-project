{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-tabnet in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from pytorch-tabnet) (2.2.5)\n",
      "Requirement already satisfied: scikit_learn>0.21 in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from pytorch-tabnet) (1.6.1)\n",
      "Requirement already satisfied: scipy>1.4 in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from pytorch-tabnet) (1.15.2)\n",
      "Requirement already satisfied: torch>=1.3 in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from pytorch-tabnet) (2.7.0)\n",
      "Requirement already satisfied: tqdm>=4.36 in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from pytorch-tabnet) (4.67.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from scikit_learn>0.21->pytorch-tabnet) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from scikit_learn>0.21->pytorch-tabnet) (3.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (2025.3.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from tqdm>=4.36->pytorch-tabnet) (0.4.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from sympy>=1.13.3->torch>=1.3->pytorch-tabnet) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from jinja2->torch>=1.3->pytorch-tabnet) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pytorch-tabnet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ë¦¬\n",
    "train = train.drop(columns=['ID'], axis = 1)\n",
    "test = test.drop(columns=['ID'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     feature  importance\n",
      "4       ì§ì› ìˆ˜    0.318028\n",
      "10  ê¸°ì—…ê°€ì¹˜_í´ë˜ìŠ¤    0.141000\n",
      "8     íˆ¬ìëŒ€ë¹„ë§¤ì¶œ    0.102152\n",
      "1         êµ­ê°€    0.101361\n",
      "9   SNSíŒ”ë¡œì›Œêµ¬ê°„    0.075463\n",
      "7   ê³ ê°ìˆ˜(ë°±ë§Œëª…)    0.071945\n",
      "13   ê³ ê°ìˆ˜ëŒ€ë¹„ë§¤ì¶œ    0.065618\n",
      "0         ë‚˜ì´    0.059264\n",
      "12   íˆ¬ìê¸ˆëŒ€ë¹„ë§¤ì¶œ    0.021470\n",
      "2       ë¶„ì•¼ì ìˆ˜    0.014619\n",
      "11   ì§ì›ìˆ˜ëŒ€ë¹„ë§¤ì¶œ    0.013443\n",
      "6       ìƒì¥ì—¬ë¶€    0.008346\n",
      "3       íˆ¬ìë‹¨ê³„    0.004768\n",
      "5       ì¸ìˆ˜ì—¬ë¶€    0.002525\n",
      "ë¶„ì•¼\n",
      "í•€í…Œí¬     0.567151\n",
      "0       0.552042\n",
      "ê¸°ìˆ       0.540103\n",
      "ë¬¼ë¥˜      0.539939\n",
      "ì—ë“€í…Œí¬    0.539011\n",
      "AI      0.537079\n",
      "í‘¸ë“œí…Œí¬    0.533731\n",
      "ê²Œì„      0.532869\n",
      "ì—ë„ˆì§€     0.529545\n",
      "ì´ì»¤ë¨¸ìŠ¤    0.520482\n",
      "í—¬ìŠ¤ì¼€ì–´    0.493671\n",
      "Name: ì„±ê³µí™•ë¥ , dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 1. ì„¤ë¦½ì—°ë„ â†’ ë‚˜ì´ ë³€í™˜\n",
    "CURRENT_YEAR = 2025\n",
    "train['ë‚˜ì´'] = CURRENT_YEAR - train['ì„¤ë¦½ì—°ë„']\n",
    "test['ë‚˜ì´'] = CURRENT_YEAR - test['ì„¤ë¦½ì—°ë„']\n",
    "\n",
    "# 2. êµ­ê°€ Encoding\n",
    "le_country = LabelEncoder()\n",
    "train['êµ­ê°€'] = le_country.fit_transform(train['êµ­ê°€'].fillna('Missing'))\n",
    "test['êµ­ê°€'] = le_country.transform(test['êµ­ê°€'].fillna('Missing'))\n",
    "\n",
    "# 3. ë¶„ì•¼ ì ìˆ˜í™”\n",
    "ë¶„ì•¼_ì ìˆ˜ = {\n",
    "    'í•€í…Œí¬': 8,\n",
    "    0: 8,               # NaN ì²˜ë¦¬ëœ 0\n",
    "    'ê¸°ìˆ ': 8,\n",
    "    'ë¬¼ë¥˜': 7,\n",
    "    'ì—ë“€í…Œí¬': 7,\n",
    "    'AI': 7,\n",
    "    'í‘¸ë“œí…Œí¬': 7,\n",
    "    'ê²Œì„': 6,\n",
    "    'ì—ë„ˆì§€': 6,\n",
    "    'ì´ì»¤ë¨¸ìŠ¤': 6,\n",
    "    'í—¬ìŠ¤ì¼€ì–´': 6\n",
    "}\n",
    "\n",
    "# ë¶„ì•¼ ì ìˆ˜ ë§µí•‘\n",
    "train['ë¶„ì•¼ì ìˆ˜'] = train['ë¶„ì•¼'].map(ë¶„ì•¼_ì ìˆ˜)\n",
    "test['ë¶„ì•¼ì ìˆ˜'] = test['ë¶„ì•¼'].map(ë¶„ì•¼_ì ìˆ˜)\n",
    "\n",
    "# í˜¹ì‹œ NaN ìˆìœ¼ë©´ 0ìœ¼ë¡œ\n",
    "train['ë¶„ì•¼ì ìˆ˜'] = train['ë¶„ì•¼ì ìˆ˜'].fillna(0)\n",
    "test['ë¶„ì•¼ì ìˆ˜'] = test['ë¶„ì•¼ì ìˆ˜'].fillna(0)\n",
    "\n",
    "# 4. íˆ¬ìë‹¨ê³„ ìˆ˜ì¹˜í™”\n",
    "stage_mapping = {'Seed': 0, 'Series A': 1, 'Series B': 2, 'Series C': 3, 'IPO': 4}\n",
    "train['íˆ¬ìë‹¨ê³„'] = train['íˆ¬ìë‹¨ê³„'].map(stage_mapping).fillna(0)\n",
    "test['íˆ¬ìë‹¨ê³„'] = test['íˆ¬ìë‹¨ê³„'].map(stage_mapping).fillna(0)\n",
    "\n",
    "# 5. ì¸ìˆ˜/ìƒì¥ ì—¬ë¶€ 0/1 ë§¤í•‘\n",
    "bool_map = {'Yes': 1, 'No': 0}\n",
    "for feature in ['ì¸ìˆ˜ì—¬ë¶€', 'ìƒì¥ì—¬ë¶€']:\n",
    "    train[feature] = train[feature].map(bool_map).fillna(0)\n",
    "    test[feature] = test[feature].map(bool_map).fillna(0)\n",
    "\n",
    "# 6. íˆ¬ìëŒ€ë¹„ë§¤ì¶œ Feature ì¶”ê°€\n",
    "train['íˆ¬ìëŒ€ë¹„ë§¤ì¶œ'] = train['ì—°ë§¤ì¶œ(ì–µì›)'].fillna(0) / (train['ì´ íˆ¬ìê¸ˆ(ì–µì›)'].fillna(0) + 1)\n",
    "test['íˆ¬ìëŒ€ë¹„ë§¤ì¶œ'] = test['ì—°ë§¤ì¶œ(ì–µì›)'].fillna(0) / (test['ì´ íˆ¬ìê¸ˆ(ì–µì›)'].fillna(0) + 1)\n",
    "\n",
    "# 7. SNS íŒ”ë¡œì›Œ ìˆ˜ êµ¬ê°„í™”\n",
    "def sns_bin(x):\n",
    "    if x < 1:\n",
    "        return 0\n",
    "    elif x < 3:\n",
    "        return 1\n",
    "    elif x < 5:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "train['SNSíŒ”ë¡œì›Œêµ¬ê°„'] = train['SNS íŒ”ë¡œì›Œ ìˆ˜(ë°±ë§Œëª…)'].apply(lambda x: sns_bin(x) if not pd.isna(x) else 0)\n",
    "test['SNSíŒ”ë¡œì›Œêµ¬ê°„'] = test['SNS íŒ”ë¡œì›Œ ìˆ˜(ë°±ë§Œëª…)'].apply(lambda x: sns_bin(x) if not pd.isna(x) else 0)\n",
    "\n",
    "# 8. ê¸°ì—…ê°€ì¹˜ ìˆ˜ì¹˜í™”\n",
    "def parse_value(x):\n",
    "    if pd.isnull(x):\n",
    "        return 0\n",
    "    elif '6000' in str(x):\n",
    "        return 6\n",
    "    elif '4500' in str(x):\n",
    "        return 5\n",
    "    elif '3500' in str(x):\n",
    "        return 4\n",
    "    elif '2500' in str(x):\n",
    "        return 3\n",
    "    elif '1500' in str(x):\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "train['ê¸°ì—…ê°€ì¹˜_í´ë˜ìŠ¤'] = train['ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)'].apply(parse_value)\n",
    "test['ê¸°ì—…ê°€ì¹˜_í´ë˜ìŠ¤'] = test['ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)'].apply(parse_value)\n",
    "\n",
    "train['ì§ì›ìˆ˜ëŒ€ë¹„ë§¤ì¶œ'] = train['ì—°ë§¤ì¶œ(ì–µì›)'] / (train['ì§ì› ìˆ˜'] + 1)\n",
    "test['ì§ì›ìˆ˜ëŒ€ë¹„ë§¤ì¶œ'] = test['ì—°ë§¤ì¶œ(ì–µì›)'] / (test['ì§ì› ìˆ˜'] + 1)\n",
    "\n",
    "train['íˆ¬ìê¸ˆëŒ€ë¹„ë§¤ì¶œ'] = train['ì—°ë§¤ì¶œ(ì–µì›)'] / (train['ì´ íˆ¬ìê¸ˆ(ì–µì›)'] + 1)\n",
    "test['íˆ¬ìê¸ˆëŒ€ë¹„ë§¤ì¶œ'] = test['ì—°ë§¤ì¶œ(ì–µì›)'] / (test['ì´ íˆ¬ìê¸ˆ(ì–µì›)'] + 1)\n",
    "\n",
    "train['ê³ ê°ìˆ˜ëŒ€ë¹„ë§¤ì¶œ'] = train['ì—°ë§¤ì¶œ(ì–µì›)'] / (train['ê³ ê°ìˆ˜(ë°±ë§Œëª…)'] + 1)\n",
    "test['ê³ ê°ìˆ˜ëŒ€ë¹„ë§¤ì¶œ'] = test['ì—°ë§¤ì¶œ(ì–µì›)'] / (test['ê³ ê°ìˆ˜(ë°±ë§Œëª…)'] + 1)\n",
    "\n",
    "\n",
    "# 9. ê²°ì¸¡ì¹˜ëŠ” ëª¨ë‘ 0ìœ¼ë¡œ ëŒ€ì²´\n",
    "train = train.fillna(0)\n",
    "test = test.fillna(0)\n",
    "\n",
    "# 10. ìµœì¢… feature ë¦¬ìŠ¤íŠ¸\n",
    "features = [\n",
    "    'ë‚˜ì´', 'êµ­ê°€', 'ë¶„ì•¼ì ìˆ˜', 'íˆ¬ìë‹¨ê³„', 'ì§ì› ìˆ˜',\n",
    "    'ì¸ìˆ˜ì—¬ë¶€', 'ìƒì¥ì—¬ë¶€', 'ê³ ê°ìˆ˜(ë°±ë§Œëª…)', 'íˆ¬ìëŒ€ë¹„ë§¤ì¶œ',\n",
    "    'SNSíŒ”ë¡œì›Œêµ¬ê°„', 'ê¸°ì—…ê°€ì¹˜_í´ë˜ìŠ¤', 'ì§ì›ìˆ˜ëŒ€ë¹„ë§¤ì¶œ', 'íˆ¬ìê¸ˆëŒ€ë¹„ë§¤ì¶œ', 'ê³ ê°ìˆ˜ëŒ€ë¹„ë§¤ì¶œ'\n",
    "]\n",
    "\n",
    "# 11. TabNetìš© cat_idxs, cat_dims ì„¤ì •\n",
    "category_features = ['êµ­ê°€', 'ë¶„ì•¼ì ìˆ˜', 'íˆ¬ìë‹¨ê³„', 'SNSíŒ”ë¡œì›Œêµ¬ê°„', 'ê¸°ì—…ê°€ì¹˜_í´ë˜ìŠ¤']\n",
    "cat_idxs = [features.index(col) for col in category_features]\n",
    "cat_dims = [int(train[col].max()) + 2 for col in category_features]\n",
    "\n",
    "\n",
    "models = []  # ëª¨ë¸ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "# í•™ìŠµìš© ì½”ë“œ (ê°„ë‹¨ ë²„ì „)\n",
    "model = TabNetRegressor(\n",
    "    cat_idxs=cat_idxs,\n",
    "    cat_dims=cat_dims,\n",
    "    seed=42222,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train=train[features].values,\n",
    "    y_train=train['ì„±ê³µí™•ë¥ '].values.reshape(-1, 1),\n",
    "    max_epochs=50,\n",
    "    patience=10,\n",
    "    batch_size=512,\n",
    "    virtual_batch_size=128,\n",
    "    eval_metric=['mae']\n",
    ")\n",
    "models.append(model) \n",
    "# ì˜ˆë¥¼ ë“¤ì–´ ëª¨ë¸ í•˜ë‚˜ (fold 0ë²ˆ ëª¨ë¸)ë¡œ í•´ë³¼ê²Œ\n",
    "model = models[0]\n",
    "\n",
    "# 1. Feature Importance ê°€ì ¸ì˜¤ê¸°\n",
    "feature_importance = model.feature_importances_\n",
    "\n",
    "# 2. Feature ì´ë¦„ê³¼ importance ë§¤ì¹­\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance': feature_importance\n",
    "})\n",
    "\n",
    "# 3. ì¤‘ìš”ë„ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬\n",
    "importance_df = importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# 4. ê²°ê³¼ ì¶œë ¥\n",
    "print(importance_df)\n",
    "\n",
    "ë¶„ì•¼ë³„_ì„±ê³µë¥  = train.groupby('ë¶„ì•¼')['ì„±ê³µí™•ë¥ '].mean().sort_values(ascending=False)\n",
    "print(ë¶„ì•¼ë³„_ì„±ê³µë¥ )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Fold 1/5\n",
      "â–¶ Pretraining...\n",
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 14 and best_val_0_unsup_loss_numpy = 10107287.0\n",
      "â–¶ Fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 11 and best_val_0_mae = 0.22078\n",
      "\n",
      "ğŸ” Fold 2/5\n",
      "â–¶ Pretraining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_0_unsup_loss_numpy = 1155383689216.0\n",
      "â–¶ Fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 56 with best_epoch = 46 and best_val_0_mae = 0.20788\n",
      "\n",
      "ğŸ” Fold 3/5\n",
      "â–¶ Pretraining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 53 with best_epoch = 43 and best_val_0_unsup_loss_numpy = 3255687.5\n",
      "â–¶ Fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 31 with best_epoch = 21 and best_val_0_mae = 0.20303\n",
      "\n",
      "ğŸ” Fold 4/5\n",
      "â–¶ Pretraining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_0_unsup_loss_numpy = 1876983808000.0\n",
      "â–¶ Fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 57 with best_epoch = 47 and best_val_0_mae = 0.20281\n",
      "\n",
      "ğŸ” Fold 5/5\n",
      "â–¶ Pretraining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_0_unsup_loss_numpy = -40.24195861816406\n",
      "â–¶ Fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 37 with best_epoch = 27 and best_val_0_mae = 0.20727\n",
      "\n",
      "âœ… ëª¨ë“  fold ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\n",
      "CV Scores (MAE): [0.22077590191064905, 0.20787698324748446, 0.2030319442340306, 0.20280840533801486, 0.20726974452563696]\n",
      "Mean CV Score (MAE): 0.20835259585116317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "# 1. íƒ€ê²Ÿ ì§€ì •\n",
    "target = train['ì„±ê³µí™•ë¥ ']  \n",
    "X = train[features]\n",
    "y = target\n",
    "\n",
    "# 2. KFold ì„¤ì •\n",
    "N_FOLDS = 5\n",
    "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "models = [] # ëª¨ë¸ ì €ì¥ ë¦¬ìŠ¤íŠ¸\n",
    "cv_scores = []\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"\\nğŸ” Fold {fold+1}/{N_FOLDS}\")\n",
    "    \n",
    "    X_train = X.iloc[train_idx].values   # ë¬´ì¡°ê±´ .values ë¶™ì—¬\n",
    "    y_train = y.iloc[train_idx].values.reshape(-1, 1)\n",
    "\n",
    "    X_valid = X.iloc[valid_idx].values   # ë¬´ì¡°ê±´ .values ë¶™ì—¬\n",
    "    y_valid = y.iloc[valid_idx].values.reshape(-1, 1)\n",
    "    \n",
    "    # for i, idx in enumerate(cat_idxs):\n",
    "    #     col_values = X_train[:, idx]\n",
    "    #     max_val = col_values.max()\n",
    "    #     expected_dim = cat_dims[i]\n",
    "    #     print(f\"Feature {features[idx]}: max value={max_val}, expected dim={expected_dim}\")\n",
    "\n",
    "    # 3. ë¹„ì§€ë„ ì‚¬ì „í•™ìŠµ (Pretraining)\n",
    "    print(\"â–¶ Pretraining...\")\n",
    "    pretrainer = TabNetPretrainer(\n",
    "        cat_idxs=cat_idxs,\n",
    "        cat_dims=cat_dims,\n",
    "        seed=42 + fold,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    pretrainer.fit(\n",
    "        X_train=X_train,\n",
    "        eval_set=[X_valid],\n",
    "        max_epochs=100,\n",
    "        batch_size=512,\n",
    "        virtual_batch_size=64,\n",
    "        patience=10\n",
    "    )\n",
    "    \n",
    "    # 4. ì§€ë„ Fine-tuning\n",
    "    print(\"â–¶ Fine-tuning...\")\n",
    "    model = TabNetRegressor(\n",
    "        cat_idxs=cat_idxs,\n",
    "        cat_dims=cat_dims,\n",
    "        seed=42 + fold,\n",
    "        verbose=0,\n",
    "        optimizer_fn=torch.optim.AdamW\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train=X_train, y_train=y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        from_unsupervised=pretrainer,\n",
    "        eval_metric=['mae'],\n",
    "        max_epochs=100,\n",
    "        patience=10\n",
    "    )\n",
    "\n",
    "    # 5. ëª¨ë¸ ì €ì¥\n",
    "    models.append(model)\n",
    "    cv_scores.append(model.best_cost)\n",
    "\n",
    "print(\"\\nâœ… ëª¨ë“  fold ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
    "print(\"CV Scores (MAE):\", cv_scores)\n",
    "print(\"Mean CV Score (MAE):\", np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict with fold 1\n"
     ]
    }
   ],
   "source": [
    "# ì €ì¥ëœ ëª¨ë¸ë“¤ë¡œ ì˜ˆì¸¡\n",
    "predictions_list = []\n",
    "\n",
    "for fold, model in enumerate(models):\n",
    "    print(f\"Predict with fold {fold+1}\")\n",
    "    preds = model.predict(test[features].values)\n",
    "    predictions_list.append(preds)\n",
    "\n",
    "# í‰ê·  ì˜ˆì¸¡\n",
    "final_predictions = np.mean(predictions_list, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['ì„±ê³µí™•ë¥ '] = final_predictions\n",
    "sample_submission.to_csv('./baseline_submission.csv', index = False, encoding = 'utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
