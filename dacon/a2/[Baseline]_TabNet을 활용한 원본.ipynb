{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-tabnet in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from pytorch-tabnet) (2.2.5)\n",
      "Requirement already satisfied: scikit_learn>0.21 in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from pytorch-tabnet) (1.6.1)\n",
      "Requirement already satisfied: scipy>1.4 in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from pytorch-tabnet) (1.15.2)\n",
      "Requirement already satisfied: torch>=1.3 in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from pytorch-tabnet) (2.7.0)\n",
      "Requirement already satisfied: tqdm>=4.36 in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from pytorch-tabnet) (4.67.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from scikit_learn>0.21->pytorch-tabnet) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from scikit_learn>0.21->pytorch-tabnet) (3.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (2025.3.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from tqdm>=4.36->pytorch-tabnet) (0.4.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from sympy>=1.13.3->torch>=1.3->pytorch-tabnet) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from jinja2->torch>=1.3->pytorch-tabnet) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pytorch-tabnet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#특성과 타겟 변수 분리\n",
    "train = train.drop(columns=['ID'], axis = 1)\n",
    "test = test.drop(columns=['ID'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\human\\AppData\\Local\\Temp\\ipykernel_5868\\3668920127.py:15: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train[feature] = train[feature].fillna('Missing')\n",
      "C:\\Users\\human\\AppData\\Local\\Temp\\ipykernel_5868\\3668920127.py:16: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test[feature] = test[feature].fillna('Missing')\n"
     ]
    }
   ],
   "source": [
    "# 설립연도 타입 변환 (int -> object)\n",
    "train['설립연도'] =train['설립연도'].astype('object')\n",
    "test['설립연도'] =test['설립연도'].astype('object')\n",
    "\n",
    "category_features = ['설립연도','국가','분야','투자단계','기업가치(백억원)']\n",
    "numeric_features = ['직원 수','고객수(백만명)','총 투자금(억원)','연매출(억원)','SNS 팔로워 수(백만명)']\n",
    "bool_features = ['인수여부','상장여부']\n",
    "\n",
    "# LabelEncoder 객체를 각 범주형 feature별로 따로 저장하여 사용\n",
    "encoders = {}\n",
    "\n",
    "# 범주형 데이터를 encoding\n",
    "for feature in category_features:\n",
    "    encoders[feature] = LabelEncoder()\n",
    "    train[feature] = train[feature].fillna('Missing')\n",
    "    test[feature] = test[feature].fillna('Missing')\n",
    "    train[feature] = encoders[feature].fit_transform(train[feature])\n",
    "    test[feature] = encoders[feature].transform(test[feature])\n",
    "\n",
    "# 불리언 값을 0과 1로 변환 ('Yes' → 1, 'No' → 0 으로 변환)\n",
    "bool_map = {'Yes': 1, 'No': 0}\n",
    "\n",
    "for feature in bool_features:\n",
    "    train[feature] = train[feature].map(bool_map)\n",
    "    test[feature] = test[feature].map(bool_map)\n",
    "\n",
    "# 수치형 변수 결측치를 평균값으로 대체\n",
    "for feature in numeric_features:\n",
    "    mean_value = train[feature].mean()\n",
    "    train[feature] = train[feature].fillna(mean_value)\n",
    "    test[feature] = test[feature].fillna(mean_value)\n",
    "\n",
    "# TabNet용 범주형 변수 인덱스(cat_idxs) 및 차원(cat_dims) 설정\n",
    "features = [col for col in train.columns if col != '성공확률']\n",
    "cat_idxs = [features.index(col) for col in category_features]\n",
    "cat_dims = [train[col].nunique() for col in category_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Fold 1/5\n",
      "Feature 설립연도: max value=22.0, expected dim=23\n",
      "Feature 국가: max value=9.0, expected dim=10\n",
      "Feature 분야: max value=10.0, expected dim=11\n",
      "Feature 투자단계: max value=4.0, expected dim=5\n",
      "Feature 기업가치(백억원): max value=5.0, expected dim=6\n",
      "▶ Pretraining...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m▶ Pretraining...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m pretrainer = TabNetPretrainer(\n\u001b[32m     31\u001b[39m     cat_idxs=cat_idxs,\n\u001b[32m     32\u001b[39m     cat_dims=cat_dims,\n\u001b[32m     33\u001b[39m     seed=\u001b[32m42\u001b[39m,\n\u001b[32m     34\u001b[39m     verbose=\u001b[32m0\u001b[39m\n\u001b[32m     35\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[43mpretrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvirtual_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\n\u001b[32m     43\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# pretrainer.fit(\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m#     X_train=X_train,\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m#     eval_set=[X_valid],\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     52\u001b[39m \n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# 지도 학습 \u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m▶ Fine-tuning...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\pretraining.py:125\u001b[39m, in \u001b[36mTabNetPretrainer.fit\u001b[39m\u001b[34m(self, X_train, eval_set, eval_name, loss_fn, pretraining_ratio, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, warm_start)\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    123\u001b[39m     \u001b[38;5;28mself\u001b[39m.loss_fn = loss_fn\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m \u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[38;5;28mself\u001b[39m.update_fit_params(\n\u001b[32m    128\u001b[39m     weights,\n\u001b[32m    129\u001b[39m )\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m# Validate and reformat eval set depending on training data\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\utils.py:507\u001b[39m, in \u001b[36mcheck_input\u001b[39m\u001b[34m(X)\u001b[39m\n\u001b[32m    505\u001b[39m     err_message = \u001b[33m\"\u001b[39m\u001b[33mPandas DataFrame are not supported: apply X.values when calling fit\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    506\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(err_message)\n\u001b[32m--> \u001b[39m\u001b[32m507\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\sklearn\\utils\\validation.py:1107\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1102\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m expected <= 2.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1103\u001b[39m         % (array.ndim, estimator_name)\n\u001b[32m   1104\u001b[39m     )\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1107\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1115\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1116\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\sklearn\\utils\\validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\sklearn\\utils\\validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "# 타겟 지정\n",
    "target = train['성공확률']  \n",
    "X = train[features]\n",
    "y = target\n",
    "\n",
    "# KFold 설정\n",
    "N_FOLDS = 5\n",
    "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "models = [] # 모델 저장 리스트\n",
    "cv_scores = []\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"\\n🔁 Fold {fold+1}/{N_FOLDS}\")\n",
    "    \n",
    "    X_train = X.iloc[train_idx].values\n",
    "    y_train = y.iloc[train_idx].values.reshape(-1, 1)\n",
    "    \n",
    "    X_valid = X.iloc[valid_idx].values\n",
    "    y_valid = y.iloc[valid_idx].values.reshape(-1, 1)\n",
    "\n",
    "    X_train = np.nan_to_num(X_train, nan=0)\n",
    "    X_valid = np.nan_to_num(X_valid, nan=0)\n",
    "\n",
    "    for i, idx in enumerate(cat_idxs):\n",
    "        col_values = X_train[:, idx]\n",
    "        max_val = col_values.max()\n",
    "        expected_dim = cat_dims[i]\n",
    "        print(f\"Feature {features[idx]}: max value={max_val}, expected dim={expected_dim}\")\n",
    "    \n",
    "    # 비지도 사전학습\n",
    "    print(\"▶ Pretraining...\")\n",
    "\n",
    "    pretrainer = TabNetPretrainer(\n",
    "        cat_idxs=cat_idxs,\n",
    "        cat_dims=cat_dims,\n",
    "        seed=42,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "\n",
    "    pretrainer.fit(\n",
    "        X_train=X_train,\n",
    "        max_epochs=100,\n",
    "        batch_size=512,\n",
    "        virtual_batch_size=64\n",
    "    )\n",
    "    # pretrainer.fit(\n",
    "    #     X_train=X_train,\n",
    "    #     eval_set=[X_valid],\n",
    "    #     max_epochs=100,\n",
    "    #     batch_size=512,\n",
    "    #     virtual_batch_size=64,\n",
    "    #     patience=10\n",
    "    # )\n",
    "\n",
    "    # 지도 학습 \n",
    "    print(\"▶ Fine-tuning...\")\n",
    "    model = TabNetRegressor(\n",
    "        cat_idxs=cat_idxs,\n",
    "        cat_dims=cat_dims,\n",
    "        seed=42,\n",
    "        verbose=0,\n",
    "        optimizer_fn=torch.optim.AdamW \n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train=X_train, y_train=y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        from_unsupervised=pretrainer,\n",
    "        eval_metric=['mae'],\n",
    "        max_epochs=100,\n",
    "        patience=10\n",
    "    )\n",
    "\n",
    "    # 모델을 메모리에 저장\n",
    "    models.append(model)\n",
    "    cv_scores.append(model.best_cost)\n",
    "\n",
    "print(\"\\n✅ 모든 fold 모델 학습 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 모델들로 예측\n",
    "predictions_list = []\n",
    "\n",
    "for fold, model in enumerate(models):\n",
    "    print(f\"Predict with fold {fold+1}\")\n",
    "    preds = model.predict(test[features].values)\n",
    "    predictions_list.append(preds)\n",
    "\n",
    "# 평균 예측\n",
    "final_predictions = np.mean(predictions_list, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['성공확률'] = final_predictions\n",
    "sample_submission.to_csv('./baseline_submission.csv', index = False, encoding = 'utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
