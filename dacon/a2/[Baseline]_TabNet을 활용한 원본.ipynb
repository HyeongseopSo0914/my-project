{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-tabnet in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from pytorch-tabnet) (2.2.5)\n",
      "Requirement already satisfied: scikit_learn>0.21 in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from pytorch-tabnet) (1.6.1)\n",
      "Requirement already satisfied: scipy>1.4 in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from pytorch-tabnet) (1.15.2)\n",
      "Requirement already satisfied: torch>=1.3 in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from pytorch-tabnet) (2.7.0)\n",
      "Requirement already satisfied: tqdm>=4.36 in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from pytorch-tabnet) (4.67.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from scikit_learn>0.21->pytorch-tabnet) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from scikit_learn>0.21->pytorch-tabnet) (3.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (2025.3.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from tqdm>=4.36->pytorch-tabnet) (0.4.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from sympy>=1.13.3->torch>=1.3->pytorch-tabnet) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\human\\.conda\\envs\\dacon\\lib\\site-packages (from jinja2->torch>=1.3->pytorch-tabnet) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pytorch-tabnet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ÌäπÏÑ±Í≥º ÌÉÄÍ≤ü Î≥ÄÏàò Î∂ÑÎ¶¨\n",
    "train = train.drop(columns=['ID'], axis = 1)\n",
    "test = test.drop(columns=['ID'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\human\\AppData\\Local\\Temp\\ipykernel_5868\\3668920127.py:15: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train[feature] = train[feature].fillna('Missing')\n",
      "C:\\Users\\human\\AppData\\Local\\Temp\\ipykernel_5868\\3668920127.py:16: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test[feature] = test[feature].fillna('Missing')\n"
     ]
    }
   ],
   "source": [
    "# ÏÑ§Î¶ΩÏó∞ÎèÑ ÌÉÄÏûÖ Î≥ÄÌôò (int -> object)\n",
    "train['ÏÑ§Î¶ΩÏó∞ÎèÑ'] =train['ÏÑ§Î¶ΩÏó∞ÎèÑ'].astype('object')\n",
    "test['ÏÑ§Î¶ΩÏó∞ÎèÑ'] =test['ÏÑ§Î¶ΩÏó∞ÎèÑ'].astype('object')\n",
    "\n",
    "category_features = ['ÏÑ§Î¶ΩÏó∞ÎèÑ','Íµ≠Í∞Ä','Î∂ÑÏïº','Ìà¨ÏûêÎã®Í≥Ñ','Í∏∞ÏóÖÍ∞ÄÏπò(Î∞±ÏñµÏõê)']\n",
    "numeric_features = ['ÏßÅÏõê Ïàò','Í≥†Í∞ùÏàò(Î∞±ÎßåÎ™Ö)','Ï¥ù Ìà¨ÏûêÍ∏à(ÏñµÏõê)','Ïó∞Îß§Ï∂ú(ÏñµÏõê)','SNS ÌåîÎ°úÏõå Ïàò(Î∞±ÎßåÎ™Ö)']\n",
    "bool_features = ['Ïù∏ÏàòÏó¨Î∂Ä','ÏÉÅÏû•Ïó¨Î∂Ä']\n",
    "\n",
    "# LabelEncoder Í∞ùÏ≤¥Î•º Í∞Å Î≤îÏ£ºÌòï featureÎ≥ÑÎ°ú Îî∞Î°ú Ï†ÄÏû•ÌïòÏó¨ ÏÇ¨Ïö©\n",
    "encoders = {}\n",
    "\n",
    "# Î≤îÏ£ºÌòï Îç∞Ïù¥ÌÑ∞Î•º encoding\n",
    "for feature in category_features:\n",
    "    encoders[feature] = LabelEncoder()\n",
    "    train[feature] = train[feature].fillna('Missing')\n",
    "    test[feature] = test[feature].fillna('Missing')\n",
    "    train[feature] = encoders[feature].fit_transform(train[feature])\n",
    "    test[feature] = encoders[feature].transform(test[feature])\n",
    "\n",
    "# Î∂àÎ¶¨Ïñ∏ Í∞íÏùÑ 0Í≥º 1Î°ú Î≥ÄÌôò ('Yes' ‚Üí 1, 'No' ‚Üí 0 ÏúºÎ°ú Î≥ÄÌôò)\n",
    "bool_map = {'Yes': 1, 'No': 0}\n",
    "\n",
    "for feature in bool_features:\n",
    "    train[feature] = train[feature].map(bool_map)\n",
    "    test[feature] = test[feature].map(bool_map)\n",
    "\n",
    "# ÏàòÏπòÌòï Î≥ÄÏàò Í≤∞Ï∏°ÏπòÎ•º ÌèâÍ∑†Í∞íÏúºÎ°ú ÎåÄÏ≤¥\n",
    "for feature in numeric_features:\n",
    "    mean_value = train[feature].mean()\n",
    "    train[feature] = train[feature].fillna(mean_value)\n",
    "    test[feature] = test[feature].fillna(mean_value)\n",
    "\n",
    "# TabNetÏö© Î≤îÏ£ºÌòï Î≥ÄÏàò Ïù∏Îç±Ïä§(cat_idxs) Î∞è Ï∞®Ïõê(cat_dims) ÏÑ§Ï†ï\n",
    "features = [col for col in train.columns if col != 'ÏÑ±Í≥µÌôïÎ•†']\n",
    "cat_idxs = [features.index(col) for col in category_features]\n",
    "cat_dims = [train[col].nunique() for col in category_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÅ Fold 1/5\n",
      "Feature ÏÑ§Î¶ΩÏó∞ÎèÑ: max value=22.0, expected dim=23\n",
      "Feature Íµ≠Í∞Ä: max value=9.0, expected dim=10\n",
      "Feature Î∂ÑÏïº: max value=10.0, expected dim=11\n",
      "Feature Ìà¨ÏûêÎã®Í≥Ñ: max value=4.0, expected dim=5\n",
      "Feature Í∏∞ÏóÖÍ∞ÄÏπò(Î∞±ÏñµÏõê): max value=5.0, expected dim=6\n",
      "‚ñ∂ Pretraining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂ Fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 30 with best_epoch = 20 and best_val_0_mae = 0.20555\n",
      "\n",
      "üîÅ Fold 2/5\n",
      "Feature ÏÑ§Î¶ΩÏó∞ÎèÑ: max value=22.0, expected dim=23\n",
      "Feature Íµ≠Í∞Ä: max value=9.0, expected dim=10\n",
      "Feature Î∂ÑÏïº: max value=10.0, expected dim=11\n",
      "Feature Ìà¨ÏûêÎã®Í≥Ñ: max value=4.0, expected dim=5\n",
      "Feature Í∏∞ÏóÖÍ∞ÄÏπò(Î∞±ÏñµÏõê): max value=5.0, expected dim=6\n",
      "‚ñ∂ Pretraining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂ Fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 61 with best_epoch = 51 and best_val_0_mae = 0.20428\n",
      "\n",
      "üîÅ Fold 3/5\n",
      "Feature ÏÑ§Î¶ΩÏó∞ÎèÑ: max value=22.0, expected dim=23\n",
      "Feature Íµ≠Í∞Ä: max value=9.0, expected dim=10\n",
      "Feature Î∂ÑÏïº: max value=10.0, expected dim=11\n",
      "Feature Ìà¨ÏûêÎã®Í≥Ñ: max value=4.0, expected dim=5\n",
      "Feature Í∏∞ÏóÖÍ∞ÄÏπò(Î∞±ÏñµÏõê): max value=5.0, expected dim=6\n",
      "‚ñ∂ Pretraining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂ Fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 46 with best_epoch = 36 and best_val_0_mae = 0.20185\n",
      "\n",
      "üîÅ Fold 4/5\n",
      "Feature ÏÑ§Î¶ΩÏó∞ÎèÑ: max value=22.0, expected dim=23\n",
      "Feature Íµ≠Í∞Ä: max value=9.0, expected dim=10\n",
      "Feature Î∂ÑÏïº: max value=10.0, expected dim=11\n",
      "Feature Ìà¨ÏûêÎã®Í≥Ñ: max value=4.0, expected dim=5\n",
      "Feature Í∏∞ÏóÖÍ∞ÄÏπò(Î∞±ÏñµÏõê): max value=5.0, expected dim=6\n",
      "‚ñ∂ Pretraining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂ Fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 34 with best_epoch = 24 and best_val_0_mae = 0.20287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÅ Fold 5/5\n",
      "Feature ÏÑ§Î¶ΩÏó∞ÎèÑ: max value=22.0, expected dim=23\n",
      "Feature Íµ≠Í∞Ä: max value=9.0, expected dim=10\n",
      "Feature Î∂ÑÏïº: max value=10.0, expected dim=11\n",
      "Feature Ìà¨ÏûêÎã®Í≥Ñ: max value=4.0, expected dim=5\n",
      "Feature Í∏∞ÏóÖÍ∞ÄÏπò(Î∞±ÏñµÏõê): max value=5.0, expected dim=6\n",
      "‚ñ∂ Pretraining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂ Fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 31 with best_epoch = 21 and best_val_0_mae = 0.20569\n",
      "\n",
      "‚úÖ Î™®Îì† fold Î™®Îç∏ ÌïôÏäµ ÏôÑÎ£å!\n",
      "CV Scores (MAE): [0.20554833059033306, 0.2042812625476292, 0.20185168299674988, 0.20286698811394827, 0.20569271735463826]\n",
      "Mean CV Score (MAE): 0.20404819632065974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "# ÌÉÄÍ≤ü ÏßÄÏ†ï\n",
    "target = train['ÏÑ±Í≥µÌôïÎ•†']  \n",
    "X = train[features]\n",
    "y = target\n",
    "\n",
    "# KFold ÏÑ§Ï†ï\n",
    "N_FOLDS = 5\n",
    "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "models = [] # Î™®Îç∏ Ï†ÄÏû• Î¶¨Ïä§Ìä∏\n",
    "cv_scores = []\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"\\nüîÅ Fold {fold+1}/{N_FOLDS}\")\n",
    "    \n",
    "    X_train = X.iloc[train_idx].values\n",
    "    y_train = y.iloc[train_idx].values.reshape(-1, 1)\n",
    "    \n",
    "    X_valid = X.iloc[valid_idx].values\n",
    "    y_valid = y.iloc[valid_idx].values.reshape(-1, 1)\n",
    "\n",
    "    X_train = np.nan_to_num(X_train, nan=0)\n",
    "    X_valid = np.nan_to_num(X_valid, nan=0)\n",
    "\n",
    "    for i, idx in enumerate(cat_idxs):\n",
    "        col_values = X_train[:, idx]\n",
    "        max_val = col_values.max()\n",
    "        expected_dim = cat_dims[i]\n",
    "        print(f\"Feature {features[idx]}: max value={max_val}, expected dim={expected_dim}\")\n",
    "    \n",
    "    # ÎπÑÏßÄÎèÑ ÏÇ¨Ï†ÑÌïôÏäµ\n",
    "    print(\"‚ñ∂ Pretraining...\")\n",
    "\n",
    "    pretrainer = TabNetPretrainer(\n",
    "        cat_idxs=cat_idxs,\n",
    "        cat_dims=cat_dims,\n",
    "        seed=42,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "\n",
    "    pretrainer.fit(\n",
    "        X_train=X_train,\n",
    "        max_epochs=100,\n",
    "        batch_size=512,\n",
    "        virtual_batch_size=64\n",
    "    )\n",
    "    # pretrainer.fit(\n",
    "    #     X_train=X_train,\n",
    "    #     eval_set=[X_valid],\n",
    "    #     max_epochs=100,\n",
    "    #     batch_size=512,\n",
    "    #     virtual_batch_size=64,\n",
    "    #     patience=10\n",
    "    # )\n",
    "\n",
    "    # ÏßÄÎèÑ ÌïôÏäµ \n",
    "    print(\"‚ñ∂ Fine-tuning...\")\n",
    "    model = TabNetRegressor(\n",
    "        cat_idxs=cat_idxs,\n",
    "        cat_dims=cat_dims,\n",
    "        seed=42,\n",
    "        verbose=0,\n",
    "        optimizer_fn=torch.optim.AdamW \n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train=X_train, y_train=y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        from_unsupervised=pretrainer,\n",
    "        eval_metric=['mae'],\n",
    "        max_epochs=100,\n",
    "        patience=10\n",
    "    )\n",
    "\n",
    "    # Î™®Îç∏ÏùÑ Î©îÎ™®Î¶¨Ïóê Ï†ÄÏû•\n",
    "    models.append(model)\n",
    "    cv_scores.append(model.best_cost)\n",
    "\n",
    "print(\"\\n‚úÖ Î™®Îì† fold Î™®Îç∏ ÌïôÏäµ ÏôÑÎ£å!\")\n",
    "print(\"CV Scores (MAE):\", cv_scores)\n",
    "print(\"Mean CV Score (MAE):\", np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict with fold 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "index -1 is out of bounds for dimension 1 with size 12",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m fold, model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(models):\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPredict with fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     preds = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     predictions_list.append(preds)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# ÌèâÍ∑† ÏòàÏ∏°\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:312\u001b[39m, in \u001b[36mTabModel.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch_nb, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[32m    311\u001b[39m     data = data.to(\u001b[38;5;28mself\u001b[39m.device).float()\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m     output, M_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    313\u001b[39m     predictions = output.cpu().detach().numpy()\n\u001b[32m    314\u001b[39m     results.append(predictions)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\tab_network.py:616\u001b[39m, in \u001b[36mTabNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    614\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    615\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.embedder(x)\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtabnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\tab_network.py:492\u001b[39m, in \u001b[36mTabNetNoEmbeddings.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    491\u001b[39m     res = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m     steps_output, M_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m     res = torch.sum(torch.stack(steps_output, dim=\u001b[32m0\u001b[39m), dim=\u001b[32m0\u001b[39m)\n\u001b[32m    495\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_multi_task:\n\u001b[32m    496\u001b[39m         \u001b[38;5;66;03m# Result will be in list format\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\tab_network.py:172\u001b[39m, in \u001b[36mTabNetEncoder.forward\u001b[39m\u001b[34m(self, x, prior)\u001b[39m\n\u001b[32m    170\u001b[39m steps_output = []\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.n_steps):\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     M = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43matt_transformers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m     M_loss += torch.mean(\n\u001b[32m    174\u001b[39m         torch.sum(torch.mul(M, torch.log(M + \u001b[38;5;28mself\u001b[39m.epsilon)), dim=\u001b[32m1\u001b[39m)\n\u001b[32m    175\u001b[39m     )\n\u001b[32m    176\u001b[39m     \u001b[38;5;66;03m# update prior\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\tab_network.py:671\u001b[39m, in \u001b[36mAttentiveTransformer.forward\u001b[39m\u001b[34m(self, priors, processed_feat)\u001b[39m\n\u001b[32m    669\u001b[39m x = \u001b[38;5;28mself\u001b[39m.bn(x)\n\u001b[32m    670\u001b[39m x = torch.mul(x, priors)\n\u001b[32m--> \u001b[39m\u001b[32m671\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mselector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\sparsemax.py:109\u001b[39m, in \u001b[36mSparsemax.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msparsemax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\torch\\autograd\\function.py:575\u001b[39m, in \u001b[36mFunction.apply\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._C._are_functorch_transforms_active():\n\u001b[32m    573\u001b[39m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[32m    574\u001b[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[32m    578\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    579\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    580\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    581\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstaticmethod. For more details, please see \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    582\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    583\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\sparsemax.py:52\u001b[39m, in \u001b[36mSparsemaxFunction.forward\u001b[39m\u001b[34m(ctx, input, dim)\u001b[39m\n\u001b[32m     50\u001b[39m max_val, _ = \u001b[38;5;28minput\u001b[39m.max(dim=dim, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     51\u001b[39m \u001b[38;5;28minput\u001b[39m -= max_val  \u001b[38;5;66;03m# same numerical stability trick as for softmax\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m tau, supp_size = \u001b[43mSparsemaxFunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_threshold_and_support\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m output = torch.clamp(\u001b[38;5;28minput\u001b[39m - tau, \u001b[38;5;28mmin\u001b[39m=\u001b[32m0\u001b[39m)\n\u001b[32m     54\u001b[39m ctx.save_for_backward(supp_size, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\pytorch_tabnet\\sparsemax.py:94\u001b[39m, in \u001b[36mSparsemaxFunction._threshold_and_support\u001b[39m\u001b[34m(input, dim)\u001b[39m\n\u001b[32m     91\u001b[39m support = rhos * input_srt > input_cumsum\n\u001b[32m     93\u001b[39m support_size = support.sum(dim=dim).unsqueeze(dim)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m tau = \u001b[43minput_cumsum\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msupport_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m tau /= support_size.to(\u001b[38;5;28minput\u001b[39m.dtype)\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tau, support_size\n",
      "\u001b[31mRuntimeError\u001b[39m: index -1 is out of bounds for dimension 1 with size 12"
     ]
    }
   ],
   "source": [
    "# Ï†ÄÏû•Îêú Î™®Îç∏Îì§Î°ú ÏòàÏ∏°\n",
    "predictions_list = []\n",
    "\n",
    "for fold, model in enumerate(models):\n",
    "    print(f\"Predict with fold {fold+1}\")\n",
    "    preds = model.predict(test[features].values)\n",
    "    predictions_list.append(preds)\n",
    "\n",
    "# ÌèâÍ∑† ÏòàÏ∏°\n",
    "final_predictions = np.mean(predictions_list, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['ÏÑ±Í≥µÌôïÎ•†'] = final_predictions\n",
    "sample_submission.to_csv('./baseline_submission.csv', index = False, encoding = 'utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
