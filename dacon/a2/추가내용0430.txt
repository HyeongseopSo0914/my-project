#오후 5:36 2025-04-29 기준 결과#
Early stopping occurred at epoch 48 with best_epoch = 38 and best_val_0_mae = 0.2075
Early stopping occurred at epoch 35 with best_epoch = 25 and best_val_0_mae = 0.20796
Early stopping occurred at epoch 46 with best_epoch = 36 and best_val_0_mae = 0.20338
Early stopping occurred at epoch 33 with best_epoch = 23 and best_val_0_mae = 0.20364
Early stopping occurred at epoch 55 with best_epoch = 45 and best_val_0_mae = 0.20531
1. 평균 CV Score (MAE): 0.20556
2. 추정 Leaderboard 점수(예측 평균 기준): 0.02790
3. 현재 모델은 성능 안정성과 일반화 측면에서 우수하여 후속 스태킹 기반 모델에 적합합니다.


# 오후 5:45 2025-04-29 현재 뒤에 3개 파생변수 제거
📌 전체 결과 요약 (동적)
1. 평균 CV Score (MAE): 0.20503
2. 추정 Leaderboard 점수(예측 평균 기준): 0.02599
3. 현재 모델은 성능 안정성과 일반화 측면에서 우수하여 후속 스태킹 기반 모델에 적합합니다.

특징 (features) 설계 강화
기존 파생변수 외에도 다음과 같은 관점에서 추가적인 특성을 고민해볼 수 있어요:

비율형 파생 변수: 예: 직원 수 / 고객 수, 매출 / 나이, 매출 / 투자단계

상호작용 변수: 예: 국가 * 분야, 상장여부 * 투자단계, 설립연도 * 기업가치

지수/로그 트랜스폼 추가 실험: 특히 skew가 있는 경우

이상치 처리 후 변수: 예: 상위/하위 1% 제외 후 평균으로 파생

군집 변수 추가 (예: KMeans로 고객수, 매출, 투자금 기준 군집화 후 클러스터 번호 변수로 사용)

2. Target Encoding / Mean Encoding
범주형 변수에 대해 target mean encoding을 적용해 볼 수 있습니다.

예: 국가별 평균 성공확률, 분야별 평균 성공확률

KFold 내부에서 누락 방지용으로 out-of-fold 방식으로 적용 필요

3. 다른 모델 추가 및 스태킹
현재는 TabNet, LightGBM, GBR을 평균했지만,

CatBoost, XGBoost, MLP, ElasticNet, SVR 등도 후보입니다.

2-stage stacking으로 한 번 더 메타 모델 학습하는 것도 성능 향상 가능

4. Feature Selection
SHAP, permutation importance 등을 이용해 불필요한 feature를 제거하거나,

중요한 feature 중심으로 집중

5. 하이퍼파라미터 튜닝
특히 TabNet과 LightGBM은 파라미터 튜닝으로 성능 차이가 꽤 큽니다.

예: n_d, n_steps, learning_rate, num_leaves 등





