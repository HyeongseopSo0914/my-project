{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fe7d0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ì„¤ë¦½ì—°ë„', 'êµ­ê°€', 'ë¶„ì•¼', 'íˆ¬ìë‹¨ê³„', 'ì§ì› ìˆ˜', 'ê³ ê°ìˆ˜(ë°±ë§Œëª…)', 'ì´ íˆ¬ìê¸ˆ(ì–µì›)', 'ì—°ë§¤ì¶œ(ì–µì›)', 'SNS íŒ”ë¡œì›Œ ìˆ˜(ë°±ë§Œëª…)', 'ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)', 'ì„±ê³µí™•ë¥ ', 'ì§ì› ìˆ˜_ê²°ì¸¡', 'ê³ ê°ìˆ˜(ë°±ë§Œëª…)_ê²°ì¸¡', 'ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)_ê²°ì¸¡']\n",
      "â–¶ ì—°ë§¤ì¶œ(ì–µì›)_ì´ìƒì¹˜ì—¬ë¶€ - ì´ìƒì¹˜ ê°œìˆ˜(train): 0\n",
      "â–¶ ê³ ê°ìˆ˜(ë°±ë§Œëª…)_ì´ìƒì¹˜ì—¬ë¶€ - ì´ìƒì¹˜ ê°œìˆ˜(train): 0\n",
      "â–¶ SNS íŒ”ë¡œì›Œ ìˆ˜(ë°±ë§Œëª…)_ì´ìƒì¹˜ì—¬ë¶€ - ì´ìƒì¹˜ ê°œìˆ˜(train): 0\n",
      "âœ… train ê²°ì¸¡ í™•ì¸:\n",
      " ì„¤ë¦½ì—°ë„              0\n",
      "êµ­ê°€                0\n",
      "ë¶„ì•¼                0\n",
      "íˆ¬ìë‹¨ê³„              0\n",
      "ì§ì› ìˆ˜              0\n",
      "ì¸ìˆ˜ì—¬ë¶€              0\n",
      "ìƒì¥ì—¬ë¶€              0\n",
      "ê³ ê°ìˆ˜(ë°±ë§Œëª…)          0\n",
      "ì´ íˆ¬ìê¸ˆ(ì–µì›)         0\n",
      "ì—°ë§¤ì¶œ(ì–µì›)           0\n",
      "SNS íŒ”ë¡œì›Œ ìˆ˜(ë°±ë§Œëª…)    0\n",
      "ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)         0\n",
      "ì„±ê³µí™•ë¥               0\n",
      "ì§ì› ìˆ˜_ê²°ì¸¡           0\n",
      "ê³ ê°ìˆ˜(ë°±ë§Œëª…)_ê²°ì¸¡       0\n",
      "ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)_ê²°ì¸¡      0\n",
      "dtype: int64\n",
      "âœ… test ê²°ì¸¡ í™•ì¸:\n",
      " ì„¤ë¦½ì—°ë„              0\n",
      "êµ­ê°€                0\n",
      "ë¶„ì•¼                0\n",
      "íˆ¬ìë‹¨ê³„              0\n",
      "ì§ì› ìˆ˜              0\n",
      "ì¸ìˆ˜ì—¬ë¶€              0\n",
      "ìƒì¥ì—¬ë¶€              0\n",
      "ê³ ê°ìˆ˜(ë°±ë§Œëª…)          0\n",
      "ì´ íˆ¬ìê¸ˆ(ì–µì›)         0\n",
      "ì—°ë§¤ì¶œ(ì–µì›)           0\n",
      "SNS íŒ”ë¡œì›Œ ìˆ˜(ë°±ë§Œëª…)    0\n",
      "ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)         0\n",
      "ì§ì› ìˆ˜_ê²°ì¸¡           0\n",
      "ê³ ê°ìˆ˜(ë°±ë§Œëª…)_ê²°ì¸¡       0\n",
      "ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)_ê²°ì¸¡      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“¦ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”©\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# âœ… í•œê¸€ ì„¤ì • ë° ë§ˆì´ë„ˆìŠ¤ ê¹¨ì§ ë°©ì§€\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# ğŸ“‚ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "train = pd.read_csv('train.csv').drop(columns=['ID'])\n",
    "test = pd.read_csv('test.csv').drop(columns=['ID'])\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# ğŸ”§ ë²”ìœ„ê°’ ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def convert_range_to_float(value):\n",
    "    if isinstance(value, str) and '-' in value:\n",
    "        try:\n",
    "            low, high = map(float, value.split('-'))\n",
    "            return (low + high) / 2\n",
    "        except:\n",
    "            return np.nan\n",
    "    try:\n",
    "        return float(value)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# ğŸ”§ ë²”ì£¼í˜• ì¸ì½”ë”©\n",
    "def encode_categoricals(df, cols):\n",
    "    df = df.copy()\n",
    "    for col in cols:\n",
    "        df[col] = LabelEncoder().fit_transform(df[col].astype(str))\n",
    "    return df\n",
    "\n",
    "# ğŸ”§ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def fill_missing_values(df, is_train=True):\n",
    "    df = df.copy()\n",
    "\n",
    "    # ë²”ìœ„ ë¬¸ìì—´ â†’ í‰ê·  ìˆ«ì ì²˜ë¦¬\n",
    "    for col in ['ì—°ë§¤ì¶œ(ì–µì›)', 'ì´ íˆ¬ìê¸ˆ(ì–µì›)', 'ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)']:\n",
    "        df[col] = df[col].apply(convert_range_to_float)\n",
    "\n",
    "    # ë¶„ì•¼ ê²°ì¸¡ ë° ì¸ì½”ë”©\n",
    "    if 'ë¶„ì•¼' in df.columns:\n",
    "        df['ë¶„ì•¼'] = df['ë¶„ì•¼'].fillna('Unknown')\n",
    "        df['ë¶„ì•¼'] = LabelEncoder().fit_transform(df['ë¶„ì•¼'])\n",
    "\n",
    "    # êµ­ê°€, íˆ¬ìë‹¨ê³„ ì¸ì½”ë”©\n",
    "    df = encode_categoricals(df, ['êµ­ê°€', 'íˆ¬ìë‹¨ê³„'])\n",
    "\n",
    "    # âœ… ê²°ì¸¡ í”Œë˜ê·¸ ì¶”ê°€ í•¨ìˆ˜\n",
    "    def add_missing_flag(column):\n",
    "        flag_col = f'{column}_ê²°ì¸¡'\n",
    "        df[flag_col] = df[column].isnull().astype(int)\n",
    "\n",
    "    # âœ… í”¼ì²˜ì…‹ ìƒì„± í•¨ìˆ˜\n",
    "    def get_features(base):\n",
    "        return base + (['ì„±ê³µí™•ë¥ '] if is_train else [])\n",
    "\n",
    "    # 1. ì§ì› ìˆ˜\n",
    "    if 'ì§ì› ìˆ˜' in df.columns:\n",
    "        add_missing_flag('ì§ì› ìˆ˜')\n",
    "        features = get_features(['ì„¤ë¦½ì—°ë„', 'êµ­ê°€', 'íˆ¬ìë‹¨ê³„', 'ì—°ë§¤ì¶œ(ì–µì›)', 'ì´ íˆ¬ìê¸ˆ(ì–µì›)', 'SNS íŒ”ë¡œì›Œ ìˆ˜(ë°±ë§Œëª…)'])\n",
    "        complete = df[df['ì§ì› ìˆ˜'].notnull()]\n",
    "        missing = df[df['ì§ì› ìˆ˜'].isnull()]\n",
    "        if not complete.empty and not missing.empty:\n",
    "            model = GradientBoostingRegressor()\n",
    "            model.fit(complete[features], complete['ì§ì› ìˆ˜'])\n",
    "            df.loc[df['ì§ì› ìˆ˜'].isnull(), 'ì§ì› ìˆ˜'] = model.predict(missing[features])\n",
    "\n",
    "    # 2. ê³ ê° ìˆ˜\n",
    "    if 'ê³ ê°ìˆ˜(ë°±ë§Œëª…)' in df.columns:\n",
    "        add_missing_flag('ê³ ê°ìˆ˜(ë°±ë§Œëª…)')\n",
    "        features = get_features(['ì„¤ë¦½ì—°ë„', 'ì§ì› ìˆ˜', 'ë¶„ì•¼', 'ì—°ë§¤ì¶œ(ì–µì›)', 'ì´ íˆ¬ìê¸ˆ(ì–µì›)', 'SNS íŒ”ë¡œì›Œ ìˆ˜(ë°±ë§Œëª…)'])\n",
    "        complete = df[df['ê³ ê°ìˆ˜(ë°±ë§Œëª…)'].notnull()]\n",
    "        missing = df[df['ê³ ê°ìˆ˜(ë°±ë§Œëª…)'].isnull()]\n",
    "        if not complete.empty and not missing.empty:\n",
    "            model = GradientBoostingRegressor()\n",
    "            model.fit(complete[features], complete['ê³ ê°ìˆ˜(ë°±ë§Œëª…)'])\n",
    "            df.loc[df['ê³ ê°ìˆ˜(ë°±ë§Œëª…)'].isnull(), 'ê³ ê°ìˆ˜(ë°±ë§Œëª…)'] = model.predict(missing[features])\n",
    "\n",
    "    # 3. ê¸°ì—…ê°€ì¹˜\n",
    "    if 'ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)' in df.columns:\n",
    "        add_missing_flag('ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)')\n",
    "        features = get_features(['ì„¤ë¦½ì—°ë„', 'ì§ì› ìˆ˜', 'ê³ ê°ìˆ˜(ë°±ë§Œëª…)', 'ë¶„ì•¼', 'ì—°ë§¤ì¶œ(ì–µì›)', 'ì´ íˆ¬ìê¸ˆ(ì–µì›)', 'SNS íŒ”ë¡œì›Œ ìˆ˜(ë°±ë§Œëª…)'])\n",
    "        complete = df[df['ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)'].notnull()]\n",
    "        missing = df[df['ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)'].isnull()]\n",
    "        if not complete.empty and not missing.empty:\n",
    "            model = GradientBoostingRegressor()\n",
    "            model.fit(complete[features], complete['ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)'])\n",
    "            df.loc[df['ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)'].isnull(), 'ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)'] = model.predict(missing[features])\n",
    "\n",
    "    return df\n",
    "\n",
    "# ğŸ“Œ ë°ì´í„° ì „ì²˜ë¦¬\n",
    "train_filled = fill_missing_values(train, is_train=True)\n",
    "test_filled = fill_missing_values(test, is_train=False)\n",
    "\n",
    "# ğŸ“Œ ì´ìƒì¹˜ ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def process_outliers(train_df, test_df, num_cols, method='flag+clip'):\n",
    "    train, test = train_df.copy(), test_df.copy()\n",
    "    for col in num_cols:\n",
    "        Q1, Q3 = train[col].quantile([0.25, 0.75])\n",
    "        IQR = Q3 - Q1\n",
    "        lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "\n",
    "        if 'flag' in method:\n",
    "            train[f'{col}_ì´ìƒì¹˜ì—¬ë¶€'] = ((train[col] < lower) | (train[col] > upper)).astype(int)\n",
    "            test[f'{col}_ì´ìƒì¹˜ì—¬ë¶€'] = ((test[col] < lower) | (test[col] > upper)).astype(int)\n",
    "\n",
    "        if 'clip' in method:\n",
    "            train[col] = train[col].clip(lower, upper)\n",
    "            test[col] = test[col].clip(lower, upper)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "num_cols = train_filled.select_dtypes(include=np.number).columns.drop('ì„±ê³µí™•ë¥ ').tolist()\n",
    "train_processed, test_processed = process_outliers(train_filled, test_filled, num_cols)\n",
    "\n",
    "\n",
    "\n",
    "def detect_outliers_summary(df, columns):\n",
    "    summary = []\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        summary.append({\n",
    "            'ì»¬ëŸ¼ëª…': col,\n",
    "            'ì´ìƒì¹˜ ìˆ˜': len(outliers),\n",
    "            'ì „ì²´ ëŒ€ë¹„ ë¹„ìœ¨(%)': round(len(outliers) / len(df) * 100, 2)\n",
    "        })\n",
    "    return pd.DataFrame(summary).sort_values(by='ì´ìƒì¹˜ ìˆ˜', ascending=False)\n",
    "\n",
    "# log ë³€í™˜ (0ë³´ë‹¤ í° ê°’ë§Œ ë³€í™˜, log1pëŠ” log(1+x))\n",
    "num_cols = train_filled.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "print(num_cols)\n",
    "log_train = train_filled[num_cols].copy()\n",
    "for col in num_cols:\n",
    "    if (log_train[col] > 0).all():  # ìŒìˆ˜, 0 ìˆëŠ” ì»¬ëŸ¼ì€ ì œì™¸\n",
    "        log_train[col] = np.log1p(log_train[col])\n",
    "\n",
    "def process_outliers_train_test(train_df, test_df, num_cols, method='flag+clip'):\n",
    "    \"\"\"\n",
    "    train ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ IQR ì´ìƒì¹˜ íƒì§€ ê¸°ì¤€ì„ ì¡ê³ ,\n",
    "    train/test ëª¨ë‘ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì´ìƒì¹˜ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    train_processed = train_df.copy()\n",
    "    test_processed = test_df.copy()\n",
    "    outlier_bounds = {}\n",
    "\n",
    "    for col in num_cols:\n",
    "        Q1 = train_df[col].quantile(0.25)\n",
    "        Q3 = train_df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - 1.5 * IQR\n",
    "        upper = Q3 + 1.5 * IQR\n",
    "        outlier_bounds[col] = (lower, upper)\n",
    "\n",
    "        # ì´ìƒì¹˜ í”Œë˜ê·¸\n",
    "        if 'flag' in method:\n",
    "            train_processed[f'{col}_ì´ìƒì¹˜ì—¬ë¶€'] = ((train_df[col] < lower) | (train_df[col] > upper)).astype(int)\n",
    "            test_processed[f'{col}_ì´ìƒì¹˜ì—¬ë¶€'] = ((test_df[col] < lower) | (test_df[col] > upper)).astype(int)\n",
    "\n",
    "        # í´ë¦¬í•‘\n",
    "        if 'clip' in method:\n",
    "            train_processed[col] = train_df[col].clip(lower, upper)\n",
    "            test_processed[col] = test_df[col].clip(lower, upper)\n",
    "\n",
    "        # ë¡œê·¸ ë³€í™˜\n",
    "        if 'log' in method:\n",
    "            if (train_processed[col] >= 0).all() and (test_processed[col] >= 0).all():\n",
    "                train_processed[col] = np.log1p(train_processed[col])\n",
    "                test_processed[col] = np.log1p(test_processed[col])\n",
    "            else:\n",
    "                print(f\"[ê²½ê³ ] {col}ì€ log1p ë¶ˆê°€ëŠ¥ (ìŒìˆ˜ ë˜ëŠ” 0 í¬í•¨)\")\n",
    "\n",
    "    return train_processed, test_processed, outlier_bounds\n",
    "\n",
    "# 'ì„±ê³µí™•ë¥ 'ì€ trainì—ë§Œ ìˆìœ¼ë¯€ë¡œ ì œì™¸\n",
    "num_cols = train_filled.select_dtypes(include=np.number).columns.tolist()\n",
    "num_cols = [col for col in num_cols if col != 'ì„±ê³µí™•ë¥ ']\n",
    "\n",
    "# ì´ìƒì¹˜ ì²˜ë¦¬ ìˆ˜í–‰\n",
    "train_processed, test_processed, bounds = process_outliers_train_test(train_filled, test_filled, num_cols, method='flag+clip')\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸: ì¼ë¶€ ì»¬ëŸ¼ì— ëŒ€í•´ ì´ìƒì¹˜ ì—¬ë¶€ í”Œë˜ê·¸ ë¶„í¬ ì¶œë ¥\n",
    "for col in ['ì—°ë§¤ì¶œ(ì–µì›)', 'ê³ ê°ìˆ˜(ë°±ë§Œëª…)', 'SNS íŒ”ë¡œì›Œ ìˆ˜(ë°±ë§Œëª…)']:\n",
    "    flag_col = f'{col}_ì´ìƒì¹˜ì—¬ë¶€'\n",
    "    if flag_col in train_processed.columns:\n",
    "        print(f\"â–¶ {flag_col} - ì´ìƒì¹˜ ê°œìˆ˜(train): {train_processed[flag_col].sum()}\")\n",
    "\n",
    "print(\"âœ… train ê²°ì¸¡ í™•ì¸:\\n\", train_filled.isnull().sum().sort_values(ascending=False))\n",
    "print(\"âœ… test ê²°ì¸¡ í™•ì¸:\\n\", test_filled.isnull().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca7d56c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Œ íŒŒìƒë³€ìˆ˜ ìƒì„±\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    return df\n",
    "\n",
    "X = train_processed.copy()  # â† ì—¬ê¸°ì„œ train_processedì— ì´ìƒì¹˜ í”Œë˜ê·¸ê°€ ë“¤ì–´ìˆì–´ì•¼ í•¨\n",
    "X = create_features(X)      # íŒŒìƒë³€ìˆ˜ ì¶”ê°€\n",
    "X_test = test_processed.copy()\n",
    "X_test = create_features(X_test)\n",
    "y = train_processed['ì„±ê³µí™•ë¥ ']  # ë˜ëŠ” ë¯¸ë¦¬ ë¶„ë¦¬í•œ y ì‚¬ìš©\n",
    "\n",
    "# ğŸ“Œ í•„ìš”ì—†ëŠ” ì»¬ëŸ¼ ì œê±°\n",
    "remove_cols = ['ì§ì› ìˆ˜', 'ì—°ë§¤ì¶œ(ì–µì›)', 'ì„¤ë¦½ì—°ë„', 'ì´ íˆ¬ìê¸ˆ(ì–µì›)', 'ì§ì› ìˆ˜_ê²°ì¸¡', 'ê³ ê°ìˆ˜_ê²°ì¸¡']\n",
    "X.drop(columns=[col for col in remove_cols if col in X.columns], inplace=True)\n",
    "X_test.drop(columns=[col for col in remove_cols if col in X_test.columns], inplace=True)\n",
    "\n",
    "# âœ… ì´ì§„í˜• ì¸ì½”ë”©\n",
    "for col in ['ì¸ìˆ˜ì—¬ë¶€', 'ìƒì¥ì—¬ë¶€']:\n",
    "    for df in [X, X_test]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].map({'No': 0, 'Yes': 1})\n",
    "\n",
    "# 'ì„±ê³µí™•ë¥ ' ì»¬ëŸ¼ì´ ìˆë‹¤ë©´ ì‚­ì œ\n",
    "if 'ì„±ê³µí™•ë¥ ' in X.columns:\n",
    "    X = X.drop(columns=['ì„±ê³µí™•ë¥ '])\n",
    "\n",
    "# y ê²°í•©\n",
    "X_with_y = X.join(y.rename(\"ì„±ê³µí™•ë¥ \"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ca5d6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 13:29:57,534 INFO: [AutoFeat] The 2 step feature engineering process could generate up to 13041 features.\n",
      "2025-05-01 13:29:57,535 INFO: [AutoFeat] With 3500 data points this new feature matrix would use about 0.18 gb of space.\n",
      "2025-05-01 13:29:57,538 INFO: [feateng] Step 1: transformation of original features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[feateng]               0/             23 features transformed\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 13:29:59,400 INFO: [feateng] Generated 25 transformed features from 23 original features - done.\n",
      "2025-05-01 13:29:59,403 INFO: [feateng] Step 2: first combination of features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[feateng]            1000/           1128 feature tuples combined\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human\\.conda\\envs\\dacon\\Lib\\site-packages\\numpy\\_core\\_methods.py:205: RuntimeWarning: overflow encountered in reduce\n",
      "2025-05-01 13:30:00,217 INFO: [feateng] Generated 614 feature combinations from 1128 original feature tuples - done.\n",
      "2025-05-01 13:30:00,241 INFO: [feateng] Generated altogether 1148 new features in 2 steps\n",
      "2025-05-01 13:30:00,242 INFO: [feateng] Removing correlated features, as well as additions at the highest level\n",
      "2025-05-01 13:30:00,357 INFO: [feateng] Generated a total of 470 additional features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[featsel] Scaling data.../           1128 feature tuples combined"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 13:30:00,530 INFO: [featsel] Feature selection run 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "âŒ ìµœì¢… ì—ëŸ¬: Input X contains NaN.\n",
      "LassoLarsCV does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n"
     ]
    }
   ],
   "source": [
    "from autofeat import AutoFeatRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. ë°ì´í„° ë¶„í• \n",
    "X_train_af, X_val_af, y_train_af, y_val_af = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. AutoFeatRegressor ì„ ì–¸\n",
    "afr = AutoFeatRegressor(verbose=1, feateng_steps=2)\n",
    "\n",
    "try:\n",
    "    # 3. íŒŒìƒë³€ìˆ˜ ìƒì„±\n",
    "    X_train_transformed = afr.fit_transform(X_train_af, y_train_af)\n",
    "    X_train_transformed = pd.DataFrame(X_train_transformed)\n",
    "    \n",
    "    # âœ… NaN ë˜ëŠ” Inf ì¡´ì¬ í™•ì¸\n",
    "    bad_cols = X_train_transformed.columns[\n",
    "        X_train_transformed.isnull().any() | np.isinf(X_train_transformed).any()\n",
    "    ]\n",
    "    print(\"ğŸ§¹ ì œê±°í•  ë¬¸ì œ ìˆëŠ” ì»¬ëŸ¼ ìˆ˜:\", len(bad_cols))\n",
    "\n",
    "    # âœ… ë¬¸ì œ ìˆëŠ” ì—´ ì œê±°\n",
    "    X_train_transformed = X_train_transformed.drop(columns=bad_cols)\n",
    "\n",
    "    # 4. val transform í›„ ê°™ì€ ì»¬ëŸ¼ë§Œ ì¶”ì¶œ\n",
    "    X_val_transformed = afr.transform(X_val_af)\n",
    "    X_val_transformed = pd.DataFrame(X_val_transformed)\n",
    "    X_val_transformed = X_val_transformed[X_train_transformed.columns]\n",
    "    X_val_transformed = X_val_transformed.replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "    # 5. XGBoost í•™ìŠµ ë° í‰ê°€\n",
    "    model = XGBRegressor(n_estimators=500, learning_rate=0.01, max_depth=5, random_state=42)\n",
    "    model.fit(X_train_transformed, y_train_af)\n",
    "    y_pred = model.predict(X_val_transformed)\n",
    "    mae = mean_absolute_error(y_val_af, y_pred)\n",
    "    print(f\"ğŸ“‰ Validation MAE: {mae:.5f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"âŒ ìµœì¢… ì—ëŸ¬:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
